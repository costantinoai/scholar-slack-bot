[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Privileged representational axes in biological and artificial neural networks",
            "pub_year": 2024,
            "citation": "bioRxiv, 2024.06. 20.599957, 2024",
            "author": "Meenakshi Khosla and Alex H Williams and Josh McDermott and Nancy Kanwisher",
            "journal": "bioRxiv",
            "pages": "2024.06. 20.599957",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "How do neurons code information? Recent work emphasizes properties of population codes, such as their geometry and decodable information, using measures that are blind to the native tunings (or \u2018axes\u2019) of neural responses. But might these representational axes matter, with some privileged systematically over others? To find out, we developed methods to test for alignment of neural tuning across brains and deep convolutional neural networks (DCNNs). Across both vision and audition, both brains and DCNNs consistently favored certain axes for representing the natural world. Moreover, the representational axes of DCNNs trained on natural inputs were aligned to those in perceptual cortices, such that axis-sensitive model-brain similarity metrics better differentiated competing models of biological sensory systems. We further show that coding schemes that privilege certain axes can reduce downstream wiring costs and improve generalization. These results motivate a new framework for understanding neural tuning in biological and artificial networks and its computational benefits."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:cBPnxVikjH8C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2024.06.20.599957.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:4B-IvYSgMhkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Utility navigation",
            "pub_year": 2024,
            "citation": "",
            "author": "Mary Ann Rankin",
            "abstract": "While dean of Natural Sciences at the University of Texas, her biggest achievement was her personal drive to redesign the education of science and math teachers for secondary schools. She investigated inadequacies and barriers, led an exhaustive redesign, and oversaw implementation and further development. This system of teacher education, known widely as UTeach, was brilliantly successful. Very shortly, it was producing science and math teacher-education majors in the UT College of Natural Sciences, with better GPAs and better diversity than in the College overall, and in numbers exceeding those preceding redesign by several fold. UTeach is the most widely emulated basis for secondary teacher education reform--and not just in STEM fields."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:Og1tA8FjbJAC",
        "num_citations": 0,
        "pub_url": "https://www.amacad.org/person/mary-ann-rankin",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8qCFG9kAbvMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Reassessing the Selectivity of the Visual Food Component",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Cyn X Fang and Nancy Kanwisher and Meenakshi Khosla",
            "publisher": "OSF",
            "abstract": "This study seeks to further examine the claimed food selectivity in the visual stream (Khosla et al, 2022; Jain et al, 2022; Pennock et al, 2023). Experiment 1 (https://doi. org/10.17605/OSF. IO/XRG8T) showed that food selectivity was not found for cutout images, in which food and nonfood objects were isolated and pasted on a white background. In this experiment, we aim to test the effects of (a) Distance,(b) Real World Size, and (c) Material Properties on the activation of the food component."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:x21FZCSn4ZoC",
        "num_citations": 0,
        "pub_url": "https://osf.io/769ys/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:DnGvpcbi9lAJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Representation of navigational affordances and ego-motion in the occipital place area",
            "pub_year": 2024,
            "citation": "bioRxiv, 2024.04. 30.591964, 2024",
            "author": "Frederik S Kamps and Emily M Chen and Nancy Kanwisher and Rebecca Saxe",
            "journal": "bioRxiv",
            "pages": "2024.04. 30.591964",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Humans effortlessly use vision to plan and guide navigation through the local environment, or \"scene\". A network of three cortical regions responds selectively to visual scene information, including the occipital (OPA), parahippocampal (PPA), and medial place areas (MPA) - but how this network supports visually-guided navigation is unclear. Recent evidence suggests that one region in particular, the OPA, supports visual representations for navigation, while PPA and MPA support other aspects of scene processing. However, most previous studies tested only static scene images, which lack the dynamic experience of navigating through scenes. We used dynamic movie stimuli to test whether OPA, PPA, and MPA represent two critical kinds of navigationally-relevant information: navigational affordances (e.g., can I walk to the left, right, or both?) and ego-motion (e.g., am I walking forward or backward? turning left or right?). We found that OPA is sensitive to both affordances and ego-motion, as well as the conflict between these cues - e.g., turning toward versus away from an open doorway. These effects were significantly weaker or absent in PPA and MPA. Responses in OPA were also dissociable from those in early visual cortex, consistent with the idea that OPA responses are not merely explained by lower-level visual features. OPA responses to affordances and ego-motion were stronger in the contralateral than ipsilateral visual field, suggesting that OPA encodes navigationally relevant information within an egocentric reference frame. Taken together, these results support the hypothesis that OPA contains visual representations that are useful \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:6VlyvFCUEfcC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2024.04.30.591964.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:b46bhzZsEFwJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Language in brains, minds, and machines",
            "pub_year": 2024,
            "citation": "Annual Review of Neuroscience 47, 2024",
            "author": "Greta Tuckute and Nancy Kanwisher and Evelina Fedorenko",
            "volume": "47",
            "publisher": "Annual Reviews",
            "abstract": "It has long been argued that only humans could produce and understand language. But now, for the first time, artificial language models (LMs) achieve this feat. Here we survey the new purchase LMs are providing on the question of how language is implemented in the brain. We discuss why, a priori, LMs might be expected to share similarities with the human language system. We then summarize evidence that LMs represent linguistic information similarly enough to humans to enable relatively accurate brain encoding and decoding during language processing. Finally, we examine which LM properties\u2014their architecture, task performance, or training\u2014are critical for capturing human neural responses to language and review studies using LMs as in silico model organisms for testing hypotheses about language. These ongoing investigations bring us closer to understanding the representations and processes that \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:7Frjd3zlGBUC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=17553192150834493324",
        "cites_id": [
            "17553192150834493324"
        ],
        "pub_url": "https://www.annualreviews.org/content/journals/10.1146/annurev-neuro-120623-101142",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jDeqqEJ3mfMJ:scholar.google.com/",
        "cites_per_year": {
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "An Efficient Multimodal fMRI Localizer for High-Level Visual, Auditory, and Cognitive Regions in Humans",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Samuel Hutchinson and Ammar Marvi and Frederik Kamps and Evelina Fedorenko and Rebecca Saxe and Nancy Kanwisher",
            "publisher": "OSF",
            "abstract": "Research using functional MRI (fMRI) and other neuroimaging methods has provided extensive evidence that some regions of the cortex in humans serve highly specific functions (Kanwisher 2010). In order to study these regions and characterize their representations, we need to first find them. While these functionally-specific cortical regions generally co-localize across people, their precise location varies from one individual to the next. It is therefore necessary to functionally identify each region in each participant individually with a functional localizer scan. However there are many such regions, and running a large number of localizer scans to identify each of them is expensive and time consuming. Here we test the effectiveness of a new experimental design to functionally localize multiple regions of interest robustly, efficiently, and accurately in each participant individually in just 23 minutes of fMRI scan time per person. This new localizer\u2013which presents simultaneous auditory and visual stimuli\u2013is designed to identify, within individual participants, cortical regions selectively engaged in visually processing faces, scenes, bodies, words, and objects, as well as speech sounds, language, and theory of mind. Here we test the success of this new localizer against the current gold standard of established standard localizers for these functions."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:xm0LlTxljI0C",
        "num_citations": 0,
        "pub_url": "https://osf.io/gjsdb/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:3YtiBYeYujYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Using fMRI to explore causal reasoning and abstract relational reasoning in the left lateral prefrontal cortex in the human brain",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Jessica Chomik-Morales and Nancy Kanwisher and Laura Schulz and RT Pramod",
            "publisher": "OSF",
            "abstract": "This study builds upon our earlier experiment (\u201cExperiment 1\u201d; https://osf. io/s54tb), which found a region in the left lateral prefrontal cortex that responded more strongly in causal reasoning about both physical and social causes compared to descriptive reasoning about physical and social situations. In Experiment 2 we test whether we can replicate the key finding from Experiment 1, namely that Physical Causal (PC) and Social Causal (SC) conditions will produce higher responses in a left frontal region than Physical Description (PD) and Social Description (SD) conditions. We further test the specificity of this region for causal reasoning per se, versus whether this region is also engaged in processing other abstract relationships like temporal order (TO) and part/whole relationships (PW), versus whether it is simply engaged during any demands on executive function broadly (EF). We also test whether the same region responds during a different causal task, when participants reason counterfactually about how an event could be prevented, for both physical (PCP) and social (SCP) conditions. In Experiment 3, we test whether the same region is more strongly engaged when participants read verbal vignettes of causal events of social or physical nature (SCn & PCn), and non-causal descriptions of either social or physical nature (SDn and PDn)."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:orDZ08hpP44C",
        "num_citations": 0,
        "pub_url": "https://osf.io/83x74/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:OIwB85sKmI8J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Fine-grained neural coding of bodies and body parts in human visual cortex",
            "pub_year": 2024,
            "citation": "",
            "author": "M Vanhoyland and Apurva Murty and T Decramer and W Van Paesschen and S Bracci and H Op de Beeck and N Kanwisher and P Janssen and T Theys",
            "abstract": "The visual image of a human body provides a valuable source of socially relevant information. However, our understanding of the neuronal mechanisms underlying body perception in humans remains limited given the spatiotemporal constraints of functional imaging. Here we recorded multi-unit spiking activity in two neurosurgical patients in or near the extrastriate body area (EBA), a critical region for body perception. Our recordings revealed a strong preference for human bodies over a large range of control stimuli. Notably, this preference was driven by a distinct selectivity for body parts. Moreover, the observed body selectivity generalized to non-photographic depictions of bodies such as silhouettes and stick figures. Overall, our study provides an unprecedented access into the representation of bodies in the human visual cortex to bridge the gap between human neuroimaging and macaque electrophysiology studies, and form a solid basis for computational models of human body processing."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:QsKbpXNoaWkC",
        "num_citations": 0,
        "pub_url": "https://europepmc.org/article/ppr/ppr805379",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:3rmQ0lkGkSgJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Fine-grained neural coding of bodies and body parts in human visual cortex",
            "pub_year": 2024,
            "citation": "bioRxiv, 2024.02. 09.579107, 2024",
            "author": "Jesus Garcia Ramirez and Michael Vanhoyland and Ratan N Apurva Murty and Thomas Decramer and Wim Van Paesschen and Stefania Bracci and Hans Op de Beeck and Nancy G Kanwisher and Peter Janssen and Tom Theys",
            "journal": "bioRxiv",
            "pages": "2024.02. 09.579107",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "The visual image of a human body provides a valuable source of socially relevant information. However, our understanding of the neuronal mechanisms underlying body perception in humans remains limited given the spatiotemporal constraints of functional imaging. Here we recorded multi-unit spiking activity in two neurosurgical patients in or near the extrastriate body area (EBA), a critical region for body perception. Our recordings revealed a strong preference for human bodies over a large range of control stimuli. Notably, this preference was driven by a  distinct selectivity for body parts. Moreover, the observed body selectivity generalized to non-photographic depictions of bodies such as silhouettes and stick figures. Overall, our study provides an unprecedented access into the representation of bodies in the human visual cortex to bridge the gap between human neuroimaging and macaque electrophysiology studies, and form a solid basis for computational models of human body processing."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:DXE8ND7PrJAC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2024.02.09.579107.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:unOfPyzsksUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "Future prediction of object contact in the human brain",
            "pub_year": 2023,
            "citation": "OSF, 2023",
            "author": "RT Pramod and Nancy Kanwisher and Cyn X Fang",
            "publisher": "OSF",
            "abstract": "In everyday life, we not only see singular objects but the relationships between them. Physical relations are also critical for predicting what will happen next\u2013when a container moves, so does its containee, but the same is not true of an object that merely occludes another object without contacting it. Thus, understanding and predicting contact and non-contact relationships is crucial for engaging successfully with the world. In our previous study (Experiment 1 pre-registration: https://osf. io/ezq3s), we have shown that the putative Physics Network in the fronto-parietal cortices of the human brain\u2013but not the ventral temporal cortex\u2013carry scenario-invariant representation of contact relationships. In the current study, we further explore the nature of object contact relationships in the human brain by probing whether the same regions (ie, the Physics Network) that carry scenario-invariant representation of contact relationships are also involved in the prediction of future contact between objects based on the events unfolding in the present. If true, this study will provide strong evidence for the Physics Engine hypothesis\u2013that the Physics network is not only involved in the representation of physical properties of the current visual scene but also the prediction of future events, perhaps through forward simulation."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:lg2tdxc6qMwC",
        "num_citations": 0,
        "pub_url": "https://osf.io/v2a3b/resources",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2301.06627, 2023",
            "author": "Kyle Mahowald and Anna A Ivanova and Idan A Blank and Nancy Kanwisher and Joshua B Tenenbaum and Evelina Fedorenko",
            "journal": "arXiv preprint arXiv:2301.06627",
            "abstract": "Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- \"thinking machines\", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:ji7lAbPyDbYC",
        "num_citations": 116,
        "citedby_url": "/scholar?hl=en&cites=6177737427708702812",
        "cites_id": [
            "6177737427708702812"
        ],
        "pub_url": "https://arxiv.org/abs/2301.06627",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:XHA2nnW7u1UJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 2,
            "2023": 110
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Using artificial neural networks to ask \u2018why\u2019questions of minds and brains",
            "pub_year": 2023,
            "citation": "Trends in Neurosciences 46 (3), 240-254, 2023",
            "author": "Nancy Kanwisher and Meenakshi Khosla and Katharina Dobs",
            "volume": "46",
            "number": "3",
            "pages": "240-254",
            "publisher": "Elsevier",
            "abstract": "Neuroscientists have long characterized the properties and functions of the nervous system, and are increasingly succeeding in answering how brains perform the tasks they do. But the question \u2018why' brains work the way they do is asked less often. The new ability to optimize artificial neural networks (ANNs) for performance on human-like tasks now enables us to approach these \u2018why' questions by asking when the properties of networks optimized for a given task mirror the behavioral and neural characteristics of humans performing the same task. Here we highlight the recent success of this strategy in explaining why the visual and auditory systems work the way they do, at both behavioral and neural levels."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:iyewoVqAXLQC",
        "num_citations": 29,
        "citedby_url": "/scholar?hl=en&cites=16034293729654481844",
        "cites_id": [
            "16034293729654481844"
        ],
        "pub_url": "https://www.cell.com/trends/neurosciences/fulltext/S0166-2236(22)00262-4?dgcid=raven_jbs_etoc_email",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:tCNUnDFBhd4J:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 25
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "CNNs reveal the computational implausibility of the expertise hypothesis",
            "pub_year": 2023,
            "citation": "Iscience 26 (2), 2023",
            "author": "Nancy Kanwisher and Pranjul Gupta and Katharina Dobs",
            "journal": "Iscience",
            "volume": "26",
            "number": "2",
            "publisher": "Elsevier",
            "abstract": "Face perception has long served as a classic example of domain specificity of mind and brain. But an alternative \"expertise\" hypothesis holds that putatively face-specific mechanisms are actually domain-general, and can be recruited for the perception of other objects of expertise (e.g., cars for car experts). Here, we demonstrate the computational implausibility of this hypothesis: Neural network models optimized for generic object categorization provide a better foundation for expert fine-grained discrimination than do models optimized for face recognition."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:wUn16MOA3RoC",
        "num_citations": 5,
        "citedby_url": "/scholar?hl=en&cites=15203082623615532629",
        "cites_id": [
            "15203082623615532629"
        ],
        "pub_url": "https://www.cell.com/iscience/pdf/S2589-0042(23)00053-6.pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VX6Exxgz_NIJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 5
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Preliminary evidence for selective cortical responses to music in one\u2010month\u2010old infants",
            "pub_year": 2023,
            "citation": "Developmental Science, e13387, 2023",
            "author": "Heather L Kosakowski and Samuel Norman\u2010Haignere and Anna Mynick and Atsushi Takahashi and Rebecca Saxe and Nancy Kanwisher",
            "journal": "Developmental Science",
            "pages": "e13387",
            "abstract": "Prior studies have observed selective neural responses in the adult human auditory cortex to music and speech that cannot be explained by the differing lower\u2010level acoustic properties of these stimuli. Does infant cortex exhibit similarly selective responses to music and speech shortly after birth? To answer this question, we attempted to collect functional magnetic resonance imaging (fMRI) data from 45 sleeping infants (2.0\u2010 to 11.9\u2010weeks\u2010old) while they listened to monophonic instrumental lullabies and infant\u2010directed speech produced by a mother. To match acoustic variation between music and speech sounds we (1) recorded music from instruments that had a similar spectral range as female infant\u2010directed speech, (2) used a novel excitation\u2010matching algorithm to match the cochleagrams of music and speech stimuli, and (3) synthesized \u201cmodel\u2010matched\u201d stimuli that were matched in spectrotemporal \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:4e5Qn2KL_jwC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=18080781322115500175",
        "cites_id": [
            "18080781322115500175"
        ],
        "pub_url": "https://onlinelibrary.wiley.com/doi/abs/10.1111/desc.13387",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:j0g2_tDW6_oJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Approaching human 3D shape perception with neurally mappable models",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2308.11300, 2023",
            "author": "Thomas P O'Connell and Tyler Bonnen and Yoni Friedman and Ayush Tewari and Josh B Tenenbaum and Vincent Sitzmann and Nancy Kanwisher",
            "journal": "arXiv preprint arXiv:2308.11300",
            "abstract": "Humans effortlessly infer the 3D shape of objects. What computations underlie this ability? Although various computational models have been proposed, none of them capture the human ability to match object shape across viewpoints. Here, we ask whether and how this gap might be closed. We begin with a relatively novel class of computational models, 3D neural fields, which encapsulate the basic principles of classic analysis-by-synthesis in a deep neural network (DNN). First, we find that a 3D Light Field Network (3D-LFN) supports 3D matching judgments well aligned to humans for within-category comparisons, adversarially-defined comparisons that accentuate the 3D failure cases of standard DNN models, and adversarially-defined comparisons for algorithmically generated shapes with no category structure. We then investigate the source of the 3D-LFN's ability to achieve human-aligned performance through a series of computational experiments. Exposure to multiple viewpoints of objects during training and a multi-view learning objective are the primary factors behind model-human alignment; even conventional DNN architectures come much closer to human behavior when trained with multi-view objectives. Finally, we find that while the models trained with multi-view learning objectives are able to partially generalize to new object categories, they fall short of human alignment. This work provides a foundation for understanding human shape inferences within neurally mappable computational architectures and highlights important questions for future work."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:nPTYJWkExTIC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2308.11300",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Behavioral signatures of face perception emerge in deep neural networks optimized for face recognition",
            "pub_year": 2023,
            "citation": "Proceedings of the National Academy of Sciences 120 (32), e2220642120, 2023",
            "author": "Katharina Dobs and Joanne Yuan and Julio Martinez and Nancy Kanwisher",
            "journal": "Proceedings of the National Academy of Sciences",
            "volume": "120",
            "number": "32",
            "pages": "e2220642120",
            "publisher": "National Academy of Sciences",
            "abstract": "Human face recognition is highly accurate and exhibits a number of distinctive and well-documented behavioral \u201csignatures\u201d such as the use of a characteristic representational space, the disproportionate performance cost when stimuli are presented upside down, and the drop in accuracy for faces from races the participant is less familiar with. These and other phenomena have long been taken as evidence that face recognition is \u201cspecial\u201d. But why does human face perception exhibit these properties in the first place? Here, we use deep convolutional neural networks (CNNs) to test the hypothesis that all of these signatures of human face perception result from optimization for the task of face recognition. Indeed, as predicted by this hypothesis, these phenomena are all found in CNNs trained on face recognition, but not in CNNs trained on object recognition, even when additionally trained to detect faces while \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:R-LXmdHK_14C",
        "num_citations": 0,
        "pub_url": "https://www.pnas.org/doi/abs/10.1073/pnas.2220642120",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:bSGN-JFa6v4J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Humans and 3D neural field models make similar 3D shape judgements",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5676-5676, 2023",
            "author": "Thomas OConnell and Tyler Bonnen and Yoni Friedman and Ayush Tewari and Josh Tenenbaum and Vincent Sitzmann and Nancy Kanwisher",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5676-5676",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Human visual perception captures the 3D shape of objects. While convolutional neural networks (CNNs) are similar to aspects of human visual processing, there is a well-documented gap in performance between CNNs and humans on shape processing tasks. A new deep learning approach, 3D neural fields (3D-NFs), has driven remarkable recent progress in 3D computer vision. 3D-NFs encode the geometry of objects in a coordinate-based representation (eg input: xyz coordinate, output: volume density and RGB at that position). Here, we investigate whether humans and 3D-NFs display similar behavior on 3D match-to-sample tasks. In each trial, a participant sees a rendered sample image of a manmade object, then matches it to a target image of the same object from a different viewpoint versus a lure image of a different object. We trained 3D-NFs that take an image as input, then output a rendered image of \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:ymY9cBF3mdcC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792594",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Decoding the Mass of Familiar Objects from MEG",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5508-5508, 2023",
            "author": "Willian De Faria and RT Pramod and Nancy Kanwisher",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5508-5508",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Successful engagement with the world requires an intuitive understanding of the physical properties of objects including their mass. Previous behavioral and neuroimaging studies have shown that people can infer the mass of an object by observing its interactions with other objects. But people also learn and remember the mass of familiar objects. Here we used magnetoencephalography (MEG) to test whether and when the mass of familiar objects can be decoded from neural responses to static images of familiar objects. Specifically, we showed participants (N= 20) sequences of images at the center of the screen and instructed them to make relative mass judgements on randomly interspersed cued trials (\u201cis this object lighter or heavier than the object in the previous trial?\u201d), which were discarded from further analysis. We collected MEG responses to 80 object images (20-32 trials/image in each subject) in which \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:pYKElYtJMmwC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791899",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Visual experience is necessary for selectivity of faces over language in the fusiform",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5848-5848, 2023",
            "author": "Elizabeth Saccone and N Apurva Ratan Murty and Judy Kim and LNU Akshi and Mengyu Tian and Nancy Kanwisher and Marina Bedny",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5848-5848",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "The contribution of innate constraints and experience to the development of face selectivity in human cortex is hotly debated. Unlike the sighted, people born blind report little interaction with faces in any modality. A recent study with blind adults found preferential responses to tactile faces over scenes and objects in the location of the fusiform face area (FFA)(Ratan Murty et al., 2020, PNAS). Blind adults also show responses to spoken and written language in a similar location (Kanjlia et al., 2016, PNAS; Tian et al., 2022, Cerebral Cortex). Does the FFA develop equivalently regardless of visual experience or does it respond to both faces and language in blind people? Congenitally blind adults (n= 8) performed both \u2018face touching\u2019and language tasks during fMRI. Participants felt 3D-printed models of faces, one at a time, and pressed a button if the same face repeated. In a control condition, participants touched 3D \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:M0leSnx2MbUC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792438",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Development of navigational affordance perception in infancy",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5716-5716, 2023",
            "author": "Frederik Kamps and Emily Chen and Adele Mah and Stephanie Washburn and Nancy Kanwisher and Rebecca Saxe",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5716-5716",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Shortly after learning to crawl or walk, toddlers successfully use vision to guide navigation through the local visual space. How does this ability develop? One hypothesis is that the emergence of navigational affordance perception depends on active navigation experience (eg, crawling). However, this hypothesis has never been tested, as almost all prior work conflates perception of navigational affordances with the integration of this information into a motor plan. Here we developed a measure of navigational affordance perception based only on preferential looking. Infants and toddlers viewed 10s videos depicting an egocentric perspective of navigation toward the corner of a room, with one wall containing an open doorway affording further navigation, and the other containing a perceptually similar distractor. Across three experiments, 16-month-old toddlers looked significantly more toward doorways (i) relative to \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:hNSvKAmkeYkC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792558",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Things versus Stuff in the Brain",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5096-5096, 2023",
            "author": "Vivian C Paulun and RT Pramod and Nancy Kanwisher",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5096-5096",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "In a seminal paper published two decades ago, Adelson (2001) noted that\" Our world contains both things and stuff, but things tend to get the attention.\" This remains the case today in the field of cognitive neuroscience. The large number of publications using fMRI to explore the lateral occipital complex (LOC) have focused almost exclusively on the role of this region in extracting the 3D shape of Things, without asking whether this region may also respond to Stuff with no fixed shape like honey, sand, or water. Similarly, investigations of the\" physics network\" previously implicated in visual intuitive physics (Fischer et al, 2016) have to date tested only Things, even though the physics of Stuff plays a comparable role in everyday life. Here, we asked whether LOC and the physics network are engaged when observing Stuff. We created 120 photorealistic short movie clips of four different computer-simulated substances \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:n3vGvpFsckwC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792274",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "fROI-level computational models enable broad-scale experimental testing and expose key divergences between models and brains",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5788-5788, 2023",
            "author": "Elizabeth Mieczkowski and Alex Abate and Willian De Faria and Kirsten Lydic and James DiCarlo and Nancy Kanwisher and N Apurva Ratan Murty",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5788-5788",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Deep convolutional neural network (DNN)-based models have emerged as our leading hypotheses of human vision. Here we describe, and expand upon, our latest effort to use DNN models of brain regions to explain key results from previous cognitive neuroscience and psychology experiments. Many stimuli in these prior experiments were highly manipulated (eg scrambled body parts, face parts, re-arranged spatial positions) often outside the domain of natural stimuli. These results can therefore be considered as tests of model generalization beyond naturalistic stimuli. We first performed these tests on the fusiform face area (FFA), parahippocampal place area (PPA) and the extrastriate body area (EBA). Our previous results (presented in VSS2022) showed that our fROI-level models recapitulate several key results from prior studies. We also observed that models did not perform as well on non-naturalistic stimuli \u2026"
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:9DLIHnF0jcYC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792491",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Testing the Selectivity of the Visual Food Component",
            "pub_year": 2023,
            "citation": "OSF, 2023",
            "author": "Cyn X Fang and Nancy Kanwisher and Meenakshi Khosla",
            "publisher": "OSF",
            "abstract": "Recently, analyses of the NSD dataset (Allen, EJ, St-Yves, G., Wu, Y. et al., 2022) revealed food selectivity in the visual stream.(Khosla et al, 2022; Jain et al, 2022; Pennock et al, 2023). This experiment seeks to further characterize the selectivity of the food component in the brain. Specifically, we seek to understand the effects of context (natural vs. cutout images), color (color vs. greyscale images), and reachability on food selectivity."
        },
        "filled": true,
        "author_pub_id": "XxBWSgoAAAAJ:jlhcAiayVhoC",
        "num_citations": 0,
        "pub_url": "https://osf.io/xrg8t/resources",
        "cites_per_year": {}
    }
]