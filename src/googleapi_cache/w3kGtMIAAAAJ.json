[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Don't trust your eyes: on the (un) reliability of feature visualizations",
            "pub_year": 2024,
            "citation": "International Conference on Learning Representations (ICML 2024), 2024",
            "author": "Robert Geirhos and Roland S Zimmermann and Blair Bilodeau and Wieland Brendel and Been Kim",
            "conference": "International Conference on Learning Representations (ICML 2024)",
            "abstract": "How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to \"explain\" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:g5m5HwL7SMYC",
        "num_citations": 13,
        "citedby_url": "/scholar?hl=en&cites=13494620787600136581",
        "cites_id": [
            "13494620787600136581"
        ],
        "pub_url": "https://arxiv.org/abs/2306.04719",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hRmC1xuGRrsJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3,
            "2024": 10
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Are Vision Language Models Texture or Shape Biased and Can We Steer Them?",
            "pub_year": 2024,
            "citation": "What is Next in Multimodal Foundation Models Workshop (CVPR 2024), 2024",
            "author": "Paul Gavrikov and Jovita Lukasik and Steffen Jung and Robert Geirhos and Bianca Lamm and Muhammad Jehanzeb Mirza and Margret Keuper and Janis Keuper",
            "conference": "What is Next in Multimodal Foundation Models Workshop (CVPR 2024)",
            "abstract": "Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text in multimodal models. If text does indeed influence visual biases, this suggests that we may be able to steer visual biases not just through visual input but also through language: a hypothesis that we confirm through extensive experiments. For instance, we are able to steer shape bias from as low as 49% to as high as 72% through prompting alone. For now, the strong human bias towards shape (96%) remains out of reach for all tested VLMs."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:1sJd4Hv_s6UC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=5422398788199890485",
        "cites_id": [
            "5422398788199890485"
        ],
        "pub_url": "https://arxiv.org/abs/2403.09193",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Nb4pAfg6QEsJ:scholar.google.com/",
        "cites_per_year": {
            "2024": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "How aligned are different alignment metrics?",
            "pub_year": 2024,
            "citation": "Re-Align Workshop on Representational Alignment (ICLR 2024), 2024",
            "author": "Jannis Ahlert and Thomas Klein and Felix A Wichmann and Robert Geirhos",
            "conference": "Re-Align Workshop on Representational Alignment (ICLR 2024)",
            "abstract": "In recent years, various methods and benchmarks have been proposed to empirically evaluate the alignment of artificial neural networks to human neural and behavioral data. But how aligned are different alignment metrics? To answer this question, we here analyze visual data from Brain-Score (Schrimpf et al., 2018), including metrics from the model-vs-human toolbox (Geirhos et al., 2021), together with human feature alignment (Linsley et al., 2018; Fel et al., 2022) and human similarity judgements (Muttenthaler et al., 2022). We find that pairwise correlations between neural scores and behavioral scores are quite low and sometimes even negative. For instance, the average correlation between those 80 models on Brain-Score that were fully evaluated on all 69 alignment metrics we considered is only 0.198. Assuming that all of the employed metrics are sound, this implies that alignment with human perception may best be thought of as a multidimensional concept, with different methods measuring fundamentally different aspects. Our results underline the importance of integrative benchmarking, but also raise questions about how to correctly combine and aggregate individual metrics. Aggregating by taking the arithmetic average, as done in Brain-Score, leads to the overall performance currently being dominated by behavior (95.25% explained variance) while the neural predictivity plays a less important role (only 33.33% explained variance). As a first step towards making sure that different alignment metrics all contribute fairly towards an integrative benchmark score, we therefore conclude by comparing three different aggregation options."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:NhqRSupF_l8C",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=cHlKB28bjV",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:x3jWoDpdgwkJ:scholar.google.com/",
        "cites_per_year": {}
    }
]