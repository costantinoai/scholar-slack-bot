[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Neither hype nor gloom do DNNs justice",
            "pub_year": 2023,
            "citation": "Behavioral and Brain Sciences 46, e412, 2023",
            "author": "Felix A Wichmann and Simon Kornblith and Robert Geirhos",
            "journal": "Behavioral and Brain Sciences",
            "volume": "46",
            "pages": "e412",
            "publisher": "Cambridge University Press",
            "abstract": "Neither the hype exemplified in some exaggerated claims about deep neural networks (DNNs), nor the gloom expressed by Bowers et al. do DNNs as models in vision science justice: DNNs rapidly evolve, and today's limitations are often tomorrow's successes. In addition, providing explanations as well as prediction and image-computability are model desiderata; one should not be favoured at the expense of the other."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:_xSYboBqXhAC",
        "num_citations": 0,
        "pub_url": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/neither-hype-nor-gloom-do-dnns-justice/639AA5BC7F6E3B91E9B9EC8463D39F77",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Irren ist menschlich: Aber was, wenn Maschinen Fehler machen?",
            "pub_year": 2023,
            "citation": "Outstanding Dissertations @ Gesellschaft f\u00fcr Informatik eV, 2023",
            "author": "Robert Geirhos",
            "publisher": "Outstanding Dissertations @ Gesellschaft f\u00fcr Informatik eV",
            "abstract": "Seit Jahrzehnten wird unsere Gesellschaft immer sta \u0308rker durch Informatik gepr\u00e4gt. Diese Entwicklung wird sich auch in Zukunft fortsetzen, nicht zuletzt durch den Einsatz von tiefen neuronalen Netzen, einem Teilgebiet der k\u00fcnstlichen Intelligenz. Wenn Maschinen mehr und mehr Entscheidungen treffen, hei\u00dft dies jedoch nicht, dass diese Entscheidungen auch f\u00fcr Menschen verst\u00e4ndlich sind-ganz im Gegenteil. Das birgt das Risiko, dass das Tempo von Anwendungen schneller steigt als unser Verst\u00e4ndnis anw\u00e4chst. Um dieser Entwicklung entgegenzutreten und maschinelle Entscheidungsprozesse besser zu verstehen, habe ich Methoden entwickelt, mit denen Menschen und Algorithmen des maschinellen Lernens verglichen werden k\u00f6nnen. Am Beispiel der Objekterkennung zeigen sich einerseits fundamentale Unterschiede zwischen den beiden, andererseits aber auch M\u00f6glichkeiten, Unterschiede zu verringern. Irren ist menschlich-doch auch Maschinen machen Fehler. Umso wichtiger ist es, zu verstehen, wann und warum. Nur so kann sichergestellt werden, dass k\u00fcnstliche Intelligenz eine Bereicherung f\u00fcr unsere Gesellschaft ist."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:bFI3QPDXJZMC",
        "num_citations": 0,
        "pub_url": "https://dl.gi.de/items/c3544f3d-0308-4c0a-ab89-c8118e3b1133",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Scaling vision transformers to 22 billion parameters",
            "pub_year": 2023,
            "citation": "Oral @ International Conference on Machine Learning (ICML 2023), 2023",
            "author": "Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Patrick Collier and Alexey Gritsenko and Vighnesh Birodkar and Cristina Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Paveti\u0107 and Dustin Tran and Thomas Kipf and Mario Lu\u010di\u0107 and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby",
            "conference": "Oral @ International Conference on Machine Learning (ICML 2023)",
            "abstract": "The scaling of Transformers has driven breakthrough capabilities for language models. At present, the largest large language models (LLMs) contain upwards of 100B parameters. Vision Transformers (ViT) have introduced the same architecture to image and video modelling, but these have not yet been successfully scaled to nearly the same degree; the largest dense ViT contains 4B parameters (Chen et al., 2022). We present a recipe for highly efficient and stable training of a 22B-parameter ViT (ViT-22B) and perform a wide variety of experiments on the resulting model. When evaluated on downstream tasks (often with a lightweight linear model on frozen features), ViT-22B demonstrates increasing performance with scale. We further observe other interesting benefits of scale, including an improved tradeoff between fairness and performance, state-of-the-art alignment to human visual perception in terms of shape/texture bias, and improved robustness. ViT-22B demonstrates the potential for\" LLM-like\" scaling in vision, and provides key steps towards getting there."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:2P1L_qKh6hAC",
        "num_citations": 114,
        "citedby_url": "/scholar?hl=en&cites=10337440426415537279",
        "cites_id": [
            "10337440426415537279"
        ],
        "pub_url": "https://proceedings.mlr.press/v202/dehghani23a.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:f7xCN0z3dY8J:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 113
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The developmental trajectory of object recognition robustness: Children are like small adults but unlike big deep neural networks",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (7), 4-4, 2023",
            "author": "Lukas S Huber and Robert Geirhos and Felix A Wichmann",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "7",
            "pages": "4-4",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "In laboratory object recognition tasks based on undistorted photographs, both adult humans and Deep Neural Networks (DNNs) perform close to ceiling. Unlike adults', whose object recognition performance is robust against a wide range of image distortions, DNNs trained on standard ImageNet (1.3M images) perform poorly on distorted images. However, the last two years have seen impressive gains in DNN distortion robustness, predominantly achieved through ever-increasing large-scale datasets$\\unicode{x2014}$orders of magnitude larger than ImageNet. While this simple brute-force approach is very effective in achieving human-level robustness in DNNs, it raises the question of whether human robustness, too, is simply due to extensive experience with (distorted) visual input during childhood and beyond. Here we investigate this question by comparing the core object recognition performance of 146 children (aged 4$\\unicode{x2013}$15) against adults and against DNNs. We find, first, that already 4$\\unicode{x2013}$6 year-olds showed remarkable robustness to image distortions and outperform DNNs trained on ImageNet. Second, we estimated the number of $\\unicode{x201C}$images$\\unicode{x201D}$ children have been exposed to during their lifetime. Compared to various DNNs, children's high robustness requires relatively little data. Third, when recognizing objects children$\\unicode{x2014}$like adults but unlike DNNs$\\unicode{x2014}$rely heavily on shape but not on texture cues. Together our results suggest that the remarkable robustness to distortions emerges early in the developmental trajectory of human object \u2026"
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:zA6iFVUQeVQC",
        "num_citations": 10,
        "citedby_url": "/scholar?hl=en&cites=10650409145062585026,1703505891390463570,1426305078765597264",
        "cites_id": [
            "10650409145062585026",
            "1703505891390463570",
            "1426305078765597264"
        ],
        "pub_url": "https://arxiv.org/abs/2205.10144",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:wvIGbLDazZMJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 3,
            "2023": 7
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Are Deep Neural Networks Adequate Behavioral Models of Human Visual Perception?",
            "pub_year": 2023,
            "citation": "Annual Review of Vision Science 9, 2023",
            "author": "Felix A Wichmann and Robert Geirhos",
            "volume": "9",
            "publisher": "Annual Reviews",
            "abstract": "Deep neural networks (DNNs) are machine learning algorithms that have revolutionized computer vision due to their remarkable successes in tasks like object classification and segmentation. The success of DNNs as computer vision algorithms has led to the suggestion that DNNs may also be good models of human visual perception. In this article, we review evidence regarding current DNNs as adequate behavioral models of human core object recognition. To this end, we argue that it is important to distinguish between statistical tools and computational models and to understand model quality as a multidimensional concept in which clarity about modeling goals is key. Reviewing a large number of psychophysical and computational explorations of core object recognition performance in humans and DNNs, we argue that DNNs are highly valuable scientific tools but that, as of today, DNNs should only be \u2026"
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:ldfaerwXgEUC",
        "num_citations": 10,
        "citedby_url": "/scholar?hl=en&cites=8870502238731100164",
        "cites_id": [
            "8870502238731100164"
        ],
        "pub_url": "https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-120522-031739",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:BODfxslaGnsJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 9
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Patch n'Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution",
            "pub_year": 2023,
            "citation": "Advances in Neural Information Processing Systems 36 (NeurIPS 2023), 2023",
            "author": "Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey Gritsenko and Mario Lu\u010di\u0107 and Neil Houlsby",
            "conference": "Advances in Neural Information Processing Systems 36 (NeurIPS 2023)",
            "abstract": "The ubiquitous and demonstrably suboptimal choice of resizing images to a fixed resolution before processing them with computer vision models has not yet been successfully challenged. However, models such as the Vision Transformer (ViT) offer flexible sequence-based modeling, and hence varying input sequence lengths. We take advantage of this with NaViT (Native Resolution ViT) which uses sequence packing during training to process inputs of arbitrary resolutions and aspect ratios. Alongside flexible model usage, we demonstrate improved training efficiency for large-scale supervised and contrastive image-text pretraining. NaViT can be efficiently transferred to standard tasks such as image and video classification, object detection, and semantic segmentation and leads to improved results on robustness and fairness benchmarks. At inference time, the input resolution flexibility can be used to smoothly navigate the test-time cost-performance trade-off. We believe that NaViT marks a departure from the standard, CNN-designed, input and modelling pipeline used by most computer vision models, and represents a promising direction for ViTs."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:pqnbT2bcN3wC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=2081084367415143760",
        "cites_id": [
            "2081084367415143760"
        ],
        "pub_url": "https://arxiv.org/abs/2307.06304",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Don't trust your eyes: on the (un) reliability of feature visualizations",
            "pub_year": 2023,
            "citation": "New Frontiers in Adversarial Machine Learning Workshop (ICML 2023), 2023",
            "author": "Robert Geirhos and Roland S Zimmermann and Blair Bilodeau and Wieland Brendel and Been Kim",
            "journal": "New Frontiers in Adversarial Machine Learning Workshop (ICML 2023)",
            "abstract": "How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to \"explain\" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:g5m5HwL7SMYC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=13494620787600136581",
        "cites_id": [
            "13494620787600136581"
        ],
        "pub_url": "https://arxiv.org/abs/2306.04719",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hRmC1xuGRrsJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Intriguing properties of generative classifiers",
            "pub_year": 2023,
            "citation": "Oral @ OOD-CV Workshop (ICCV 2023), 2023",
            "author": "Priyank Jaini and Kevin Clark and Robert Geirhos",
            "journal": "Oral @ OOD-CV Workshop (ICCV 2023)",
            "abstract": "What is the best paradigm to recognize objects -- discriminative inference (fast but potentially prone to shortcut learning) or using a generative model (slow but potentially more robust)? We build on recent advances in generative modeling that turn text-to-image models into classifiers. This allows us to study their behavior and to compare them against discriminative models and human psychophysical data. We report four intriguing emergent properties of generative classifiers: they show a record-breaking human-like shape bias (99% for Imagen), near human-level out-of-distribution accuracy, state-of-the-art alignment with human classification errors, and they understand certain perceptual illusions. Our results indicate that while the current dominant paradigm for modeling human object recognition is discriminative inference, zero-shot generative models approximate human object recognition data surprisingly well."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:u_35RYKgDlwC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=7271744308070275417",
        "cites_id": [
            "7271744308070275417"
        ],
        "pub_url": "https://arxiv.org/abs/2309.16779",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Getting aligned on representational alignment",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2310.13018, 2023",
            "author": "Ilia Sucholutsky and Lukas Muttenthaler and Adrian Weller and Andi Peng and Andreea Bobu and Been Kim and Bradley C Love and Erin Grant and Jascha Achterberg and Joshua B Tenenbaum and Katherine M Collins and Katherine L Hermann and Kerem Oktar and Klaus Greff and Martin N Hebart and Nori Jacoby and Raja Marjieh and Robert Geirhos and Sherol Chen and Simon Kornblith and Sunayana Rane and Talia Konkle and Thomas P O'Connell and Thomas Unterthiner and Andrew K Lampinen and Klaus-Robert M\u00fcller and Mariya Toneva and Thomas L Griffiths",
            "journal": "arXiv preprint arXiv:2310.13018",
            "abstract": "Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \\textbf{\\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from the fields of cognitive science, neuroscience, and machine learning, and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions."
        },
        "filled": true,
        "author_pub_id": "w3kGtMIAAAAJ:4OULZ7Gr8RgC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2310.13018",
        "cites_per_year": {}
    }
]