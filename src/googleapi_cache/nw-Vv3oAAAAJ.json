[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Modeling dynamic social vision highlights gaps between deep learning and humans",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Kathy Garcia and Emalie McMahon and Colin Conwell and Michael F Bonner and Leyla Isik",
            "publisher": "OSF",
            "abstract": "Deep learning models trained on computer vision tasks are widely considered the most successful models of human vision to date. The majority of work that supports this idea evaluates how accurately these models predict brain and behavioral responses to static images of objects and natural scenes. Real-world vision, however, is highly dynamic, and far less work has focused on evaluating the accuracy of deep learning models in predicting responses to stimuli that move, and that involve more complicated, higher-order phenomena like social interactions. Here, we present a dataset of natural videos and captions involving complex multi-agent interactions, and we benchmark 350+ image, video, and language models on behavioral and neural responses to the videos. As with prior work, we find that many vision models reach the noise ceiling in predicting visual scene features and responses along the ventral visual stream (often considered the primary neural substrate of object and scene recognition). In contrast, image models poorly predict human action and social interaction ratings and neural responses in the lateral stream (a neural pathway increasingly theorized as specializing in dynamic, social vision). Language models (given human sentence captions of the videos) predict action and social ratings better than either image or video models, but they still perform poorly at predicting neural responses in the lateral stream. Together these results identify a major gap in AI\u2019s ability to match human social vision and highlight the importance of studying vision in dynamic, natural contexts."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:D03iK_w7-QYC",
        "num_citations": 0,
        "pub_url": "https://osf.io/4mpd9/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uZSPgUvyUNYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Multidimensional neural representations of social features during movie viewing.",
            "pub_year": 2024,
            "citation": "Social Cognitive and Affective Neuroscience 19 (1), nsae030-nsae030, 2024",
            "author": "L Chang and L Isik",
            "journal": "Social Cognitive and Affective Neuroscience",
            "volume": "19",
            "number": "1",
            "pages": "nsae030-nsae030",
            "abstract": "The social world is dynamic and contextually embedded. Yet, most studies utilize simple stimuli that do not capture the complexity of everyday social episodes. To address this, we implemented a movie viewing paradigm and investigated how everyday social episodes are processed in the brain. Participants watched one of two movies during an MRI scan. Neural patterns from brain regions involved in social perception, mentalization, action observation and sensory processing were extracted. Representational similarity analysis results revealed that several labeled social features (including social interaction, mentalization, the actions of others, characters talking about themselves, talking about others and talking about objects) were represented in the superior temporal gyrus (STG) and middle temporal gyrus (MTG). The mentalization feature was also represented throughout the theory of mind network, and characters talking about others engaged the temporoparietal junction (TPJ), suggesting that listeners may spontaneously infer the mental state of those being talked about. In contrast, we did not observe the action representations in the frontoparietal regions of the action observation network. The current findings indicate that STG and MTG serve as key regions for social processing, and that listening to characters talk about others elicits spontaneous mental state inference in TPJ during natural movie viewing."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:g5m5HwL7SMYC",
        "num_citations": 0,
        "pub_url": "https://europepmc.org/article/pmc/pmc11130526",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VzamigWfdVgJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Social interaction processing in infants",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Jamie Soeun Park and Lindsey Powell and Brandon M Woo and Leyla Isik and Lauren Marie Smith",
            "publisher": "OSF",
            "abstract": "Recognizing and attending to social interactions is important for social development. Infants show increased attention to social interactions toward the end of the first year (Thiele et al., 2021), but there is little neural evidence regarding the development of social interaction processing in the brain. In adults, a region of the posterior superior temporal sulcus (STS) is functionally specialized for processing social interactions relative to independent actions (Isik et al., 2017). This region responds both to naturalistic video of human interaction and simplified, abstract representations of interactions that retain relational features. We will be investigating if there is a functionally specialized region for processing social interactions in infant STS. Additionally, if such a region exists, is it initially responsive to 1) concrete, naturalistic features of interacting people, 2) abstract, relational features, or 3) both? This study investigates 7.0-to 9.0-month-old-infants\u2019 brain activity while observing social interactions and independent actions. Infants will be fitted with a fNIRS cap to measure activation in several brain regions, including the left and right superior temporal sulcus, medial prefrontal cortex, and right MT and anterior temporal lobe. To examine what drives activation in any regions selectively responsive to social interactions, we will also manipulate the features in the videos, to test if concrete or abstract features drive early specialized responses to social interactions."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:a0OBvERweLwC",
        "num_citations": 0,
        "pub_url": "https://osf.io/zsxe8/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ut8nbOxrQTkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Abstract social interaction representations along the lateral pathway",
            "pub_year": 2024,
            "citation": "Trends in Cognitive Sciences, 2024",
            "author": "Emalie McMahon and Leyla Isik",
            "publisher": "Elsevier",
            "abstract": "TICS 2552 No. of Pages 2'hindering'scenarios. Representations in motion-selective middle temporal area (MT)[6], on the other hand, do not generalize to novel scenarios, suggesting that goal compatibility is not confounded with motion congruency in these stimuli. Recent computational work has also provided a mechanism by which these abstract visual representations could be constructed to generalize across motion patterns [10].Finally, Papeo points out that the STS primarily responds to dynamic social content and asks how its representations can be considered abstract if they do not generalize across static and dynamic scenes [2]. Although this highlights the importance of motion in STS processing, we do not believe that it poses a major challenge to abstract representations in the STS because the overwhelming majority of real-world social interactions are dynamic and recognized based on motion cues."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:yD5IFk8b50cC",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(24)00073-1",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:WDP0qThlUB4J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Generative Adversarial Collaborations: A practical guide for conference organizers and participating scientists",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2402.12604, 2024",
            "author": "Gunnar Blohm and Benjamin Peters and Ralf Haefner and Leyla Isik and Nikolaus Kriegeskorte and Jennifer S Lieberman and Carlos R Ponce and Gemma Roig and Megan AK Peters",
            "journal": "arXiv preprint arXiv:2402.12604",
            "abstract": "Generative adversarial collaborations (GACs) are a form of formal teamwork between groups of scientists with diverging views. The goal of GACs is to identify and ultimately resolve the most important challenges, controversies, and exciting theoretical and empirical debates in a given research field. A GAC team would develop specific, agreed-upon avenues to resolve debates in order to move a field of research forward in a collaborative way. Such adversarial collaborations have many benefits and opportunities but also come with challenges. Here, we use our experience from (1) creating and running the GAC program for the Cognitive Computational Neuroscience (CCN) conference and (2) implementing and leading GACs on particular scientific problems to provide a practical guide for future GAC program organizers and leaders of individual GACs."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:rO6llkc54NcC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2402.12604",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uxFImCCXViEJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A shared neural code for perceiving and remembering social interactions in the human superior temporal sulcus",
            "pub_year": 2024,
            "citation": "Neuropsychologia, 108823, 2024",
            "author": "Haemy Lee Masson and Janice Chen and Leyla Isik",
            "journal": "Neuropsychologia",
            "pages": "108823",
            "publisher": "Pergamon",
            "abstract": "Recognizing and remembering social information is a crucial cognitive skill. Neural patterns in the superior temporal sulcus (STS) support our ability to perceive others' social interactions. However, despite the prominence of social interactions in memory, the neural basis of remembering social interactions is still unknown. To fill this gap, we investigated the brain mechanisms underlying memory of others' social interactions during free spoken recall of a naturalistic movie. By applying machine learning-based fMRI encoding analyses to densely labeled movie and recall data we found that a subset of the STS activity evoked by viewing social interactions predicted neural responses in not only held-out movie data, but also during memory recall. These results provide the first evidence that activity in the STS is reinstated in response to specific social content and that its reactivation underlies our ability to remember \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:3s1wT3WcHBgC",
        "num_citations": 0,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0028393224000381",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:491E_l-th3oJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The neurodevelopmental origins of seeing social interactions",
            "pub_year": 2024,
            "citation": "Trends in Cognitive Sciences, 2024",
            "author": "Emalie McMahon and Leyla Isik",
            "publisher": "Elsevier",
            "abstract": "], Grossmann argues that, in young children and non-human primates, third-party social interaction recognition is supported by top-down processing in the medial prefrontal cortex (mPFC). He suggests that top-down signals in the developing brain may be used to train neural systems in the superior temporal sulcus (STS), which, in adults, appears to process social interactions in a visual manner ["
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:ZHo1McVdvXMC",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(23)00307-8",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:l6oT4jk-amkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "How does the primate brain combine generative and discriminative computations in vision?",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2401.06005, 2024",
            "author": "Benjamin Peters and James J DiCarlo and Todd Gureckis and Ralf Haefner and Leyla Isik and Joshua Tenenbaum and Talia Konkle and Thomas Naselaris and Kimberly Stachenfeld and Zenna Tavares and Doris Tsao and Ilker Yildirim and Nikolaus Kriegeskorte",
            "journal": "arXiv preprint arXiv:2401.06005",
            "abstract": "Vision is widely understood as an inference problem. However, two contrasting conceptions of the inference process have each been influential in research on biological vision as well as the engineering of machine vision. The first emphasizes bottom-up signal flow, describing vision as a largely feedforward, discriminative inference process that filters and transforms the visual information to remove irrelevant variation and represent behaviorally relevant information in a format suitable for downstream functions of cognition and behavioral control. In this conception, vision is driven by the sensory data, and perception is direct because the processing proceeds from the data to the latent variables of interest. The notion of \"inference\" in this conception is that of the engineering literature on neural networks, where feedforward convolutional neural networks processing images are said to perform inference. The alternative conception is that of vision as an inference process in Helmholtz's sense, where the sensory evidence is evaluated in the context of a generative model of the causal processes giving rise to it. In this conception, vision inverts a generative model through an interrogation of the evidence in a process often thought to involve top-down predictions of sensory data to evaluate the likelihood of alternative hypotheses. The authors include scientists rooted in roughly equal numbers in each of the conceptions and motivated to overcome what might be a false dichotomy between them and engage the other perspective in the realm of theory and experiment. The primate brain employs an unknown algorithm that may combine the advantages of \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:HoB7MX3m0LUC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2401.06005",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "The neural development of social scene perception in young children",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Elizabeth Jiwon Im and Angira Shirahatti and Leyla Isik",
            "publisher": "PsyArXiv",
            "abstract": "From a young age, children have advanced social perceptual and reasoning abilities. However, the neural development of these abilities is still poorly understood due to the challenges of acquiring brain imaging data from young children with traditional neuroimaging methods. To address this gap, we used fMRI data collected while children and adults watched an engaging and socially rich movie to investigate how the cortical basis of social processing changes throughout development. We labeled the movie with visual and social features, including motion energy, presence of a face, presence of a social interaction, theory of mind (ToM) events, valence and arousal. Using a voxel-wise encoding model trained on these features, we find that models based on visual (motion energy) and social (faces, social interaction, ToM, valence, and arousal) features can both predict brain activity in children as young as three years old across the cortex, with particularly high predictivity in motion selective MT and the superior temporal sulcus (STS). Furthermore, models trained on individual social features showed that while representations for some social features, like ToM, develop throughout childhood, social interaction representations in the STS appear adult-like in even the youngest children. The current study, for the first time, links neural activity in children to specific social features in a natural movie and suggests social interaction perception is supported by early developing neural responses in the STS."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:pqnbT2bcN3wC",
        "num_citations": 0,
        "pub_url": "https://osf.io/preprints/psyarxiv/aqryd/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Prospective Learning: Principled Extrapolation to the Future",
            "pub_year": 2023,
            "citation": "Conference on Lifelong Learning Agents, 347-357, 2023",
            "author": "Ashwin De Silva and Rahul Ramesh and Lyle Ungar and Marshall Hussain Shuler and Noah J Cowan and Michael Platt and Chen Li and Leyla Isik and Seung-Eon Roh and Adam Charles and Archana Venkataraman and Brian Caffo and Javier J How and Justus M Kebschull and John W Krakauer and Maxim Bichuch and Kaleab Alemayehu Kinfu and Eva Yezerets and Dinesh Jayaraman and Jong M Shin and Soledad Villar and Ian Phillips and Carey E Priebe and Thomas Hartung and Michael I Miller and Jayanta Dey and Ningyuan Huang and Eric Eaton and Ralph Etienne-Cummings and Elizabeth L Ogburn and Randal Burns and Onyema Osuagwu and Brett Mensh and Alysson R Muotri and Julia Brown and Chris White and Weiwei Yang and Andrei A Rusu Timothy Verstynen and Konrad P Kording and Pratik Chaudhari and Joshua T Vogelstein",
            "conference": "Conference on Lifelong Learning Agents",
            "pages": "347-357",
            "publisher": "PMLR",
            "abstract": "Learning is a process which can update decision rules, based on past experience, such that future performance improves. Traditionally, machine learning is often evaluated under the assumption that the future will be identical to the past in distribution or change adversarially. But these assumptions can be either too optimistic or pessimistic for many problems in the real world. Real world scenarios evolve over multiple spatiotemporal scales with partially predictable dynamics. Here we reformulate the learning problem to one that centers around this idea of dynamic futures that are partially learnable. We conjecture that certain sequences of tasks are not retrospectively learnable (in which the data distribution is fixed), but are prospectively learnable (in which distributions may be dynamic), suggesting that prospective learning is more difficult in kind than retrospective learning. We argue that prospective learning more accurately characterizes many real world problems that (1) currently stymie existing artificial intelligence solutions and/or (2) lack adequate explanations for how natural intelligences solve them. Thus, studying prospective learning will lead to deeper insights and solutions to currently vexing challenges in both natural and artificial intelligences."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:NaGl4SEjCO4C",
        "num_citations": 0,
        "pub_url": "https://proceedings.mlr.press/v232/de-silva23a.html",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Multidimensional neural representations of social features during movie viewing",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023",
            "author": "Haemy Lee Masson and Lucy Chang and Leyla Isik",
            "journal": "bioRxiv",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "The social world is dynamic and contextually embedded. Yet, most studies utilize simple stimuli that do not capture the complexity of everyday social episodes. To address this, we implemented a movie viewing paradigm and investigated how the everyday social episodes are processed in the brain. Participants watched one of two movies during an MRI scan. Neural patterns from brain regions involved in social perception, mentalization, action observation, and sensory processing were extracted. Representational similarity analysis results revealed that several labeled social features (including social interaction, mentalization, the actions of others, characters talking about themselves, talking about others, and talking about objects) were represented in superior temporal gyrus (STG) and middle temporal gyrus (MTG). The mentalization feature was also represented throughout the theory of mind network, and \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:M05iB0D1s5AC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.11.22.568258",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Relational visual representations underlie human social interaction recognition",
            "pub_year": 2023,
            "citation": "Nature Communications 14 (1), 7317, 2023",
            "author": "Manasi Malik and Leyla Isik",
            "journal": "Nature Communications",
            "volume": "14",
            "number": "1",
            "pages": "7317",
            "publisher": "Nature Publishing Group UK",
            "abstract": "Humans effortlessly recognize social interactions from visual input. Attempts to model this ability have typically relied on generative inverse planning models, which make predictions by inverting a generative model of agents\u2019 interactions based on their inferred goals, suggesting humans use a similar process of mental inference to recognize interactions. However, growing behavioral and neuroscience evidence suggests that recognizing social interactions is a visual process, separate from complex mental state inference. Yet despite their success in other domains, visual neural network models have been unable to reproduce human-like interaction recognition. We hypothesize that humans rely on relational visual information in particular, and develop a relational, graph neural network model, SocialGNN. Unlike prior models, SocialGNN accurately predicts human interaction judgments across both animated and \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:ldfaerwXgEUC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=8229960487833301031,13231645132255288957,14061505396373524982",
        "cites_id": [
            "8229960487833301031",
            "13231645132255288957",
            "14061505396373524982"
        ],
        "pub_url": "https://www.nature.com/articles/s41467-023-43156-8",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:fV4aviQ_oLcJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Disentangled deep generative models reveal coding principles of the human face processing network",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 15.528489, 2023",
            "author": "Paul Soulos and Leyla Isik",
            "journal": "bioRxiv",
            "pages": "2023.02. 15.528489",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Despite decades of research, much is still unknown about the computations carried out in the human face processing network. Recently deep networks have been proposed as a computational account of human visual processing, but while they provide a good match to neural data throughout visual cortex, they lack interpretability. We introduce a method for interpreting brain activity using a new class of deep generative models, disentangled representation learning models, which learn a low-dimensional latent space that \u201cdisentangles\u201d different semantically meaningful dimensions of faces, such as rotation, lighting, or hairstyle, in an unsupervised manner by enforcing statistical independence between dimensions. We find that the majority of our model\u2019s learned latent dimensions are interpretable by human raters. Further, these latent dimensions serve as a good encoding model for human fMRI data. We next investigated the representation of different latent dimensions across face-selective voxels. We find a gradient from low- to high-level face feature representations along posterior to anterior face-selective regions, corroborating prior models of human face recognition. Interestingly, though, we find no spatial segregation between identity-relevant and irrelevant face features. Finally, we provide new insight into the few \u201centangled\u201d (uninterpretable) dimensions in our model by showing that they match responses across the ventral stream and carry significant information about facial identity. Disentangled face encoding models provide an exciting alternative to standard \u201cblack box\u201d deep learning approaches for modeling and interpreting human \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:NMxIlDl6LWMC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=13296271178315983779,17731731228263384890",
        "cites_id": [
            "13296271178315983779",
            "17731731228263384890"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.15.528489.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:o-vJpDDYhbgJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Hierarchical organization of social action features along the lateral visual pathway",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Emalie McMahon and Michael F Bonner and Leyla Isik",
            "publisher": "PsyArXiv",
            "abstract": "Recent theoretical work has argued that in addition to the classical ventral (what) and dorsal (where/how) visual streams, there is a third visual stream on the lateral surface of the brain specialized for processing social information. Like visual representations in the ventral and dorsal streams, representations in the lateral stream are thought to be hierarchically organized. However, no prior studies have comprehensively investigated the organization of naturalistic, social visual content in the lateral stream. Here we used a data-rich approach to test the hypothesis that social action information is extracted hierarchically along this pathway. Social scenes pose several computational challenges beyond those of object recognition: they are often highly dynamic and involve relations between two or more people. To address these challenges, we curated a naturalistic stimulus set of 250 three-second videos of two people engaged in everyday actions. Each clip was richly annotated for its low-level visual features, mid-level scene & object properties, visual social primitives (including the distance between people and the extent to which they were facing), and high-level information about social interactions and affective content. Using a condition-rich fMRI experiment and a within-subject encoding model approach, we found that low-level visual features are represented in early visual cortex (EVC) and middle temporal area (MT), mid-level visual-social features in extrastriate body area (EBA) and lateral occipital complex (LOC), and high-level social interaction information along the superior temporal sulcus (STS). Communicative interactions, in particular \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:YFjsv_pBGBYC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=16072428916932549311",
        "cites_id": [
            "16072428916932549311"
        ],
        "pub_url": "https://psyarxiv.com/x3avb/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vxp7z_G8DN8J:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Rapid processing of observed touch through social perceptual brain regions: an EEG-fMRI fusion study",
            "pub_year": 2023,
            "citation": "Journal of Neuroscience, 2023",
            "author": "Haemy Lee Masson and Leyla Isik",
            "journal": "Journal of Neuroscience",
            "publisher": "Society for Neuroscience",
            "abstract": "Seeing social touch triggers a strong social-affective response that involves multiple brain networks, including visual, social perceptual, and somatosensory systems. Previous studies have identified the specific functional role of each system, but little is known about the speed and directionality of the information flow. Is this information extracted via the social perceptual system or from simulation from somatosensory cortex? To address this, we examined the spatiotemporal neural processing of observed touch. Twenty-one human participants (7 males) watched 500 ms video clips showing social and non-social touch during EEG recording. Visual and social-affective features were rapidly extracted in the brain, beginning at 90 and 150 ms after video onset, respectively. Combining the EEG data with fMRI data from our prior study with the same stimuli reveals that neural information first arises in early visual cortex (EVC \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:GnPB-g6toBAC",
        "num_citations": 0,
        "pub_url": "https://www.jneurosci.org/content/early/2023/10/13/JNEUROSCI.0995-23.2023.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:BEMs7WNFzAwJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Seeing social interactions",
            "pub_year": 2023,
            "citation": "Trends in Cognitive Sciences, 2023",
            "author": "Emalie McMahon and Leyla Isik",
            "publisher": "Elsevier",
            "abstract": "Seeing the interactions between other people is a critical part of our everyday visual experience, but recognizing the social interactions of others is often considered outside the scope of vision and grouped with higher-level social cognition like theory of mind. Recent work, however, has revealed that recognition of social interactions is efficient and automatic, is well modeled by bottom-up computational algorithms, and occurs in visually-selective regions of the brain. We review recent evidence from these three methodologies (behavioral, computational, and neural) that converge to suggest the core of social interaction perception is visual. We propose a computational framework for how this process is carried out in the brain and offer directions for future interdisciplinary investigations of social perception."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:35N4QoGY0k4C",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(23)00248-6",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Lateralization of dynamic social interaction perception",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5521-5521, 2023",
            "author": "Hannah Small and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5521-5521",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Social perception emerges early, occurs automatically, and is used ubiquitously in daily life. Understanding its neural underpinnings is critical to cognitive neuroscience. A region in the right posterior superior temporal sulcus (STS) that selectively supports social interaction perception has been found by contrasting brain responses to interacting and non-interacting point light displays across subjects. However, in naturalistic stimuli, both the left and right STS support social interaction perception. Are the left and right regions performing similar computations? In this work, we combined univariate and multivariate fMRI analyses to investigate the relationship between the left and right STS. We used point light displays to localize social interaction perception selective voxels in the STS (SI-STS) and examined their response to videos of two simple shapes engaged in helping or hindering actions. Despite prior reports of \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:vV6vV6tmYwMC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791886",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Observed social touch is processed in a rapid, feedforward manner: an EEG-fMRI fusion study",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4754-4754, 2023",
            "author": "Haemy Lee Masson and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4754-4754",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Observing social touch evokes a strong social-affective response. Our ability to extract the social-affective meaning of observed touch is supported by enhanced communication between brain networks, including social brain regions and somatosensory cortex. Yet, the direction of information flow across these networks and the overall neural dynamics of these processes remain unknown. The current study uses electroencephalography (EEG) to uncover how representations unfold spatial-temporally in the brain during touch observation. Twenty participants watched 500 ms video clips showing social and non-social touch during EEG recording. Representational similarity analysis reveals that EEG neural patterns are explained by visual features beginning at 90 ms post video onset. Social-affective features are processed shortly after, explaining neural patterns beginning at 150 ms. Next, we tracked the spatial \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:lSLTfruPkqcC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791745",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Both Purely Visual and Simulation-based Models Uniquely Explain Human Social Interaction Judgements",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5111-5111, 2023",
            "author": "Manasi Malik and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5111-5111",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Humans are very adept at detecting and recognizing social interactions. However, the underlying computations that enable us to extract social information from visual scenes are still largely unknown. One theory proposes that humans recognize social relationships by simulating the inferred goals of others, and has been instantiated using generative inverse planning models. In contrast, recent behavioral and neural evidence has suggested that social interaction perception is a bottom-up, visual process separate from complex mental simulation. Relatedly, recent work has found that a purely visual model with relational inductive biases can successfully model human social interaction judgments, lending computational support to this bottom-up theory. To directly compare these two alternatives, we look at the relationship between our purely visual model (SocialGNN), and a generative inverse planning model \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:RYcK_YlVTxYC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792260",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The spatiotemporal dynamics of social scene perception in the human brain",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4628-4628, 2023",
            "author": "Emalie McMahon and Taylor Abel and Jorge Gonzalez-Martinez and Michael F Bonner and Avniel Ghuman and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4628-4628",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Social perception is an important part of everyday life that develops early and is shared with non-human primates. To understand the spatiotemporal dynamics of naturalistic social perception in the human brain, we first curated a dataset of 250 500-ms video clips of two people performing everyday actions. We densely labeled these videos with features of visual social scene, including scene and object features, visual social primitives, and higher-level social/affective features. To investigate when and where these features are represented in the brain, patients with implanted stereoelectroencephalography electrodes viewed the videos. We used time-resolved encoding models in individual channels to investigate the time course of representations across the human brain. We find that an encoding model based on all of our social scene features predicts responses in a subset of channels around 400 ms after video \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:J_g5lzvAfSwC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791837",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A data-driven investigation of human action representations",
            "pub_year": 2023,
            "citation": "Scientific Reports 13 (1), 5171, 2023",
            "author": "Diana C Dima and Martin N Hebart and Leyla Isik",
            "journal": "Scientific Reports",
            "volume": "13",
            "number": "1",
            "pages": "5171",
            "publisher": "Nature Publishing Group UK",
            "abstract": "Understanding actions performed by others requires us to integrate different types of information about people, scenes, objects, and their interactions. What organizing dimensions does the mind use to make sense of this complex action space? To address this question, we collected intuitive similarity judgments across two large-scale sets of naturalistic videos depicting everyday actions. We used cross-validated sparse non-negative matrix factorization to identify the structure underlying action similarity judgments. A low-dimensional representation, consisting of nine to ten dimensions, was sufficient to accurately reconstruct human similarity judgments. The dimensions were robust to stimulus set perturbations and reproducible in a separate odd-one-out experiment. Human labels mapped these dimensions onto semantic axes relating to food, work, and home life; social axes relating to people and emotions; and one \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:BqipwSGYUEgC",
        "num_citations": 0,
        "pub_url": "https://www.nature.com/articles/s41598-023-32192-5",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZiMKje3GGn8J:scholar.google.com/",
        "cites_per_year": {}
    }
]