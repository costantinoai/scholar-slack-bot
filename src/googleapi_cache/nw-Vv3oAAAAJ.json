[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Disentangled deep generative models reveal coding principles of the human face processing network",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 15.528489, 2023",
            "author": "Paul Soulos and Leyla Isik",
            "journal": "bioRxiv",
            "pages": "2023.02. 15.528489",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Despite decades of research, much is still unknown about the computations carried out in the human face processing network. Recently deep networks have been proposed as a computational account of human visual processing, but while they provide a good match to neural data throughout visual cortex, they lack interpretability. We introduce a method for interpreting brain activity using a new class of deep generative models, disentangled representation learning models, which learn a low-dimensional latent space that \u201cdisentangles\u201d different semantically meaningful dimensions of faces, such as rotation, lighting, or hairstyle, in an unsupervised manner by enforcing statistical independence between dimensions. We find that the majority of our model\u2019s learned latent dimensions are interpretable by human raters. Further, these latent dimensions serve as a good encoding model for human fMRI data. We next investigated the representation of different latent dimensions across face-selective voxels. We find a gradient from low- to high-level face feature representations along posterior to anterior face-selective regions, corroborating prior models of human face recognition. Interestingly, though, we find no spatial segregation between identity-relevant and irrelevant face features. Finally, we provide new insight into the few \u201centangled\u201d (uninterpretable) dimensions in our model by showing that they match responses across the ventral stream and carry significant information about facial identity. Disentangled face encoding models provide an exciting alternative to standard \u201cblack box\u201d deep learning approaches for modeling and interpreting human \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:NMxIlDl6LWMC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=13296271178315983779,17731731228263384890",
        "cites_id": [
            "13296271178315983779",
            "17731731228263384890"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.15.528489.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:o-vJpDDYhbgJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Hierarchical organization of social action features along the lateral visual pathway",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Emalie McMahon and Michael F Bonner and Leyla Isik",
            "publisher": "PsyArXiv",
            "abstract": "Recent theoretical work has argued that in addition to the classical ventral (what) and dorsal (where/how) visual streams, there is a third visual stream on the lateral surface of the brain specialized for processing social information. Like visual representations in the ventral and dorsal streams, representations in the lateral stream are thought to be hierarchically organized. However, no prior studies have comprehensively investigated the organization of naturalistic, social visual content in the lateral stream. Here we used a data-rich approach to test the hypothesis that social action information is extracted hierarchically along this pathway. Social scenes pose several computational challenges beyond those of object recognition: they are often highly dynamic and involve relations between two or more people. To address these challenges, we curated a naturalistic stimulus set of 250 three-second videos of two people engaged in everyday actions. Each clip was richly annotated for its low-level visual features, mid-level scene & object properties, visual social primitives (including the distance between people and the extent to which they were facing), and high-level information about social interactions and affective content. Using a condition-rich fMRI experiment and a within-subject encoding model approach, we found that low-level visual features are represented in early visual cortex (EVC) and middle temporal area (MT), mid-level visual-social features in extrastriate body area (EBA) and lateral occipital complex (LOC), and high-level social interaction information along the superior temporal sulcus (STS). Communicative interactions, in particular \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:YFjsv_pBGBYC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=16072428916932549311",
        "cites_id": [
            "16072428916932549311"
        ],
        "pub_url": "https://psyarxiv.com/x3avb/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vxp7z_G8DN8J:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Seeing social interactions",
            "pub_year": 2023,
            "citation": "Trends in Cognitive Sciences, 2023",
            "author": "Emalie McMahon and Leyla Isik",
            "publisher": "Elsevier",
            "abstract": "Seeing the interactions between other people is a critical part of our everyday visual experience, but recognizing the social interactions of others is often considered outside the scope of vision and grouped with higher-level social cognition like theory of mind. Recent work, however, has revealed that recognition of social interactions is efficient and automatic, is well modeled by bottom-up computational algorithms, and occurs in visually-selective regions of the brain. We review recent evidence from these three methodologies (behavioral, computational, and neural) that converge to suggest the core of social interaction perception is visual. We propose a computational framework for how this process is carried out in the brain and offer directions for future interdisciplinary investigations of social perception."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:35N4QoGY0k4C",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(23)00248-6",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Lateralization of dynamic social interaction perception",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5521-5521, 2023",
            "author": "Hannah Small and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5521-5521",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Social perception emerges early, occurs automatically, and is used ubiquitously in daily life. Understanding its neural underpinnings is critical to cognitive neuroscience. A region in the right posterior superior temporal sulcus (STS) that selectively supports social interaction perception has been found by contrasting brain responses to interacting and non-interacting point light displays across subjects. However, in naturalistic stimuli, both the left and right STS support social interaction perception. Are the left and right regions performing similar computations? In this work, we combined univariate and multivariate fMRI analyses to investigate the relationship between the left and right STS. We used point light displays to localize social interaction perception selective voxels in the STS (SI-STS) and examined their response to videos of two simple shapes engaged in helping or hindering actions. Despite prior reports of \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:vV6vV6tmYwMC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791886",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Observed social touch is processed in a rapid, feedforward manner: an EEG-fMRI fusion study",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4754-4754, 2023",
            "author": "Haemy Lee Masson and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4754-4754",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Observing social touch evokes a strong social-affective response. Our ability to extract the social-affective meaning of observed touch is supported by enhanced communication between brain networks, including social brain regions and somatosensory cortex. Yet, the direction of information flow across these networks and the overall neural dynamics of these processes remain unknown. The current study uses electroencephalography (EEG) to uncover how representations unfold spatial-temporally in the brain during touch observation. Twenty participants watched 500 ms video clips showing social and non-social touch during EEG recording. Representational similarity analysis reveals that EEG neural patterns are explained by visual features beginning at 90 ms post video onset. Social-affective features are processed shortly after, explaining neural patterns beginning at 150 ms. Next, we tracked the spatial \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:lSLTfruPkqcC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791745",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Both Purely Visual and Simulation-based Models Uniquely Explain Human Social Interaction Judgements",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5111-5111, 2023",
            "author": "Manasi Malik and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5111-5111",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Humans are very adept at detecting and recognizing social interactions. However, the underlying computations that enable us to extract social information from visual scenes are still largely unknown. One theory proposes that humans recognize social relationships by simulating the inferred goals of others, and has been instantiated using generative inverse planning models. In contrast, recent behavioral and neural evidence has suggested that social interaction perception is a bottom-up, visual process separate from complex mental simulation. Relatedly, recent work has found that a purely visual model with relational inductive biases can successfully model human social interaction judgments, lending computational support to this bottom-up theory. To directly compare these two alternatives, we look at the relationship between our purely visual model (SocialGNN), and a generative inverse planning model \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:RYcK_YlVTxYC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792260",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The spatiotemporal dynamics of social scene perception in the human brain",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4628-4628, 2023",
            "author": "Emalie McMahon and Taylor Abel and Jorge Gonzalez-Martinez and Michael F Bonner and Avniel Ghuman and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4628-4628",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Social perception is an important part of everyday life that develops early and is shared with non-human primates. To understand the spatiotemporal dynamics of naturalistic social perception in the human brain, we first curated a dataset of 250 500-ms video clips of two people performing everyday actions. We densely labeled these videos with features of visual social scene, including scene and object features, visual social primitives, and higher-level social/affective features. To investigate when and where these features are represented in the brain, patients with implanted stereoelectroencephalography electrodes viewed the videos. We used time-resolved encoding models in individual channels to investigate the time course of representations across the human brain. We find that an encoding model based on all of our social scene features predicts responses in a subset of channels around 400 ms after video \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:J_g5lzvAfSwC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791837",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A data-driven investigation of human action representations",
            "pub_year": 2023,
            "citation": "Scientific Reports 13 (1), 5171, 2023",
            "author": "Diana C Dima and Martin N Hebart and Leyla Isik",
            "journal": "Scientific Reports",
            "volume": "13",
            "number": "1",
            "pages": "5171",
            "publisher": "Nature Publishing Group UK",
            "abstract": "Understanding actions performed by others requires us to integrate different types of information about people, scenes, objects, and their interactions. What organizing dimensions does the mind use to make sense of this complex action space? To address this question, we collected intuitive similarity judgments across two large-scale sets of naturalistic videos depicting everyday actions. We used cross-validated sparse non-negative matrix factorization to identify the structure underlying action similarity judgments. A low-dimensional representation, consisting of nine to ten dimensions, was sufficient to accurately reconstruct human similarity judgments. The dimensions were robust to stimulus set perturbations and reproducible in a separate odd-one-out experiment. Human labels mapped these dimensions onto semantic axes relating to food, work, and home life; social axes relating to people and emotions; and one \u2026"
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:BqipwSGYUEgC",
        "num_citations": 0,
        "pub_url": "https://www.nature.com/articles/s41598-023-32192-5",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZiMKje3GGn8J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Rapid processing of observed touch through social perceptual brain regions: an EEG-fMRI fusion study",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.05. 11.540376, 2023",
            "author": "Haemy Lee Masson and Leyla Isik",
            "journal": "bioRxiv",
            "pages": "2023.05. 11.540376",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Seeing social touch triggers a strong social-affective response that involves multiple brain networks, including visual, social perceptual, and somatosensory systems. Previous studies have identified the specific functional role of each system, but little is known about the speed and directionality of the information flow. Is this information extracted via the social perceptual system or from simulation from somatosensory cortex? To address this, we examined the spatiotemporal neural processing of observed touch. Twenty participants watched 500 ms video clips showing social and non-social touch during EEG recording. Visual and social-affective features were rapidly extracted in the brain, beginning at 90 and 150 ms after video onset, respectively. Combining the EEG data with fMRI data from our prior study with the same stimuli reveals that neural information first arises in early visual cortex (EVC), then in the temporoparietal junction and posterior superior temporal sulcus (TPJ/pSTS), and finally in the somatosensory cortex. EVC and TPJ/pSTS uniquely explain EEG neural patterns, while somatosensory cortex does not contribute to EEG patterns alone, suggesting that social-affective information may flow from TPJ/pSTS to somatosensory cortex. Together, these findings show that social touch is processed quickly, within the timeframe of feedforward visual processes, and that the social-affective meaning of touch is first extracted by a social perceptual pathway. Such rapid processing of social touch may be vital to its effective use during social interaction."
        },
        "filled": true,
        "author_pub_id": "nw-Vv3oAAAAJ:GnPB-g6toBAC",
        "num_citations": 0,
        "pub_url": "https://europepmc.org/article/ppr/ppr659726",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:BEMs7WNFzAwJ:scholar.google.com/",
        "cites_per_year": {}
    }
]