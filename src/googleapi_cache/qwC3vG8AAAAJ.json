[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A single computational objective drives specialization of streams in visual cortex",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.12. 19.572460, 2023",
            "author": "Dawn Finzi and Eshed Margalit and Kendrick Kay and Daniel LK Yamins and Kalanit Grill-Spector",
            "journal": "bioRxiv",
            "pages": "2023.12. 19.572460",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Human visual cortex is organized into dorsal, lateral, and ventral streams. A long-standing hypothesis is that the functional organization into streams emerged to support distinct visual behaviors. Here, we use a neural network-based computational model and a massive fMRI dataset to test how visual streams emerge. We find that models trained for stream-specific visual behaviors poorly capture neural responses and organization. Instead, a self-supervised Topographic Deep Artificial Neural Network, which encourages nearby units to respond similarly, successfully predicts neural responses, spatial segregation, and functional differentiation across streams. These findings challenge the prevailing view that streams evolved to separately support different behaviors, and suggest instead that functional organization arises from a single principle: balancing general representation learning with local spatial constraints."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:Y5dfb0dijaUC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.12.19.572460.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Physion++: Evaluating Physical Scene Understanding with Objects Consisting of Different Physical Attributes in Humans and Machines",
            "pub_year": 2023,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society, 2023",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Sirui Tao and Vedang Lad and Daniel Bear and Chuang Gan and Josh Tenenbaum and Daniel Yamins and Judith Fan and Kevin Smith",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "abstract": "Human physical scene understanding requires more than simply localizing and recognizing objects \u00d1 we can quickly adapt our predictions about how a scene will unfold by incorporating objects' latent physics properties, such as the masses of the objects in the scene. What are the underlying computational mechanisms that allow humans to infer these physical properties and adapt their physical predictions so efficiently from visual inputs? One hypothesis is that general intuitive physics knowledge can be learned from enough raw data, instantiated as computational models that predict future video frames in large datasets of complex scenes. To test this hypothesis, we evaluate existing state-of-the-art video models. We measured both model and human performance on Physion++, a novel dataset and benchmark that rigorously evaluates visual physical prediction in humans and machines, under circumstances where accurate physical prediction relies on accurate estimates of the latent physical properties of objects in the scene. Specifically, we tested scenarios where accurate prediction relied on accurate estimates of objects' mechanical properties, including masses, friction, elasticity and deformability, and the values of these mechanical properties could only be inferred by observing how these objects moved and interacted with other objects and/or fluids. We found that models that encode objectness and physical states tend to perform better, yet there is still a huge gap compared to human performance. We also found most models' predictions correlate poorly with that made by humans. These results show that current deep learning models that \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:eMMeJKvmdy0C",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/3x9960zn",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The Limiting Dynamics of SGD: Modified Loss, Phase-Space Oscillations, and Anomalous Diffusion",
            "pub_year": 2023,
            "citation": "Neural Computation, 1-25, 2023",
            "author": "Daniel Kunin and Javier Sagastuy-Brena and Lauren Gillespie and Eshed Margalit and Hidenori Tanaka and Surya Ganguli and Daniel LK Yamins",
            "journal": "Neural Computation",
            "pages": "1-25",
            "publisher": "MIT Press",
            "abstract": "In this work, we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance traveled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction among the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase-space dynamics of the parameters and their instantaneous \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:P5F9QuxV20EC",
        "num_citations": 8,
        "citedby_url": "/scholar?hl=en&cites=13126960835023986037",
        "cites_id": [
            "13126960835023986037"
        ],
        "pub_url": "https://direct.mit.edu/neco/article/doi/10.1162/neco_a_01626/118266",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:dd0RcFRVLLYJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 4,
            "2023": 4
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Counterfactual World Modeling for Physical Dynamics Understanding",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2312.06721, 2023",
            "author": "Rahul Venkatesh and Honglin Chen and Kevin Feigelis and Khaled Jedoui and Klemen Kotar and Felix Binder and Wanhee Lee and Sherry Liu and Kevin A Smith and Judith E Fan and Daniel LK Yamins",
            "journal": "arXiv preprint arXiv:2312.06721",
            "abstract": "The ability to understand physical dynamics is essential to learning agents acting in the world. This paper presents Counterfactual World Modeling (CWM), a candidate pure vision foundational model for physical dynamics understanding. CWM consists of three basic concepts. First, we propose a simple and powerful temporally-factored masking policy for masked prediction of video data, which encourages the model to learn disentangled representations of scene appearance and dynamics. Second, as a result of the factoring, CWM is capable of generating counterfactual next-frame predictions by manipulating a few patch embeddings to exert meaningful control over scene dynamics. Third, the counterfactual modeling capability enables the design of counterfactual queries to extract vision structures similar to keypoints, optical flows, and segmentations, which are useful for dynamics understanding. We show that zero-shot readouts of these structures extracted by the counterfactual queries attain competitive performance to prior methods on real-world datasets. Finally, we demonstrate that CWM achieves state-of-the-art performance on the challenging Physion benchmark for evaluating physical dynamics understanding."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:AXPGKjj_ei8C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2312.06721",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Let's move forward: Image-computable models and a common model evaluation scheme are prerequisites for a scientific understanding of human vision",
            "pub_year": 2023,
            "citation": "Behavioral and Brain Sciences 46, e390, 2023",
            "author": "James J DiCarlo and Daniel LK Yamins and Michael E Ferguson and Evelina Fedorenko and Matthias Bethge and Tyler Bonnen and Martin Schrimpf",
            "journal": "Behavioral and Brain Sciences",
            "volume": "46",
            "pages": "e390",
            "publisher": "Cambridge University Press",
            "abstract": "In the target article, Bowers et al. dispute deep artificial neural network (ANN) models as the currently leading models of human vision without producing alternatives. They eschew the use of public benchmarking platforms to compare vision models with the brain and behavior, and they advocate for a fragmented, phenomenon-specific modeling approach. These are unconstructive to scientific progress. We outline how the Brain-Score community is moving forward to add new model-to-human comparisons to its community-transparent suite of benchmarks."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:Mojj43d5GZwC",
        "num_citations": 0,
        "pub_url": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/lets-move-forward-imagecomputable-models-and-a-common-model-evaluation-scheme-are-prerequisites-for-a-scientific-understanding-of-human-vision/F2302912C8652DC2582F0E159C1BB6AB",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The BabyView camera: Designing a new head-mounted camera to capture children\u2019s early social and visual environments",
            "pub_year": 2023,
            "citation": "Behavior Research Methods, 1-12, 2023",
            "author": "Bria Long and Sarah Goodin and George Kachergis and Virginia A Marchman and Samaher F Radwan and Robert Z Sparks and Violet Xiang and Chengxu Zhuang and Oliver Hsu and Brett Newman and Daniel LK Yamins and Michael C Frank",
            "journal": "Behavior Research Methods",
            "pages": "1-12",
            "publisher": "Springer US",
            "abstract": "Head-mounted cameras have been used in developmental psychology research for more than a decade to provide a rich and comprehensive view of what infants see during their everyday experiences. However, variation between these devices has limited the field\u2019s ability to compare results across studies and across labs. Further, the video data captured by these cameras to date has been relatively low-resolution, limiting how well machine learning algorithms can operate over these rich video data. Here, we provide a well-tested and easily constructed design for a head-mounted camera assembly\u2014the BabyView\u2014developed in collaboration with Daylight Design, LLC., a professional product design firm. The BabyView collects high-resolution video, accelerometer, and gyroscope data from children approximately 6\u201330 months of age via a GoPro camera custom mounted on a soft child-safety helmet. The \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:08ZZubdj9fEC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=5355374867511737620",
        "cites_id": [
            "5355374867511737620"
        ],
        "pub_url": "https://link.springer.com/article/10.3758/s13428-023-02206-1",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:FH0XaREdUkoJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The BabyView camera: designing a new head-mounted camera to capture children\u2019s early social and visual environments",
            "pub_year": 2023,
            "citation": "Behavior Research Methods, 1-12, 2023",
            "author": "Bria Long and Sarah Goodin and George Kachergis and Virginia A Marchman and Samaher F Radwan and Robert Z Sparks and Violet Xiang and Chengxu Zhuang and Oliver Hsu and Brett Newman and Daniel LK Yamins and Michael C Frank",
            "journal": "Behavior Research Methods",
            "pages": "1-12",
            "publisher": "Springer US",
            "abstract": "Head-mounted cameras have been used in developmental psychology research for more than a decade to provide a rich and comprehensive view of what infants see during their everyday experiences. However, variation between these devices has limited the field\u2019s ability to compare results across studies and across labs. Further, the video data captured by these cameras to date has been relatively low-resolution, limiting how well machine learning algorithms can operate over these rich video data. Here, we provide a well-tested and easily constructed design for a head-mounted camera assembly\u2014the BabyView\u2014developed in collaboration with Daylight Design, LLC., a professional product design firm. The BabyView collects high-resolution video, accelerometer, and gyroscope data from children approximately 6\u201330 months of age via a GoPro camera custom mounted on a soft child-safety helmet. The \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:08ZZubdj9fEC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=5355374867511737620",
        "cites_id": [
            "5355374867511737620"
        ],
        "pub_url": "https://link.springer.com/article/10.3758/s13428-023-02206-1",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:FH0XaREdUkoJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A Unifying Principle for the Functional Organization of Visual Cortex",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.05. 18.541361, 2023",
            "author": "Eshed Margalit and Hyodong Lee and Dawn Finzi and James J DiCarlo and Kalanit Grill-Spector and Daniel LK Yamins",
            "journal": "bioRxiv",
            "pages": "2023.05. 18.541361",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "A key feature of many cortical systems is functional organization: the arrangement of neurons with specific functional properties in characteristic spatial patterns across the cortical surface. However, the principles underlying the emergence and utility of functional organization are poorly understood. Here we develop the Topographic Deep Artificial Neural Network (TDANN), the first unified model to accurately predict the functional organization of multiple cortical areas in the primate visual system. We analyze the key factors responsible for the TDANN's success and find that it strikes a balance between two specific objectives: achieving a task-general sensory representation that is self-supervised, and maximizing the smoothness of responses across the cortical sheet according to a metric that scales relative to cortical surface area. In turn, the representations learned by the TDANN are lower dimensional and more brain-like than those in models that lack a spatial smoothness constraint. Finally, we provide evidence that the TDANN's functional organization balances performance with inter-area connection length, and use the resulting models for a proof-of-principle optimization of cortical prosthetic design. Our results thus offer a unified principle for understanding functional organization and a novel view of the functional role of the visual system in particular."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:q3oQSFYPqjQC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=615012094074270269",
        "cites_id": [
            "615012094074270269"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.05.18.541361.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:PbLdtz32iAgJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Are These the Same Apple? Comparing Images Based on Object Intrinsics",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2311.00750, 2023",
            "author": "Klemen Kotar and Stephen Tian and Hong-Xing Yu and Daniel LK Yamins and Jiajun Wu",
            "journal": "arXiv preprint arXiv:2311.00750",
            "abstract": "The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of  images of  objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:HE397vMXCloC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2311.00750",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Evaluating physical scene understanding with objects consisting of different physical attributes in humans and machines",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5622-5622, 2023",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Daniel Bear and Chuang Gan and Joshua Tenenbaum and Daniel Yamins and Judith Fan and Kevin Smith",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5622-5622",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Human physical scene understanding requires more than simply localizing and recognizing objects\u2014we can quickly adapt our predictions about how a scene will unfold by incorporating objects' latent physics properties, such as the masses of the objects in the scene. What are the underlying computational mechanisms that allow humans to infer these physical properties and adapt their physical predictions so efficiently from visual inputs? One hypothesis is that general intuitive physics knowledge can be learned from enough raw data, instantiated as computational models that predict future video frames in large datasets of complex scenes. To test this hypothesis, we evaluated how well two state-of-the-art video models\u2014MCVD (Voleti et al., 2022) and ALOE (Ding et al., 2021)\u2014could approximate human-level physical scene understanding. We measured both model and human performance on Physion++, a \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:XiVPGOgt02cC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792645",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2306.15668, 2023",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Daniel Bear and Chuang Gan and Joshua B Tenenbaum and Daniel LK Yamins and Judith E Fan and Kevin A Smith",
            "journal": "arXiv preprint arXiv:2306.15668",
            "abstract": "General physical scene understanding requires more than simply localizing and recognizing objects -- it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, and where the values of those properties can only be inferred by observing how objects move and interact with other objects or fluids. We evaluate the performance of a number of state-of-the-art prediction models that span a variety of levels of learning vs. built-in knowledge, and compare that performance to a set of human predictions. We find that models that have been trained using standard regimes and datasets do not spontaneously learn to make inferences about latent properties, but also that models that encode objectness and physical states tend to make better predictions. However, there is still a huge gap between all models and human performance, and all models' \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:5ugPr518TE4C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2306.15668",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Unifying (Machine) Vision via Counterfactual World Modeling",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2306.01828, 2023",
            "author": "Daniel M Bear and Kevin Feigelis and Honglin Chen and Wanhee Lee and Rahul Venkatesh and Klemen Kotar and Alex Durango and Daniel LK Yamins",
            "journal": "arXiv preprint arXiv:2306.01828",
            "abstract": "Leading approaches in machine vision employ different architectures for different tasks, trained on costly task-specific labeled datasets. This complexity has held back progress in areas, such as robotics, where robust task-general perception remains a bottleneck. In contrast, \"foundation models\" of natural language have shown how large pre-trained neural networks can provide zero-shot solutions to a broad spectrum of apparently distinct tasks. Here we introduce Counterfactual World Modeling (CWM), a framework for constructing a visual foundation model: a unified, unsupervised network that can be prompted to perform a wide variety of visual computations. CWM has two key components, which resolve the core issues that have hindered application of the foundation model concept to vision. The first is structured masking, a generalization of masked prediction methods that encourages a prediction model to capture the low-dimensional structure in visual data. The model thereby factors the key physical components of a scene and exposes an interface to them via small sets of visual tokens. This in turn enables CWM's second main idea -- counterfactual prompting -- the observation that many apparently distinct visual representations can be computed, in a zero-shot manner, by comparing the prediction model's output on real inputs versus slightly modified (\"counterfactual\") inputs. We show that CWM generates high-quality readouts on real-world images and videos for a diversity of tasks, including estimation of keypoints, optical flow, occlusions, object segments, and relative depth. Taken together, our results show that CWM is a promising path \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:J-pR_7NvFogC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2306.01828",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_Br_hSX_2dgJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Measuring and Modeling Physical Intrinsic Motivation",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2305.13452, 2023",
            "author": "Julio Martinez and Felix Binder and Haoliang Wang and Nicker Haber and Judith Fan and Daniel LK Yamins",
            "journal": "arXiv preprint arXiv:2305.13452",
            "abstract": "Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:kRWSkSYxWN8C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2305.13452",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8QuZVygSmpMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Developmental Curiosity and Social Interaction in Virtual Agents",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2305.13396, 2023",
            "author": "Chris Doyle and Sarah Shader and Michelle Lau and Megumi Sano and Daniel LK Yamins and Nick Haber",
            "journal": "arXiv preprint arXiv:2305.13396",
            "abstract": "Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:mvPsJ3kp5DgC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2305.13396",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:XyLvMJA_IigJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Medial temporal cortex supports compositional visual inferences",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.09. 07.556737, 2023",
            "author": "Tyler Bonnen and Anthony D Wagner and Daniel LK Yamins",
            "journal": "bioRxiv",
            "pages": "2023.09. 07.556737",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Perception unfolds across multiple timescales. For humans and other primates, many object-centric visual attributes can be inferred 'at a glance' (i.e., with <200ms of visual information), an ability supported by ventral temporal cortex (VTC). Other perceptual inferences require more time; to determine a novel object's identity, we might need to represent its unique configuration of visual features, requiring multiple 'glances.' Here we evaluate whether medial temporal cortex (MTC), downstream from VTC, supports object perception by integrating over such visuospatial sequences. We first compare human visual inferences directly to electrophysiological recordings from macaque VTC. While human performance 'at a glance' is approximated by a linear readout of VTC, participants radically outperform VTC given longer viewing times (i.e.,  >200ms). Next, we demonstrate the causal role of MTC in these temporally extended visual inferences: just as time restricted performance can be approximated by a linear readout of VTC, the performance of (time unrestricted) MTC-lesioned humans resembles a computational proxy for VTC. Finally, we characterize these visual abilities through a series of eyetracking experiments. With extended viewing times participants sequentially sample task-relevant features via multiple saccades\u2014visuospatial patterns that are reliable across participants and necessary for performance. From these data, we suggest that MTC transforms visuospatial sequences into 'compositional' representations that support visual object perception."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:olpn-zPbct0C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.09.07.556737.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Advancing Cognitive Science and AI with Cognitive-AI Benchmarking",
            "pub_year": 2023,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society 45 (45), 2023",
            "author": "Felix Jedidja Binder and Logan Matthew Cross and Yoni Friedman and Robert Hawkins and Daniel LK Yamins and Judith E Fan",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "volume": "45",
            "number": "45",
            "abstract": "What are the current limits of AI models in explaining human cognition and behavior? How might approaches from the cognitive sciences drive the development of more robust and reliable AI systems? The goal of this workshop is bring together researchers across cognitive science and artificial intelligence (AI) to engage with these questions and identify opportunities to work together to advance progress in both fields. In particular, we propose Cognitive-AI Benchmarking as a particularly promising strategy --- that is, the community-coordinated establishment of common benchmarks, tools, and best practices for model-human comparisons across diverse and ecologically relevant domains and tasks. We will host a combination of talks, panel discussion, and breakout activities to: highlight past successes in Cognitive-AI Benchmarking and limitations of current approaches, share tools and best practices, and outline future challenges and goals for the field."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:bnK-pcrLprsC",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/5v56249j",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes",
            "pub_year": 2023,
            "citation": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \u2026, 2023",
            "author": "Haotian Xue and Antonio Torralba and Joshua Tenenbaum and Daniel Yamins and Yunzhu Li and Hsiao-Yu Tung",
            "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "3624-3634",
            "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:BrmTIyaxlBUC",
        "num_citations": 0,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Xue_3D-IntPhys_Towards_More_Generalized_3D-Grounded_Visual_Intuitive_Physics_Under_Challenging_CVPRW_2023_paper.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:V0m9MxS-jCMJ:scholar.google.com/",
        "cites_per_year": {}
    }
]