[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "3D-IntPhys: towards more generalized 3D-grounded visual intuitive physics under challenging scenes",
            "pub_year": 2024,
            "citation": "Advances in Neural Information Processing Systems 36, 2024",
            "author": "Haotian Xue and Antonio Torralba and Josh Tenenbaum and Dan Yamins and Yunzhu Li and Hsiao-Yu Tung",
            "journal": "Advances in Neural Information Processing Systems",
            "volume": "36",
            "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:BrmTIyaxlBUC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=10708783696087646423",
        "cites_id": [
            "10708783696087646423"
        ],
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/164687cb815daae754d33364716e65e6-Abstract-Conference.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:14DMtQg-nZQJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1,
            "2024": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Are these the same apple? comparing images based on object intrinsics",
            "pub_year": 2024,
            "citation": "Advances in Neural Information Processing Systems 36, 2024",
            "author": "Klemen Kotar and Stephen Tian and Hong-Xing Yu and Dan Yamins and Jiajun Wu",
            "journal": "Advances in Neural Information Processing Systems",
            "volume": "36",
            "abstract": "The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of 18, 000 images of 180 objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian. github. io/projects/cute/for visualizations of the data and demos of our \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:HE397vMXCloC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=13467557835464201948",
        "cites_id": [
            "13467557835464201948"
        ],
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/803c6ab3d62346e004ef70211d2d15b8-Abstract-Datasets_and_Benchmarks.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:3FbfRn9g5roJ:scholar.google.com/",
        "cites_per_year": {
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The BabyView dataset: High-resolution egocentric videos of infants' and young children's everyday experiences",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2406.10447, 2024",
            "author": "Bria Long and Violet Xiang and Stefan Stojanov and Robert Z Sparks and Zi Yin and Grace E Keene and Alvin WM Tan and Steven Y Feng and Chengxu Zhuang and Virginia A Marchman and Daniel LK Yamins and Michael C Frank",
            "journal": "arXiv preprint arXiv:2406.10447",
            "abstract": "Human children far exceed modern machine learning algorithms in their sample efficiency, achieving high performance in key domains with much less data than current models. This ''data gap'' is a key challenge both for building intelligent artificial systems and for understanding human development. Egocentric video capturing children's experience -- their ''training data'' -- is a key ingredient for comparison of humans and models and for the development of algorithmic innovations to bridge this gap. Yet there are few such datasets available, and extant data are low-resolution, have limited metadata, and importantly, represent only a small set of children's experiences. Here, we provide the first release of the largest developmental egocentric video dataset to date -- the BabyView dataset -- recorded using a high-resolution camera with a large vertical field-of-view and gyroscope/accelerometer data. This 493 hour dataset includes egocentric videos from children spanning 6 months - 5 years of age in both longitudinal, at-home contexts and in a preschool environment. We provide gold-standard annotations for the evaluation of speech transcription, speaker diarization, and human pose estimation, and evaluate models in each of these domains. We train self-supervised language and vision models and evaluate their transfer to out-of-distribution tasks including syntactic structure learning, object recognition, depth estimation, and image segmentation. Although performance in each scales with dataset size, overall performance is relatively lower than when models are trained on curated datasets, especially in the visual domain. Our dataset stands as an \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:N5tVd3kTz84C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2406.10447",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_F0haxmkfEMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Animate Agent World Modeling Benchmark",
            "pub_year": 2024,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society 46, 2024",
            "author": "Logan Matthew Cross and Violet Xiang and Nick Haber and Daniel Yamins",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "volume": "46",
            "abstract": "To advance the capacity of intuitive psychology in machines, we introduce the Animate Agent World Modeling Benchmark. This benchmark features agents engaged in a diverse repertoire of behaviors, such as goal-directed interactions with objects and multi-agent interactions, all governed by realistic physics. Humans tend to predict the future based on expected events rather than simulating step-by-step. Thus, our benchmark includes a cognitively-inspired evaluation pipeline designed to assess whether the simulated trajectories of world models capture the correct sequences of events. To perform well, models need to leverage predictive cues from the observations to accurately simulate the goals of animate agents over long horizons. We demonstrate that current state-of-the-art models perform poorly in our evaluations. A hierarchical oracle model sets an upper bound for performance, suggesting that to excel, a model should scaffold their predictions with abstractions like goals that guide the simulation process towards relevant future events"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:PR6Y55bgFSsC",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/7r41x81m",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:AF0m716RccwJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Probabilistic simulation supports generalizable intuitive physics",
            "pub_year": 2024,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society 46, 2024",
            "author": "Haoliang Wang and Khaled Jedoui and Rahul Venkatesh and Felix Jedidja Binder and Josh Tenenbaum and Judith E Fan and Daniel Yamins and Kevin A Smith",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "volume": "46",
            "abstract": "How do people perform general-purpose physical reasoning across a variety of scenarios in everyday life? Across two studies with seven different physical scenarios, we asked participants to predict whether or where two objects will make contact. People achieved high accuracy and were highly consistent with each other in their predictions. We hypothesize that this robust generalization is a consequence of mental simulations of noisy physics. We designed an \"intuitive physics engine'' model to capture this generalizable simulation. We find that this model generalized in human-like ways to unseen stimuli and to a different query of predictions. We evaluated several state-of-the-art deep learning and scene feature models on the same task and found that they could not explain human predictions as well. This study provides evidence that human's robust generalization in physics predictions are supported by a probabilistic simulation model, and suggests the need for structure in learned dynamics models."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:LjlpjdlvIbIC",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/93j3f86q",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:7n1HVHWGPbMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A unifying framework for functional organization in early and higher ventral visual cortex",
            "pub_year": 2024,
            "citation": "Neuron, 2024",
            "author": "Eshed Margalit and Hyodong Lee and Dawn Finzi and James J DiCarlo and Kalanit Grill-Spector and Daniel LK Yamins",
            "journal": "Neuron",
            "publisher": "Elsevier",
            "abstract": "A key feature of cortical systems is functional organization: the arrangement of functionally distinct neurons in characteristic spatial patterns. However, the principles underlying the emergence of functional organization in the cortex are poorly understood. Here, we develop the topographic deep artificial neural network (TDANN), the first model to predict several aspects of the functional organization of multiple cortical areas in the primate visual system. We analyze the factors driving the TDANN's success and find that it balances two objectives: learning a task-general sensory representation and maximizing the spatial smoothness of responses according to a metric that scales with cortical surface area. In turn, the representations learned by the TDANN are more brain-like than in spatially unconstrained models. Finally, we provide evidence that the TDANN's functional organization balances performance with between \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:WqliGbK-hY8C",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/neuron/abstract/S0896-6273(24)00279-4",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:z1sG3uKMzgIJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Explanatory models in neuroscience, Part 1: Taking mechanistic abstraction seriously",
            "pub_year": 2024,
            "citation": "Cognitive Systems Research, 101244, 2024",
            "author": "Rosa Cao and Daniel Yamins",
            "journal": "Cognitive Systems Research",
            "pages": "101244",
            "publisher": "Elsevier",
            "abstract": "Despite the recent success of neural network models in mimicking animal performance on various tasks, critics worry that these models fail to illuminate brain function. We take it that a central approach to explanation in systems neuroscience is that of mechanistic modeling, where understanding the system requires us to characterize its parts, organization, and activities, and how those give rise to behaviors of interest. However, it remains controversial what it takes for a model to be mechanistic, and whether computational models such as neural networks qualify as explanatory on this approach.We argue that certain kinds of neural network models are actually good examples of mechanistic models, when an appropriate notion of mechanistic mapping is deployed. Building on existing work on model-to-mechanism mapping (3M), we describe criteria delineating such a notion, which we call 3M++. These criteria \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:a0OBvERweLwC",
        "num_citations": 33,
        "citedby_url": "/scholar?hl=en&cites=18157355135239908140",
        "cites_id": [
            "18157355135239908140"
        ],
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S138904172400038X",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:LOuWMEvi-_sJ:scholar.google.com/",
        "cites_per_year": {
            "2021": 2,
            "2022": 7,
            "2023": 14,
            "2024": 10
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Lessons learned in the study of representational alignment in physical reasoning",
            "pub_year": 2024,
            "citation": "ICLR 2024 Workshop on Representational Alignment, 2024",
            "author": "Felix Jedidja Binder and Rahul Mysore Venkatesh and Daniel LK Yamins and Judith E Fan",
            "conference": "ICLR 2024 Workshop on Representational Alignment",
            "abstract": "Recent developments allow AI systems to perform cognitively complex and rich tasks. At the same time, collecting human behavior at scale is more feasible than ever. This convergence of trends allows for the combined large-scale study of human and AI behavior in rich domains and tasks. Such experiments promise to provide better insight into the representations and strategies underlying both human and AI behavior. However, doing so in a way that does justice to both humans and AI systems is challenging. Here, we outline the key considerations and challenges we've faced in a benchmarking study investigating physical understanding across humans and AI systems and discuss how we've addressed them."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:eq2jaN3J8jMC",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=3DKVOYQFMk",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jss_mtk42VQJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Inter-animal transforms as a guide to model-brain comparison",
            "pub_year": 2024,
            "citation": "ICLR 2024 Workshop on Representational Alignment, 2024",
            "author": "Imran Thobani and Javier Sagastuy-Brena and Aran Nayebi and Rosa Cao and Daniel LK Yamins",
            "conference": "ICLR 2024 Workshop on Representational Alignment",
            "abstract": "A fundamental question for computational neuroscience is how to assess neural response similarity between a mechanistic model and the brain. We propose to map models to brains using the same set of transforms that map animal subjects to each other for the same species and brain area. We show that identifying a good transform class requires taking aspects of the mechanism underlying the brain responses into account, specifically the non-linear activation function. We therefore introduce a transform class, Inverse-Linear-Nonlinear-Poisson (ILNP), that accounts for the effect of the biological activation function. On an electro-physiological dataset of 31 mouse subjects, ILNP increases same-area similarity scores across subjects while maintaining inter-area separability compared to ridge regression and soft matching. We also find that a transform class of this kind better differentiates between various models of the mouse visual stream with respect to brain predictivity, though for some model comparisons, soft matching does better. We hypothesize that integrating some neuron-level tuning properties into the mechanistic constraints of ILNP is a promising next step in characterizing a good inter-animal transform class in order to better assess model accuracy."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:SdhP9T11ey4C",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=Kt7lEKuL4A",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:k6WXw0oWIiEJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle",
            "pub_year": 2024,
            "citation": "Cognitive Systems Research 85, 101200, 2024",
            "author": "Rosa Cao and Daniel Yamins",
            "journal": "Cognitive Systems Research",
            "volume": "85",
            "pages": "101200",
            "publisher": "Elsevier",
            "abstract": "Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally \u201ctop-down\u201d, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:_B80troHkn4C",
        "num_citations": 0,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1389041723001341",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:nvjJ8f40td8J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Physion++: Evaluating physical scene understanding that requires online inference of different physical properties",
            "pub_year": 2024,
            "citation": "Advances in Neural Information Processing Systems 36, 2024",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Daniel Bear and Chuang Gan and Josh Tenenbaum and Dan Yamins and Judith Fan and Kevin Smith",
            "journal": "Advances in Neural Information Processing Systems",
            "volume": "36",
            "abstract": "General physical scene understanding requires more than simply localizing and recognizing objects--it requires knowledge that objects can have different latent properties (eg, mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (eg, size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, and where the values of those properties can only be inferred by observing how objects move and interact with other objects or fluids. We evaluate the performance of a number of state-of-the-art prediction models that span a variety of levels of learning vs. built-in knowledge, and compare that performance to a set of human predictions. We find that models that have been trained using standard regimes and datasets do not spontaneously learn to make inferences about latent properties, but also that models that encode objectness and physical states tend to make better predictions. However, there is still a huge gap between all models and human performance, and all models' \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:5ugPr518TE4C",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=2183333906365352376",
        "cites_id": [
            "2183333906365352376"
        ],
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/d3e8011c912e651ab2a76e7935a1e464-Abstract-Datasets_and_Benchmarks.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uAn5up7CTB4J:scholar.google.com/",
        "cites_per_year": {
            "2023": 1,
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Let's move forward: Image-computable models and a common model evaluation scheme are prerequisites for a scientific understanding of human vision\u2013CORRIGENDUM",
            "pub_year": 2024,
            "citation": "Behavioral and Brain Sciences 47, e66, 2024",
            "author": "James J DiCarlo and Daniel LK Yamins and Michael E Ferguson and Evelina Fedorenko and Matthias Bethge and Tyler Bonnen and Martin Schrimpf",
            "journal": "Behavioral and Brain Sciences",
            "volume": "47",
            "pages": "e66",
            "publisher": "Cambridge University Press",
            "abstract": "DiCarlo, JJ, Yamins, DLK, Ferguson, ME, Fedorenko, E., Bethge, M., Bonnen, T., & Schrimpf, M.(2023). Let's move forward: Image-computable models and a common model evaluation scheme are prerequisites for a scientific understanding of human vision. Behavioral and Brain Sciences, 46, e390. Google Scholar"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:PELIpwtuRlgC",
        "num_citations": 0,
        "pub_url": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/lets-move-forward-imagecomputable-models-and-a-common-model-evaluation-scheme-are-prerequisites-for-a-scientific-understanding-of-human-vision-corrigendum/4CC1766D3C5337FD5F46B825AF74D597",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:R_Qj6o1IBjoJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "A single computational objective drives specialization of streams in visual cortex",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.12. 19.572460, 2023",
            "author": "Dawn Finzi and Eshed Margalit and Kendrick Kay and Daniel LK Yamins and Kalanit Grill-Spector",
            "journal": "bioRxiv",
            "pages": "2023.12. 19.572460",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Human visual cortex is organized into dorsal, lateral, and ventral streams. A long-standing hypothesis is that the functional organization into streams emerged to support distinct visual behaviors. Here, we use a neural network-based computational model and a massive fMRI dataset to test how visual streams emerge. We find that models trained for stream-specific visual behaviors poorly capture neural responses and organization. Instead, a self-supervised Topographic Deep Artificial Neural Network, which encourages nearby units to respond similarly, successfully predicts neural responses, spatial segregation, and functional differentiation across streams. These findings challenge the prevailing view that streams evolved to separately support different behaviors, and suggest instead that functional organization arises from a single principle: balancing general representation learning with local spatial constraints."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:Y5dfb0dijaUC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.12.19.572460.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Physion++: Evaluating Physical Scene Understanding with Objects Consisting of Different Physical Attributes in Humans and Machines",
            "pub_year": 2023,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society, 2023",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Sirui Tao and Vedang Lad and Daniel Bear and Chuang Gan and Josh Tenenbaum and Daniel Yamins and Judith Fan and Kevin Smith",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "abstract": "Human physical scene understanding requires more than simply localizing and recognizing objects \u00d1 we can quickly adapt our predictions about how a scene will unfold by incorporating objects' latent physics properties, such as the masses of the objects in the scene. What are the underlying computational mechanisms that allow humans to infer these physical properties and adapt their physical predictions so efficiently from visual inputs? One hypothesis is that general intuitive physics knowledge can be learned from enough raw data, instantiated as computational models that predict future video frames in large datasets of complex scenes. To test this hypothesis, we evaluate existing state-of-the-art video models. We measured both model and human performance on Physion++, a novel dataset and benchmark that rigorously evaluates visual physical prediction in humans and machines, under circumstances where accurate physical prediction relies on accurate estimates of the latent physical properties of objects in the scene. Specifically, we tested scenarios where accurate prediction relied on accurate estimates of objects' mechanical properties, including masses, friction, elasticity and deformability, and the values of these mechanical properties could only be inferred by observing how these objects moved and interacted with other objects and/or fluids. We found that models that encode objectness and physical states tend to perform better, yet there is still a huge gap compared to human performance. We also found most models' predictions correlate poorly with that made by humans. These results show that current deep learning models that \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:eMMeJKvmdy0C",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/3x9960zn",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The Limiting Dynamics of SGD: Modified Loss, Phase-Space Oscillations, and Anomalous Diffusion",
            "pub_year": 2023,
            "citation": "Neural Computation, 1-25, 2023",
            "author": "Daniel Kunin and Javier Sagastuy-Brena and Lauren Gillespie and Eshed Margalit and Hidenori Tanaka and Surya Ganguli and Daniel LK Yamins",
            "journal": "Neural Computation",
            "pages": "1-25",
            "publisher": "MIT Press",
            "abstract": "In this work, we explore the limiting dynamics of deep neural networks trained with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by a process of anomalous diffusion in which distance traveled grows as a power law in the number of gradient updates with a nontrivial exponent. We reveal an intricate interaction among the hyperparameters of optimization, the structure in the gradient noise, and the Hessian matrix at the end of training that explains this anomalous diffusion. To build this understanding, we first derive a continuous-time model for SGD with finite learning rates and batch sizes as an underdamped Langevin equation. We study this equation in the setting of linear regression, where we can derive exact, analytic expressions for the phase-space dynamics of the parameters and their instantaneous \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:P5F9QuxV20EC",
        "num_citations": 8,
        "citedby_url": "/scholar?hl=en&cites=13126960835023986037",
        "cites_id": [
            "13126960835023986037"
        ],
        "pub_url": "https://direct.mit.edu/neco/article/doi/10.1162/neco_a_01626/118266",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:dd0RcFRVLLYJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 4,
            "2023": 4
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Counterfactual World Modeling for Physical Dynamics Understanding",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2312.06721, 2023",
            "author": "Rahul Venkatesh and Honglin Chen and Kevin Feigelis and Khaled Jedoui and Klemen Kotar and Felix Binder and Wanhee Lee and Sherry Liu and Kevin A Smith and Judith E Fan and Daniel LK Yamins",
            "journal": "arXiv preprint arXiv:2312.06721",
            "abstract": "The ability to understand physical dynamics is essential to learning agents acting in the world. This paper presents Counterfactual World Modeling (CWM), a candidate pure vision foundational model for physical dynamics understanding. CWM consists of three basic concepts. First, we propose a simple and powerful temporally-factored masking policy for masked prediction of video data, which encourages the model to learn disentangled representations of scene appearance and dynamics. Second, as a result of the factoring, CWM is capable of generating counterfactual next-frame predictions by manipulating a few patch embeddings to exert meaningful control over scene dynamics. Third, the counterfactual modeling capability enables the design of counterfactual queries to extract vision structures similar to keypoints, optical flows, and segmentations, which are useful for dynamics understanding. We show that zero-shot readouts of these structures extracted by the counterfactual queries attain competitive performance to prior methods on real-world datasets. Finally, we demonstrate that CWM achieves state-of-the-art performance on the challenging Physion benchmark for evaluating physical dynamics understanding."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:AXPGKjj_ei8C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2312.06721",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Let's move forward: Image-computable models and a common model evaluation scheme are prerequisites for a scientific understanding of human vision",
            "pub_year": 2023,
            "citation": "Behavioral and Brain Sciences 46, e390, 2023",
            "author": "James J DiCarlo and Daniel LK Yamins and Michael E Ferguson and Evelina Fedorenko and Matthias Bethge and Tyler Bonnen and Martin Schrimpf",
            "journal": "Behavioral and Brain Sciences",
            "volume": "46",
            "pages": "e390",
            "publisher": "Cambridge University Press",
            "abstract": "In the target article, Bowers et al. dispute deep artificial neural network (ANN) models as the currently leading models of human vision without producing alternatives. They eschew the use of public benchmarking platforms to compare vision models with the brain and behavior, and they advocate for a fragmented, phenomenon-specific modeling approach. These are unconstructive to scientific progress. We outline how the Brain-Score community is moving forward to add new model-to-human comparisons to its community-transparent suite of benchmarks."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:Mojj43d5GZwC",
        "num_citations": 0,
        "pub_url": "https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/lets-move-forward-imagecomputable-models-and-a-common-model-evaluation-scheme-are-prerequisites-for-a-scientific-understanding-of-human-vision/F2302912C8652DC2582F0E159C1BB6AB",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The BabyView camera: Designing a new head-mounted camera to capture children\u2019s early social and visual environments",
            "pub_year": 2023,
            "citation": "Behavior Research Methods, 1-12, 2023",
            "author": "Bria Long and Sarah Goodin and George Kachergis and Virginia A Marchman and Samaher F Radwan and Robert Z Sparks and Violet Xiang and Chengxu Zhuang and Oliver Hsu and Brett Newman and Daniel LK Yamins and Michael C Frank",
            "journal": "Behavior Research Methods",
            "pages": "1-12",
            "publisher": "Springer US",
            "abstract": "Head-mounted cameras have been used in developmental psychology research for more than a decade to provide a rich and comprehensive view of what infants see during their everyday experiences. However, variation between these devices has limited the field\u2019s ability to compare results across studies and across labs. Further, the video data captured by these cameras to date has been relatively low-resolution, limiting how well machine learning algorithms can operate over these rich video data. Here, we provide a well-tested and easily constructed design for a head-mounted camera assembly\u2014the BabyView\u2014developed in collaboration with Daylight Design, LLC., a professional product design firm. The BabyView collects high-resolution video, accelerometer, and gyroscope data from children approximately 6\u201330 months of age via a GoPro camera custom mounted on a soft child-safety helmet. The \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:08ZZubdj9fEC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=5355374867511737620",
        "cites_id": [
            "5355374867511737620"
        ],
        "pub_url": "https://link.springer.com/article/10.3758/s13428-023-02206-1",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:FH0XaREdUkoJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The BabyView camera: designing a new head-mounted camera to capture children\u2019s early social and visual environments",
            "pub_year": 2023,
            "citation": "Behavior Research Methods, 1-12, 2023",
            "author": "Bria Long and Sarah Goodin and George Kachergis and Virginia A Marchman and Samaher F Radwan and Robert Z Sparks and Violet Xiang and Chengxu Zhuang and Oliver Hsu and Brett Newman and Daniel LK Yamins and Michael C Frank",
            "journal": "Behavior Research Methods",
            "pages": "1-12",
            "publisher": "Springer US",
            "abstract": "Head-mounted cameras have been used in developmental psychology research for more than a decade to provide a rich and comprehensive view of what infants see during their everyday experiences. However, variation between these devices has limited the field\u2019s ability to compare results across studies and across labs. Further, the video data captured by these cameras to date has been relatively low-resolution, limiting how well machine learning algorithms can operate over these rich video data. Here, we provide a well-tested and easily constructed design for a head-mounted camera assembly\u2014the BabyView\u2014developed in collaboration with Daylight Design, LLC., a professional product design firm. The BabyView collects high-resolution video, accelerometer, and gyroscope data from children approximately 6\u201330 months of age via a GoPro camera custom mounted on a soft child-safety helmet. The \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:08ZZubdj9fEC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=5355374867511737620",
        "cites_id": [
            "5355374867511737620"
        ],
        "pub_url": "https://link.springer.com/article/10.3758/s13428-023-02206-1",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:FH0XaREdUkoJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A Unifying Principle for the Functional Organization of Visual Cortex",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.05. 18.541361, 2023",
            "author": "Eshed Margalit and Hyodong Lee and Dawn Finzi and James J DiCarlo and Kalanit Grill-Spector and Daniel LK Yamins",
            "journal": "bioRxiv",
            "pages": "2023.05. 18.541361",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "A key feature of many cortical systems is functional organization: the arrangement of neurons with specific functional properties in characteristic spatial patterns across the cortical surface. However, the principles underlying the emergence and utility of functional organization are poorly understood. Here we develop the Topographic Deep Artificial Neural Network (TDANN), the first unified model to accurately predict the functional organization of multiple cortical areas in the primate visual system. We analyze the key factors responsible for the TDANN's success and find that it strikes a balance between two specific objectives: achieving a task-general sensory representation that is self-supervised, and maximizing the smoothness of responses across the cortical sheet according to a metric that scales relative to cortical surface area. In turn, the representations learned by the TDANN are lower dimensional and more brain-like than those in models that lack a spatial smoothness constraint. Finally, we provide evidence that the TDANN's functional organization balances performance with inter-area connection length, and use the resulting models for a proof-of-principle optimization of cortical prosthetic design. Our results thus offer a unified principle for understanding functional organization and a novel view of the functional role of the visual system in particular."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:q3oQSFYPqjQC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=615012094074270269",
        "cites_id": [
            "615012094074270269"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.05.18.541361.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:PbLdtz32iAgJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Are These the Same Apple? Comparing Images Based on Object Intrinsics",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2311.00750, 2023",
            "author": "Klemen Kotar and Stephen Tian and Hong-Xing Yu and Daniel LK Yamins and Jiajun Wu",
            "journal": "arXiv preprint arXiv:2311.00750",
            "abstract": "The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of  images of  objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:HE397vMXCloC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2311.00750",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Evaluating physical scene understanding with objects consisting of different physical attributes in humans and machines",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5622-5622, 2023",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Daniel Bear and Chuang Gan and Joshua Tenenbaum and Daniel Yamins and Judith Fan and Kevin Smith",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5622-5622",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Human physical scene understanding requires more than simply localizing and recognizing objects\u2014we can quickly adapt our predictions about how a scene will unfold by incorporating objects' latent physics properties, such as the masses of the objects in the scene. What are the underlying computational mechanisms that allow humans to infer these physical properties and adapt their physical predictions so efficiently from visual inputs? One hypothesis is that general intuitive physics knowledge can be learned from enough raw data, instantiated as computational models that predict future video frames in large datasets of complex scenes. To test this hypothesis, we evaluated how well two state-of-the-art video models\u2014MCVD (Voleti et al., 2022) and ALOE (Ding et al., 2021)\u2014could approximate human-level physical scene understanding. We measured both model and human performance on Physion++, a \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:XiVPGOgt02cC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792645",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2306.15668, 2023",
            "author": "Hsiao-Yu Tung and Mingyu Ding and Zhenfang Chen and Daniel Bear and Chuang Gan and Joshua B Tenenbaum and Daniel LK Yamins and Judith E Fan and Kevin A Smith",
            "journal": "arXiv preprint arXiv:2306.15668",
            "abstract": "General physical scene understanding requires more than simply localizing and recognizing objects -- it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, and where the values of those properties can only be inferred by observing how objects move and interact with other objects or fluids. We evaluate the performance of a number of state-of-the-art prediction models that span a variety of levels of learning vs. built-in knowledge, and compare that performance to a set of human predictions. We find that models that have been trained using standard regimes and datasets do not spontaneously learn to make inferences about latent properties, but also that models that encode objectness and physical states tend to make better predictions. However, there is still a huge gap between all models and human performance, and all models' \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:5ugPr518TE4C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2306.15668",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Unifying (Machine) Vision via Counterfactual World Modeling",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2306.01828, 2023",
            "author": "Daniel M Bear and Kevin Feigelis and Honglin Chen and Wanhee Lee and Rahul Venkatesh and Klemen Kotar and Alex Durango and Daniel LK Yamins",
            "journal": "arXiv preprint arXiv:2306.01828",
            "abstract": "Leading approaches in machine vision employ different architectures for different tasks, trained on costly task-specific labeled datasets. This complexity has held back progress in areas, such as robotics, where robust task-general perception remains a bottleneck. In contrast, \"foundation models\" of natural language have shown how large pre-trained neural networks can provide zero-shot solutions to a broad spectrum of apparently distinct tasks. Here we introduce Counterfactual World Modeling (CWM), a framework for constructing a visual foundation model: a unified, unsupervised network that can be prompted to perform a wide variety of visual computations. CWM has two key components, which resolve the core issues that have hindered application of the foundation model concept to vision. The first is structured masking, a generalization of masked prediction methods that encourages a prediction model to capture the low-dimensional structure in visual data. The model thereby factors the key physical components of a scene and exposes an interface to them via small sets of visual tokens. This in turn enables CWM's second main idea -- counterfactual prompting -- the observation that many apparently distinct visual representations can be computed, in a zero-shot manner, by comparing the prediction model's output on real inputs versus slightly modified (\"counterfactual\") inputs. We show that CWM generates high-quality readouts on real-world images and videos for a diversity of tasks, including estimation of keypoints, optical flow, occlusions, object segments, and relative depth. Taken together, our results show that CWM is a promising path \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:J-pR_7NvFogC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2306.01828",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:_Br_hSX_2dgJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Measuring and Modeling Physical Intrinsic Motivation",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2305.13452, 2023",
            "author": "Julio Martinez and Felix Binder and Haoliang Wang and Nicker Haber and Judith Fan and Daniel LK Yamins",
            "journal": "arXiv preprint arXiv:2305.13452",
            "abstract": "Humans are interactive agents driven to seek out situations with interesting physical dynamics. Here we formalize the functional form of physical intrinsic motivation. We first collect ratings of how interesting humans find a variety of physics scenarios. We then model human interestingness responses by implementing various hypotheses of intrinsic motivation including models that rely on simple scene features to models that depend on forward physics prediction. We find that the single best predictor of human responses is adversarial reward, a model derived from physical prediction loss. We also find that simple scene feature models do not generalize their prediction of human responses across all scenarios. Finally, linearly combining the adversarial model with the number of collisions in a scene leads to the greatest improvement in predictivity of human responses, suggesting humans are driven towards scenarios that result in high information gain and physical activity."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:kRWSkSYxWN8C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2305.13452",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:8QuZVygSmpMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Developmental Curiosity and Social Interaction in Virtual Agents",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2305.13396, 2023",
            "author": "Chris Doyle and Sarah Shader and Michelle Lau and Megumi Sano and Daniel LK Yamins and Nick Haber",
            "journal": "arXiv preprint arXiv:2305.13396",
            "abstract": "Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:mvPsJ3kp5DgC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2305.13396",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:XyLvMJA_IigJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Medial temporal cortex supports compositional visual inferences",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.09. 07.556737, 2023",
            "author": "Tyler Bonnen and Anthony D Wagner and Daniel LK Yamins",
            "journal": "bioRxiv",
            "pages": "2023.09. 07.556737",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Perception unfolds across multiple timescales. For humans and other primates, many object-centric visual attributes can be inferred 'at a glance' (i.e., with <200ms of visual information), an ability supported by ventral temporal cortex (VTC). Other perceptual inferences require more time; to determine a novel object's identity, we might need to represent its unique configuration of visual features, requiring multiple 'glances.' Here we evaluate whether medial temporal cortex (MTC), downstream from VTC, supports object perception by integrating over such visuospatial sequences. We first compare human visual inferences directly to electrophysiological recordings from macaque VTC. While human performance 'at a glance' is approximated by a linear readout of VTC, participants radically outperform VTC given longer viewing times (i.e.,  >200ms). Next, we demonstrate the causal role of MTC in these temporally extended visual inferences: just as time restricted performance can be approximated by a linear readout of VTC, the performance of (time unrestricted) MTC-lesioned humans resembles a computational proxy for VTC. Finally, we characterize these visual abilities through a series of eyetracking experiments. With extended viewing times participants sequentially sample task-relevant features via multiple saccades\u2014visuospatial patterns that are reliable across participants and necessary for performance. From these data, we suggest that MTC transforms visuospatial sequences into 'compositional' representations that support visual object perception."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:olpn-zPbct0C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.09.07.556737.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Advancing Cognitive Science and AI with Cognitive-AI Benchmarking",
            "pub_year": 2023,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society 45 (45), 2023",
            "author": "Felix Jedidja Binder and Logan Matthew Cross and Yoni Friedman and Robert Hawkins and Daniel LK Yamins and Judith E Fan",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "volume": "45",
            "number": "45",
            "abstract": "What are the current limits of AI models in explaining human cognition and behavior? How might approaches from the cognitive sciences drive the development of more robust and reliable AI systems? The goal of this workshop is bring together researchers across cognitive science and artificial intelligence (AI) to engage with these questions and identify opportunities to work together to advance progress in both fields. In particular, we propose Cognitive-AI Benchmarking as a particularly promising strategy --- that is, the community-coordinated establishment of common benchmarks, tools, and best practices for model-human comparisons across diverse and ecologically relevant domains and tasks. We will host a combination of talks, panel discussion, and breakout activities to: highlight past successes in Cognitive-AI Benchmarking and limitations of current approaches, share tools and best practices, and outline future challenges and goals for the field."
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:bnK-pcrLprsC",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/5v56249j",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes",
            "pub_year": 2023,
            "citation": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \u2026, 2023",
            "author": "Haotian Xue and Antonio Torralba and Joshua Tenenbaum and Daniel Yamins and Yunzhu Li and Hsiao-Yu Tung",
            "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "3624-3634",
            "abstract": "Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex \u2026"
        },
        "filled": true,
        "author_pub_id": "qwC3vG8AAAAJ:BrmTIyaxlBUC",
        "num_citations": 0,
        "pub_url": "https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Xue_3D-IntPhys_Towards_More_Generalized_3D-Grounded_Visual_Intuitive_Physics_Under_Challenging_CVPRW_2023_paper.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:V0m9MxS-jCMJ:scholar.google.com/",
        "cites_per_year": {}
    }
]