[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Learning human action recognition representations without real humans",
            "pub_year": 2024,
            "citation": "Advances in Neural Information Processing Systems 36, 2024",
            "author": "Howard Zhong and Samarth Mishra and Donghyun Kim and SouYoung Jin and Rameswar Panda and Hilde Kuehne and Leonid Karlinsky and Venkatesh Saligrama and Aude Oliva and Rogerio Feris",
            "journal": "Advances in Neural Information Processing Systems",
            "volume": "36",
            "abstract": "Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the {\\em transferability} of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with {\\em humans removed} and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5\\% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github. com/howardzh01/PPMA."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:Xz60mAmATU4C",
        "num_citations": 0,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/cd556f38dba3a6c367c42fa85fc0801c-Abstract-Datasets_and_Benchmarks.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VWAOonQ-Z5wJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2406.08164, 2024",
            "author": "Irene Huang and Wei Lin and M Jehanzeb Mirza and Jacob A Hansen and Sivan Doveh and Victor Ion Butoi and Roei Herzig and Assaf Arbelle and Hilde Kuhene and Trevor Darrel and Chuang Gan and Aude Oliva and Rogerio Feris and Leonid Karlinsky",
            "journal": "arXiv preprint arXiv:2406.08164",
            "abstract": "Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:X9ykpCP0fEIC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2406.08164",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:LkeO-EBB3eoJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": ": towards data-free Transferable Parameter Efficient Finetuning",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2405.17258, 2024",
            "author": "Runqian Wang and Soumya Ghosh and David Cox and Diego Antognini and Aude Oliva and Rogerio Feris and Leonid Karlinsky",
            "journal": "arXiv preprint arXiv:2405.17258",
            "abstract": "Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose  -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the  task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:27LrP4qxOz0C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2405.17258",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:sGKMYUJGREUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dynamic multi-resolution processing for video classification",
            "pub_year": 2024,
            "citation": "US Patent 11,954,910, 2024",
            "author": "Rameswar Panda and Yue Meng and Chung-Ching Lin and Rogerio Schmidt Feris and Aude Jeanne Oliva",
            "abstract": "Methods, apparatus, and systems for multi-resolution processing for video classification. A plurality of video frames of a video are obtained and a resolution for classifying each video frame of the plurality of video frames is determined by analyzing each video frame using a policy network. Based on the determined resolution, each video frame having a determined resolution is rescaled and each rescaled video frame is routed to a classifier of a backbone network that corresponds to the determined resolution. Each rescaled video frame is classified using the corresponding classifier of the backbone network to obtain a plurality of classifications and the classifications are averaged to determine an action classification of the video."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:aIdbFUkbNIkC",
        "num_citations": 0,
        "pub_url": "https://patents.google.com/patent/US11954910B2/en",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Z2du9ZEIMfkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2404.05567, 2024",
            "author": "Bowen Pan and Yikang Shen and Haokun Liu and Mayank Mishra and Gaoyuan Zhang and Aude Oliva and Colin Raffel and Rameswar Panda",
            "journal": "arXiv preprint arXiv:2404.05567",
            "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2-4 compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally require 2-4 times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation. In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference. Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40% of the model's parameters). Performance tests using vLLM show that our DS-MoE-6B model runs up to  faster than similar dense models like Mistral-7B, and between  and  faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:qwy9JoKyICEC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2404.05567",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ACnb8h09W6wJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Visual perception of highly memorable images is mediated by a distributed network of ventral visual regions that enable a late memorability response",
            "pub_year": 2024,
            "citation": "Plos Biology 22 (4), e3002564, 2024",
            "author": "Benjamin Lahner and Yalda Mohsenzadeh and Caitlin Mullin and Aude Oliva",
            "journal": "Plos Biology",
            "volume": "22",
            "number": "4",
            "pages": "e3002564",
            "publisher": "Public Library of Science",
            "abstract": "Behavioral and neuroscience studies in humans and primates have shown that memorability is an intrinsic property of an image that predicts its strength of encoding into and retrieval from memory. While previous work has independently probed when or where this memorability effect may occur in the human brain, a description of its spatiotemporal dynamics is missing. Here, we used representational similarity analysis (RSA) to combine functional magnetic resonance imaging (fMRI) with source-estimated magnetoencephalography (MEG) to simultaneously measure when and where the human cortex is sensitive to differences in image memorability. Results reveal that visual perception of High Memorable images, compared to Low Memorable images, recruits a set of regions of interest (ROIs) distributed throughout the ventral visual cortex: a late memorability response (from around 300 ms) in early visual cortex (EVC), inferior temporal cortex, lateral occipital cortex, fusiform gyrus, and banks of the superior temporal sulcus. Image memorability magnitude results are represented after high-level feature processing in visual regions and reflected in classical memory regions in the medial temporal lobe (MTL). Our results present, to our knowledge, the first unified spatiotemporal account of visual memorability effect across the human cortex, further supporting the levels-of-processing theory of perception and memory."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:g5Ck-dwhA_QC",
        "num_citations": 0,
        "pub_url": "https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3002564",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:tVCHeKRZSO8J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Effects of Browsing Conditions and Visual Alert Design on Human Susceptibility to Deepfakes",
            "pub_year": 2024,
            "citation": "Journal of Online Trust and Safety 2 (2), 2024",
            "author": "Emilie Josephs and Camilo Fosco and Aude Oliva",
            "journal": "Journal of Online Trust and Safety",
            "volume": "2",
            "number": "2",
            "abstract": "The increasing reach of deepfakes raises practical questions about people\u2019s ability to detect false videos online. How vulnerable are people to deepfake videos? What technologies can help improve detection? Previous experiments that measure human deepfake detection historically omit a number of conditions that can exist in typical browsing conditions. Here, we operationalized four such conditions (low prevalence, brief presentation, low video quality, and divided attention), and found in a series of online experiments that all conditions lowered detection relative to baseline, suggesting that the current literature underestimates people\u2019s susceptibility to deepfakes. Next, we examined how AI assistance could be integrated into the human decision process. We found that a model that exposes deepfakes by amplifying artifacts increases detection rates, and also leads to higher rates of incorporating AI feedback and higher final confidence than text-based prompts. Overall, this suggests that visual indicators that cause distortions on fake videos may be effective at mitigating the impact of falsified video."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:2v_ZtQDX9iAC",
        "num_citations": 0,
        "pub_url": "https://www.tsjournal.org/index.php/jots/article/view/144",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:51i7ORAdrMYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths",
            "pub_year": 2024,
            "citation": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer \u2026, 2024",
            "author": "Ximeng Sun and Rameswar Panda and Chun-Fu Richard Chen and Naigang Wang and Bowen Pan and Aude Oliva and Rogerio Feris and Kate Saenko",
            "conference": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
            "pages": "957-967",
            "abstract": "Quantizing deep networks with adaptive bit-widths is a promising technique for efficient inference across many devices and resource constraints. In contrast to static methods that repeat the quantization process and train different models for different constraints, adaptive quantization enables us to flexibly adjust the bit-widths of a single deep network during inference for instant adaptation in different scenarios. While existing research shows encouraging results on common image classification benchmarks, this paper investigates how to train such adaptive networks more effectively. Specifically, we present two novel techniques for quantizing deep neural networks with adaptive bit-widths of weights and activations. First, we propose a collaborative strategy to choose a high-precision\" teacher\" for transferring knowledge to the low-precision\" student\" while jointly optimizing the model with all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic block swapping method by randomly replacing the blocks in the lower-precision student network with the corresponding blocks in the higher-precision teacher network. Extensive experiments on multiple image classification datasets and novel video classification experiments, well demonstrate the efficacy of our approach over state-of-the-art methods."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:F2UWTTQJPOcC",
        "num_citations": 0,
        "pub_url": "https://openaccess.thecvf.com/content/WACV2024/html/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZZut4HTtZaUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Learning Human Action Recognition Representations Without Real Humans",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2311.06231, 2023",
            "author": "Howard Zhong and Samarth Mishra and Donghyun Kim and SouYoung Jin and Rameswar Panda and Hilde Kuehne and Leonid Karlinsky and Venkatesh Saligrama and Aude Oliva and Rogerio Feris",
            "journal": "arXiv preprint arXiv:2311.06231",
            "abstract": "Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the transferability of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with humans removed and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA ."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:Xz60mAmATU4C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2311.06231",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The algonauts project 2023 challenge: How the human brain makes sense of natural scenes",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2301.03198, 2023",
            "author": "Alessandro T Gifford and Benjamin Lahner and Sari Saba-Sadiya and Martina G Vilas and Alex Lascelles and Aude Oliva and Kendrick Kay and Gemma Roig and Radoslaw M Cichy",
            "journal": "arXiv preprint arXiv:2301.03198",
            "abstract": "The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to ~73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:dBIO0h50nwkC",
        "num_citations": 9,
        "citedby_url": "/scholar?hl=en&cites=1174422521076802658",
        "cites_id": [
            "1174422521076802658"
        ],
        "pub_url": "https://arxiv.org/abs/2301.03198",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:YmQAgQ9jTBAJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 9
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Going beyond nouns with vision & language models using synthetic data",
            "pub_year": 2023,
            "citation": "Proceedings of the IEEE/CVF International Conference on Computer Vision \u2026, 2023",
            "author": "Paola Cascante-Bonilla and Khaled Shehada and James Seale Smith and Sivan Doveh and Donghyun Kim and Rameswar Panda and Gul Varol and Aude Oliva and Vicente Ordonez and Rogerio Feris and Leonid Karlinsky",
            "conference": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
            "pages": "20155-20165",
            "abstract": "Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go'beyond nouns' such as the meaning of non-object words (eg, attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC)-a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (eg by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:HIFyuExEbWQC",
        "num_citations": 4,
        "citedby_url": "/scholar?hl=en&cites=3904652032954906011",
        "cites_id": [
            "3904652032954906011"
        ],
        "pub_url": "http://openaccess.thecvf.com/content/ICCV2023/html/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:m7XIE1YcMDYJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 4
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Leveraging Temporal Context in Low Representational Power Regimes",
            "pub_year": 2023,
            "citation": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \u2026, 2023",
            "author": "Camilo L Fosco and SouYoung Jin and Emilie Josephs and Aude Oliva",
            "conference": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "pages": "10693-10703",
            "abstract": "Computer vision models are excellent at identifying and exploiting regularities in the world. However, it is computationally costly to learn these regularities from scratch. This presents a challenge for low-parameter models, like those running on edge devices (eg smartphones). Can the performance of models with low representational power be improved by supplementing training with additional information about these statistical regularities? We explore this in the domains of action recognition and action anticipation, leveraging the fact that actions are typically embedded in stereotypical sequences. We introduce the Event Transition Matrix (ETM), computed from action labels in an untrimmed video dataset, which captures the temporal context of a given action, operationalized as the likelihood that it was preceded or followed by each other action in the set. We show that including information from the ETM during training improves action recognition and anticipation performance on various egocentric video datasets. Through ablation and control studies, we show that the coherent sequence of information captured by our ETM is key to this effect, and we find that the benefit of this explicit representation of temporal context is most pronounced for smaller models. Code, matrices and models are available in our project page: https://camilofosco. com/etm_website."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:4vMrXwiscB8C",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=6865103784530314443",
        "cites_id": [
            "6865103784530314443"
        ],
        "pub_url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fosco_Leveraging_Temporal_Context_in_Low_Representational_Power_Regimes_CVPR_2023_paper.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:y4Qe54S_RV8J:scholar.google.com/",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "LangNav: Language as a Perceptual Representation for Navigation",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2310.07889, 2023",
            "author": "Bowen Pan and Rameswar Panda and SouYoung Jin and Rogerio Feris and Aude Oliva and Phillip Isola and Yoon Kim",
            "journal": "arXiv preprint arXiv:2310.07889",
            "abstract": "We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer a policy learned on a simulated environment (ALFRED) to a real-world environment (R2R). Our approach is found to improve upon strong baselines that rely on visual features in settings where only a few gold trajectories (10-100) are available, demonstrating the potential of using language as a perceptual representation for navigation tasks."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:qe6vwMD2xtsC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2310.07889",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "INTERPRETABILITY-AWARE REDUNDANCY REDUCTION FOR VISION TRANSFORMERS",
            "pub_year": 2023,
            "citation": "US Patent App. 17/559,053, 2023",
            "author": "Bowen Pan and Rameswar Panda and Rogerio Schmidt Feris and Aude Jeanne Oliva",
            "abstract": "A sequence of patch tokens representing an image can be received. A network can be trained to learn informative patch tokens and uninformative patch tokens in the sequence of patch tokens, in learning to recognize an object in the image. The sequence of patch tokens can be reduced by removing the uninformative patch tokens from the sequence of patch tokens. The reduced sequence of patch tokens can be input to an attention-based deep learning neural network. The attention-based deep learning neural network can be fine-tuned to recognize the object in the image using the reduced sequence of patch tokens."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:Dem6FJhTUoYC",
        "num_citations": 0,
        "pub_url": "https://www.freepatentsonline.com/y2023/0196710.html",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Artifact magnification on deepfake videos increases human detection and subjective confidence",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2304.04733, 2023",
            "author": "Emilie Josephs and Camilo Fosco and Aude Oliva",
            "journal": "arXiv preprint arXiv:2304.04733",
            "abstract": "The development of technologies for easily and automatically falsifying video has raised practical questions about people's ability to detect false information online. How vulnerable are people to deepfake videos? What technologies can be applied to boost their performance? Human susceptibility to deepfake videos is typically measured in laboratory settings, which do not reflect the challenges of real-world browsing. In typical browsing, deepfakes are rare, engagement with the video may be short, participants may be distracted, or the video streaming quality may be degraded. Here, we tested deepfake detection under these ecological viewing conditions, and found that detection was lowered in all cases. Principles from signal detection theory indicated that different viewing conditions affected different dimensions of detection performance. Overall, this suggests that the current literature underestimates people's susceptibility to deepfakes. Next, we examined how computer vision models might be integrated into users' decision process to increase accuracy and confidence during deepfake detection. We evaluated the effectiveness of communicating the model's prediction to the user by amplifying artifacts in fake videos. We found that artifact amplification was highly effective at making fake video distinguishable from real, in a manner that was robust across viewing conditions. Additionally, compared to a traditional text-based prompt, artifact amplification was more convincing: people accepted the model's suggestion more often, and reported higher final confidence in their model-supported decision, particularly for more challenging videos. Overall \u2026"
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:mNrWkgRL2YcC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2304.04733",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ES4SYfMgEWAJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Adaptive redundancy reduction for efficient video understanding",
            "pub_year": 2023,
            "citation": "US Patent App. 17/476,437, 2023",
            "author": "Bowen Pan and Rameswar Panda and Camilo Luciano Fosco and Rogerio Schmidt Feris and Aude Jeanne Oliva",
            "abstract": "For each convolution layer of a plurality of convolution layers of a convolutional neural network (CNN), apply an input-dependent policy network to determine: a first fraction of input feature maps to the given layer for which first corresponding output feature maps are to be fully computed by the layer; and a second fraction of input feature maps to the layer for which second corresponding output feature maps are not to be fully computed, but to be reconstructed from the first corresponding output feature maps. Fully computing the first corresponding output feature maps and reconstruct the second corresponding output feature maps. For a final one of the convolution layers of the plurality of convolution layers of the neural network, input the first corresponding output feature maps and the second corresponding output feature maps to an output layer to obtain an inference result."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:SGW5VrABaM0C",
        "num_citations": 0,
        "pub_url": "https://patents.google.com/patent/US20230082448A1/en",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "BOLD Moments: modeling short visual events through a video fMRI dataset and metadata",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.03. 12.530887, 2023",
            "author": "Benjamin Lahner and Kshitij Dwivedi and Polina Iamshchinina and Monika Graumann and Alex Lascelles and Gemma Roig and Alessandro Thomas Gifford and Bowen Pan and SouYoung Jin and N Apurva Ratan Murty and Kendrick Kay and Aude Oliva and Radoslaw Cichy",
            "journal": "bioRxiv",
            "pages": "2023.03. 12.530887",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Grasping the meaning of everyday visual events is a fundamental feat of human intelligence that hinges on diverse neural processes ranging from vision to higher-level cognition. Deciphering the neural basis of visual event understanding requires rich, extensive, and appropriately designed experimental data. However, this type of data is hitherto missing. To fill this gap, we introduce the BOLD Moments Dataset (BMD), a large dataset of whole-brain fMRI responses to over 1,000 short (3s) naturalistic video clips and accompanying metadata. We show visual events interface with an array of processes, extending even to memory, and we reveal a match in hierarchical processing between brains and video-computable deep neural networks. Furthermore, we showcase that BMD successfully captures temporal dynamics of visual events at second resolution. BMD thus establishes a critical groundwork for investigations of the neural basis of visual event understanding."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:cWzG1nlazyYC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.03.12.530887.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jESFCRR-e_wJ:scholar.google.com/",
        "cites_per_year": {}
    }
]