[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2404.05567, 2024",
            "author": "Bowen Pan and Yikang Shen and Haokun Liu and Mayank Mishra and Gaoyuan Zhang and Aude Oliva and Colin Raffel and Rameswar Panda",
            "journal": "arXiv preprint arXiv:2404.05567",
            "abstract": "Mixture-of-Experts (MoE) language models can reduce computational costs by 2-4 compared to dense models without sacrificing performance, making them more efficient in computation-bounded scenarios. However, MoE models generally require 2-4 times more parameters to achieve comparable performance to a dense model, which incurs larger GPU memory requirements and makes MoE models less efficient in I/O-bounded scenarios like autoregressive generation. In this work, we propose a hybrid dense training and sparse inference framework for MoE models (DS-MoE) which achieves strong computation and parameter efficiency by employing dense computation across all experts during training and sparse computation during inference. Our experiments on training LLMs demonstrate that our DS-MoE models are more parameter-efficient than standard sparse MoEs and are on par with dense models in terms of total parameter size and performance while being computationally cheaper (activating 30-40% of the model's parameters). Performance tests using vLLM show that our DS-MoE-6B model runs up to  faster than similar dense models like Mistral-7B, and between  and  faster than comparable MoEs, such as DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:qwy9JoKyICEC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=12419587596242069760",
        "cites_id": [
            "12419587596242069760"
        ],
        "pub_url": "https://arxiv.org/abs/2404.05567",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ACnb8h09W6wJ:scholar.google.com/",
        "cites_per_year": {
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Visual perception of highly memorable images is mediated by a distributed network of ventral visual regions that enable a late memorability response",
            "pub_year": 2024,
            "citation": "Plos Biology 22 (4), e3002564, 2024",
            "author": "Benjamin Lahner and Yalda Mohsenzadeh and Caitlin Mullin and Aude Oliva",
            "journal": "Plos Biology",
            "volume": "22",
            "number": "4",
            "pages": "e3002564",
            "publisher": "Public Library of Science",
            "abstract": "Behavioral and neuroscience studies in humans and primates have shown that memorability is an intrinsic property of an image that predicts its strength of encoding into and retrieval from memory. While previous work has independently probed when or where this memorability effect may occur in the human brain, a description of its spatiotemporal dynamics is missing. Here, we used representational similarity analysis (RSA) to combine functional magnetic resonance imaging (fMRI) with source-estimated magnetoencephalography (MEG) to simultaneously measure when and where the human cortex is sensitive to differences in image memorability. Results reveal that visual perception of High Memorable images, compared to Low Memorable images, recruits a set of regions of interest (ROIs) distributed throughout the ventral visual cortex: a late memorability response (from around 300 ms) in early visual cortex (EVC), inferior temporal cortex, lateral occipital cortex, fusiform gyrus, and banks of the superior temporal sulcus. Image memorability magnitude results are represented after high-level feature processing in visual regions and reflected in classical memory regions in the medial temporal lobe (MTL). Our results present, to our knowledge, the first unified spatiotemporal account of visual memorability effect across the human cortex, further supporting the levels-of-processing theory of perception and memory."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:g5Ck-dwhA_QC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=17242129736319586485",
        "cites_id": [
            "17242129736319586485"
        ],
        "pub_url": "https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3002564",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:tVCHeKRZSO8J:scholar.google.com/",
        "cites_per_year": {
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2406.08164, 2024",
            "author": "Irene Huang and Wei Lin and M Jehanzeb Mirza and Jacob A Hansen and Sivan Doveh and Victor Ion Butoi and Roei Herzig and Assaf Arbelle and Hilde Kuhene and Trevor Darrel and Chuang Gan and Aude Oliva and Rogerio Feris and Leonid Karlinsky",
            "journal": "arXiv preprint arXiv:2406.08164",
            "abstract": "Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM-only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:X9ykpCP0fEIC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2406.08164",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:LkeO-EBB3eoJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": ": towards data-free Transferable Parameter Efficient Finetuning",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2405.17258, 2024",
            "author": "Runqian Wang and Soumya Ghosh and David Cox and Diego Antognini and Aude Oliva and Rogerio Feris and Leonid Karlinsky",
            "journal": "arXiv preprint arXiv:2405.17258",
            "abstract": "Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose  -- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the  task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:27LrP4qxOz0C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2405.17258",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:sGKMYUJGREUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dynamic multi-resolution processing for video classification",
            "pub_year": 2024,
            "citation": "US Patent 11,954,910, 2024",
            "author": "Rameswar Panda and Yue Meng and Chung-Ching Lin and Rogerio Schmidt Feris and Aude Jeanne Oliva",
            "abstract": "Methods, apparatus, and systems for multi-resolution processing for video classification. A plurality of video frames of a video are obtained and a resolution for classifying each video frame of the plurality of video frames is determined by analyzing each video frame using a policy network. Based on the determined resolution, each video frame having a determined resolution is rescaled and each rescaled video frame is routed to a classifier of a backbone network that corresponds to the determined resolution. Each rescaled video frame is classified using the corresponding classifier of the backbone network to obtain a plurality of classifications and the classifications are averaged to determine an action classification of the video."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:aIdbFUkbNIkC",
        "num_citations": 0,
        "pub_url": "https://patents.google.com/patent/US11954910/en",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:gYtpdLYPgAMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Effects of Browsing Conditions and Visual Alert Design on Human Susceptibility to Deepfakes",
            "pub_year": 2024,
            "citation": "Journal of Online Trust and Safety 2 (2), 2024",
            "author": "Emilie Josephs and Camilo Fosco and Aude Oliva",
            "journal": "Journal of Online Trust and Safety",
            "volume": "2",
            "number": "2",
            "abstract": "The increasing reach of deepfakes raises practical questions about people\u2019s ability to detect false videos online. How vulnerable are people to deepfake videos? What technologies can help improve detection? Previous experiments that measure human deepfake detection historically omit a number of conditions that can exist in typical browsing conditions. Here, we operationalized four such conditions (low prevalence, brief presentation, low video quality, and divided attention), and found in a series of online experiments that all conditions lowered detection relative to baseline, suggesting that the current literature underestimates people\u2019s susceptibility to deepfakes. Next, we examined how AI assistance could be integrated into the human decision process. We found that a model that exposes deepfakes by amplifying artifacts increases detection rates, and also leads to higher rates of incorporating AI feedback and higher final confidence than text-based prompts. Overall, this suggests that visual indicators that cause distortions on fake videos may be effective at mitigating the impact of falsified video."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:2v_ZtQDX9iAC",
        "num_citations": 0,
        "pub_url": "https://www.tsjournal.org/index.php/jots/article/view/144",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:51i7ORAdrMYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Learning human action recognition representations without real humans",
            "pub_year": 2024,
            "citation": "Advances in Neural Information Processing Systems 36, 2024",
            "author": "Howard Zhong and Samarth Mishra and Donghyun Kim and SouYoung Jin and Rameswar Panda and Hilde Kuehne and Leonid Karlinsky and Venkatesh Saligrama and Aude Oliva and Rogerio Feris",
            "journal": "Advances in Neural Information Processing Systems",
            "volume": "36",
            "abstract": "Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the {\\em transferability} of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with {\\em humans removed} and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. Our approach outperforms previous baselines by up to 5\\% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github. com/howardzh01/PPMA."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:Xz60mAmATU4C",
        "num_citations": 0,
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/cd556f38dba3a6c367c42fa85fc0801c-Abstract-Datasets_and_Benchmarks.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VWAOonQ-Z5wJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Improved Techniques for Quantizing Deep Networks with Adaptive Bit-Widths",
            "pub_year": 2024,
            "citation": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer \u2026, 2024",
            "author": "Ximeng Sun and Rameswar Panda and Chun-Fu Richard Chen and Naigang Wang and Bowen Pan and Aude Oliva and Rogerio Feris and Kate Saenko",
            "conference": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision",
            "pages": "957-967",
            "abstract": "Quantizing deep networks with adaptive bit-widths is a promising technique for efficient inference across many devices and resource constraints. In contrast to static methods that repeat the quantization process and train different models for different constraints, adaptive quantization enables us to flexibly adjust the bit-widths of a single deep network during inference for instant adaptation in different scenarios. While existing research shows encouraging results on common image classification benchmarks, this paper investigates how to train such adaptive networks more effectively. Specifically, we present two novel techniques for quantizing deep neural networks with adaptive bit-widths of weights and activations. First, we propose a collaborative strategy to choose a high-precision\" teacher\" for transferring knowledge to the low-precision\" student\" while jointly optimizing the model with all bit-widths. Second, to effectively transfer knowledge, we develop a dynamic block swapping method by randomly replacing the blocks in the lower-precision student network with the corresponding blocks in the higher-precision teacher network. Extensive experiments on multiple image classification datasets and novel video classification experiments, well demonstrate the efficacy of our approach over state-of-the-art methods."
        },
        "filled": true,
        "author_pub_id": "FNhl50sAAAAJ:F2UWTTQJPOcC",
        "num_citations": 0,
        "pub_url": "https://openaccess.thecvf.com/content/WACV2024/html/Sun_Improved_Techniques_for_Quantizing_Deep_Networks_With_Adaptive_Bit-Widths_WACV_2024_paper.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:nFlG3wuy-T4J:scholar.google.com/",
        "cites_per_year": {}
    }
]