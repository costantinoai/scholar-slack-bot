[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Category-specific effects of high-level relations in visual search",
            "pub_year": 2023,
            "citation": "OSF preprints, 2023",
            "author": "Nicolas Goupil and Daniel Kaiser and Liuba Papeo",
            "journal": "OSF preprints",
            "abstract": "Recent empirical findings demonstrate that, in visual search for a target in an array of distractors, observers exploit information about object relations to increase search efficiency. We investigated how people searched for interacting people in a crowd, and how the eccentricity of the target affected the search (Experiments 1-3). Participants briefly viewed crowded arrays and had to search for an interacting dyad (two bodies face-to-face) among non-interacting dyads (back-to-back distractors) or vice versa, with the target presented in the attended central location or at peripheral locations. With central targets, we found a search asymmetry, whereby interacting people among non-interacting people were detected better than non-interacting people among interacting people. With peripheral targets, non-interacting targets were detected better than interacting targets. In Experiment 4, we asked whether these asymmetries generalized to object pairs whose spatial relations did or did not form functionally interacting sets (computer screen above keyboard). Results showed that non-interacting targets were detected better than interacting targets, whether presented in central or peripheral locations. Thus, the effect of relational information on visual search is contingent on both stimulus category and attentional focus. Across both stimulus categories (bodies and objects), search is facilitated when individual distractor-items can be organized in larger structured units (social interaction or functional set), effectively reducing the number of distractors. The presentation of social interaction at the attended (central) location breaks this search pattern by readily \u2026"
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:z_wVstp3MssC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=9316523349721484676",
        "cites_id": [
            "9316523349721484676"
        ],
        "pub_url": "https://osf.io/n7uvk/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hA3IIaDwSoEJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2308.09431, 2023",
            "author": "Zejin Lu and Adrien Doerig and Victoria Bosch and Bas Krahmer and Daniel Kaiser and Radoslaw M Cichy and Tim C Kietzmann",
            "journal": "arXiv preprint arXiv:2308.09431",
            "abstract": "Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:uc_IGeMz5qoC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2308.09431",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Individual differences in internal models explain idiosyncrasies in scene perception",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Matthew J Foxwell* and Gongting Wang* and Radoslaw M Cichy and David Pitcher and Daniel Kaiser",
            "publisher": "PsyArXiv",
            "abstract": "According to predictive processing theories, vision is facilitated by predictions derived from our internal models of what the world should look like. However, the contents of these models and how they vary across people remains unclear. Here, we use drawing to directly access the contents of the internal models of individual participants. Participants were first asked to draw typical versions of scene categories, as descriptors of their internal models. These drawings were converted into standardized 3d renders, which we used as stimuli in subsequent scene categorization experiments. Across two experiments, participants\u2019 scene categorization was more accurate for renders tailored to their own drawings compared to renders based on others\u2019 drawings or copies of scene photographs, suggesting that scene perception is determined by a match with idiosyncratic internal models. These results demonstrate that visual perception can only be fully understood through the lens of our personally unique models of the world."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:nrtMV_XWKgEC",
        "num_citations": 0,
        "pub_url": "https://psyarxiv.com/98wt7/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jCng5_Qk1GYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "EEG decoding reveals neural predictions for naturalistic material behaviors",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 15.528640, 2023",
            "author": "Daniel Kaiser and Rico Stecher and Katja Doerschner",
            "journal": "bioRxiv",
            "pages": "2023.02. 15.528640",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Material properties like softness or stickiness determine how an object can be used. Based on our real-life experience, we form strong expectations about how objects should behave under force, given their typical material properties. Such expectations have been shown to modulate perceptual processes, but we currently do not know how expectation influences the temporal dynamics of the cortical visual analysis for objects and their materials. Here, we tracked the neural representations of expected and unexpected material behaviors using time-resolved EEG decoding in a violation-of-expectation paradigm, where objects fell to the ground and deformed in expected or unexpected ways. Participants were 25 men and women. Our study yielded three key results: First, both objects and materials were represented rapidly and in a temporally sustained fashion. Second, objects exhibiting unexpected material behaviors were more successfully decoded than objects exhibiting expected behaviors within 190ms after the impact, which might indicate additional processing demands when expectations are unmet. Third, general signals of expectation fulfillment that generalize across specific objects and materials were found within the first 150ms after the impact. Together, our results provide new insights into the temporal neural processing cascade that underlies the analysis of real-world material behaviors. They reveal a sequence of predictions, with cortical signals progressing from a general signature of expectation fulfillment towards increased processing of unexpected material behaviors.In the real world, we can make accurate \u2026"
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:uJ-U7cs_P_0C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.15.528640.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VQbNSel9v64J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Alpha-frequency feedback to early visual cortex orchestrates coherent natural vision",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 10.527986, 2023",
            "author": "Lixiang Chen and Radoslaw Martin Cichy* and Daniel Kaiser*",
            "journal": "bioRxiv",
            "pages": "2023.02. 10.527986",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "For coherent visual experience to emerge, the brain needs to spatially integrate the complex and dynamic information it receives from the environment. To meet this challenge, the visual system uses contextual information from one part of the visual field to create feedback signals that guide analysis in other parts of the visual field. Here, we set out to characterize the nature of this feedback across brain rhythms and cortical regions. In EEG and fMRI experiments, we experimentally recreated the spatially distributed nature of visual inputs by presenting natural videos at different visual field locations. Critically, we manipulated the spatiotemporal congruency of the videos, so that they did or did not demand integration into a coherent percept. Decoding stimulus information from frequency-specific EEG patterns revealed a shift from representations in feedforward-related gamma activity for spatiotemporally inconsistent videos to representations in feedback-related alpha activity for spatiotemporally consistent videos. Our fMRI data suggest high-level scene-selective areas as the putative source of this feedback. Combining the EEG data with spatially resolved fMRI recordings, we demonstrate that alpha-frequency feedback is directly associated with representations in early visual cortex. Together this demonstrates how the human brain orchestrates coherent visual experience across space: it uses feedback to integrate information from high-level to early visual cortex through a dedicated rhythmic code in the alpha frequency range."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:-_dYPAW6P2MC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.10.527986.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:fb5VqTo388UJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Your place or mine? The neural dynamics of personally familiar scene recognition suggests category independent familiarity encoding",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.06. 29.547012, 2023",
            "author": "Hannah Klink and Daniel Kaiser and Rico Stecher and Geza Gergely Ambrus and Gyula Kovacs",
            "journal": "bioRxiv",
            "pages": "2023.06. 29.547012",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Recognizing a stimulus as familiar is an important capacity in our everyday life. Recent investigation of visual processes has led to important insights into the nature of the neural representations of familiarity for human faces. Still, little is known about how familiarity affects the neural dynamics of non-face stimulus processing. Here we report the results of an EEG study, examining the representational dynamics of personally familiar scenes. Participants viewed highly variable images of their own apartments and unfamiliar ones, as well as personally familiar and unfamiliar faces. Multivariate pattern analyses were used to examine the time course of differential processing of familiar and unfamiliar stimuli. Time resolved classification revealed that familiarity is decodable from the EEG data similarly for scenes and faces. The temporal dynamics showed delayed onsets and peaks for scenes as compared to faces. Familiarity information, starting at 200 ms, generalized across stimulus categories and led to a robust familiarity effect. In addition, familiarity enhanced category representations in early (250 to 300 ms) and later (>400 ms) processing stages. Our results extend previous face familiarity results to another stimulus category and suggest that familiarity as a construct can be understood as a general, stimulus-independent processing step during recognition."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:0KyAp5RtaNEC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.06.29.547012.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Integrative processing in artificial and biological vision predicts the perceived beauty of natural images",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.05. 05.539579, 2023",
            "author": "Sanjeev Nara and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.05. 05.539579",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Previous research indicates that the beauty of natural images is already determined during perceptual analysis. However, it is still largely unclear which perceptual computations give rise to the perception of beauty. Theories of processing fluency suggest that the ease of processing for an image determines its perceived beauty. Here, we tested whether perceived beauty is related to the amount of spatial integration across an image, a perceptual computation that reduces processing demands by aggregating image elements into more efficient representations of the whole. We hypothesized that higher degrees of integration reduce processing demands in the visual system and thereby predispose the perception of beauty. We quantified integrative processing in an artificial deep neural network model of vision: We compared activations between parts of the image and the whole image, where the degree of integration was determined by the amount of deviation between activations for the whole image and its constituent parts. This quantification of integration predicted the beauty ratings for natural images across four studies, which featured different stimuli and task demands. In a complementary fMRI study, we show that integrative processing in human visual cortex predicts perceived beauty in a similar way as in artificial neural networks. Together, our results establish integration as a computational principle that facilitates perceptual analysis and thereby mediates the perception of beauty."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:yB1At4FlUx8C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.05.05.539579.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:5WVMrLwMM9MJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "TMS disruption of the lateral prefrontal cortex increases neural activity in the default mode network when naming facial expressions",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.03. 09.531897, 2023",
            "author": "David Pitcher and Magdalena Sliwinska and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.03. 09.531897",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Recognizing facial expressions is dependent on multiple brain networks specialized for different cognitive functions. In the current study participants (N=20) were scanned using functional magnetic resonance imaging (fMRI) while they performed a covert facial expression naming task. Immediately prior to scanning thetaburst transcranial magnetic stimulation (TMS) was delivered over the right lateral prefrontal cortex (PFC), or the vertex control site. A group whole-brain analysis revealed that TMS induced opposite effects in the neural responses across different brain networks. Stimulation of the right PFC (compared to stimulation of the vertex) decreased neural activity in the left lateral PFC but increased neural activity in three nodes of the default mode network (DMN): the right superior frontal gyrus (SFG), right angular gyrus and the bilateral middle cingulate gyrus. A region of interest (ROI) analysis showed that TMS delivered over the right PFC reduced neural activity across all functionally localised face areas (including in the PFC) compared to TMS delivered over the vertex. These results causally demonstrate that visually recognizing facial expressions is dependent on the dynamic interaction of the face processing network and the DMN. Our study also demonstrates the utility of combined TMS / fMRI studies for revealing the dynamic interactions between different functional brain networks."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:t7zJ5fGR-2UC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.03.09.531897.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hVnPRsAA4AQJ:scholar.google.com/",
        "cites_per_year": {}
    }
]