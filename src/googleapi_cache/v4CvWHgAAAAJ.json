[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The effects of visual backward masking on visual spatiotemporal dynamics",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4711-4711, 2023",
            "author": "Siying Xie and Daniel Kaiser and Johannes Singer and Radoslaw Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4711-4711",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Disentangling the neural computations performed through feedforward and feedback information flow in visual processing is challenging. Backward masking is an efficient experimental approach for this purpose, as it effectively interferes with reentrant feedback processing but leaves feedforward processing relatively intact. We used backward masking to dissect the spatiotemporal flow of visual information in the human brain. We briefly presented natural objects which were followed by a dynamic visual mask to participants. The mask could appear shortly after the object (16.7 ms), resulting in low visibility, or with a substantial delay (600ms), resulting in high visibility of the object. We performed multivariate analysis on EEG (n= 32) and fMRI (n= 27) data to characterize temporal and spatial object representations in these two visibility conditions. While visual representations changed rapidly in time for both conditions \u2026"
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:uWiczbcajpAC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791786",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Coherent categorical information triggers integration-related alpha dynamics",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.12. 04.569908, 2023",
            "author": "Lixiang Chen and Radoslaw Martin Cichy and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.12. 04.569908",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "To create coherent visual experiences, the brain spatially integrates the complex and dynamic information it receives from the environment. We previously demonstrated that feedback-related alpha activity carries stimulus-specific information when two spatially and temporally coherent naturalistic inputs can be integrated into a unified percept. In this study, we sought to determine whether such integration-related alpha dynamics are triggered by categorical coherence in visual inputs. In an EEG experiment, we manipulated the degree of coherence by presenting pairs of videos from the same or different categories through two apertures in the left and right visual hemifields. Critically, video pairs could be video-level coherent (i.e., stem from the same video), coherent in their basic-level category, coherent in their superordinate category, or incoherent (i.e., stem from videos from two entirely different categories). We conducted multivariate classification analyses on rhythmic EEG responses to decode between the video stimuli in each condition. As the key result, we significantly decoded the video-level coherent and basic-level coherent stimuli, but not the superordinate coherent and incoherent stimuli, from cortical alpha rhythms. This suggests that alpha dynamics play a critical role in integrating information across space, and that cortical integration processes are flexible enough to accommodate information from different exemplars of the same basic-level category."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:vDijr-p_gm4C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.12.04.569908.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Alpha-frequency feedback to early visual cortex orchestrates coherent naturalistic vision",
            "pub_year": 2023,
            "citation": "Science Advances 9 (45), eadi2321, 2023",
            "author": "Lixiang Chen and Radoslaw M Cichy and Daniel Kaiser",
            "journal": "Science Advances",
            "volume": "9",
            "number": "45",
            "pages": "eadi2321",
            "publisher": "American Association for the Advancement of Science",
            "abstract": "During naturalistic vision, the brain generates coherent percepts by integrating sensory inputs scattered across the visual field. Here, we asked whether this integration process is mediated by rhythmic cortical feedback. In electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) experiments, we experimentally manipulated integrative processing by changing the spatiotemporal coherence of naturalistic videos presented across visual hemifields. Our EEG data revealed that information about incoherent videos is coded in feedforward-related gamma activity while information about coherent videos is coded in feedback-related alpha activity, indicating that integration is indeed mediated by rhythmic activity. Our fMRI data identified scene-selective cortex and human middle temporal complex (hMT) as likely sources of this feedback. Analytically combining our EEG and fMRI data further revealed \u2026"
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:zLWjf1WUPmwC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=14263805169901092477,9318733707643179384",
        "cites_id": [
            "14263805169901092477",
            "9318733707643179384"
        ],
        "pub_url": "https://www.science.org/doi/abs/10.1126/sciadv.adi2321",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:fb5VqTo388UJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2308.09431, 2023",
            "author": "Zejin Lu and Adrien Doerig and Victoria Bosch and Bas Krahmer and Daniel Kaiser and Radoslaw M Cichy and Tim C Kietzmann",
            "journal": "arXiv preprint arXiv:2308.09431",
            "abstract": "Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:uc_IGeMz5qoC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=17139289309057150061",
        "cites_id": [
            "17139289309057150061"
        ],
        "pub_url": "https://arxiv.org/abs/2308.09431",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Category-specific effects of high-level relations in visual search",
            "pub_year": 2023,
            "citation": "OSF preprints, 2023",
            "author": "Nicolas Goupil and Daniel Kaiser and Liuba Papeo",
            "journal": "OSF preprints",
            "abstract": "Recent empirical findings demonstrate that, in visual search for a target in an array of distractors, observers exploit information about object relations to increase search efficiency. We investigated how people searched for interacting people in a crowd, and how the eccentricity of the target affected the search (Experiments 1-3). Participants briefly viewed crowded arrays and had to search for an interacting dyad (two bodies face-to-face) among non-interacting dyads (back-to-back distractors) or vice versa, with the target presented in the attended central location or at peripheral locations. With central targets, we found a search asymmetry, whereby interacting people among non-interacting people were detected better than non-interacting people among interacting people. With peripheral targets, non-interacting targets were detected better than interacting targets. In Experiment 4, we asked whether these asymmetries generalized to object pairs whose spatial relations did or did not form functionally interacting sets (computer screen above keyboard). Results showed that non-interacting targets were detected better than interacting targets, whether presented in central or peripheral locations. Thus, the effect of relational information on visual search is contingent on both stimulus category and attentional focus. Across both stimulus categories (bodies and objects), search is facilitated when individual distractor-items can be organized in larger structured units (social interaction or functional set), effectively reducing the number of distractors. The presentation of social interaction at the attended (central) location breaks this search pattern by readily \u2026"
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:z_wVstp3MssC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=9316523349721484676",
        "cites_id": [
            "9316523349721484676"
        ],
        "pub_url": "https://osf.io/n7uvk/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hA3IIaDwSoEJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Alpha-frequency feedback to early visual cortex orchestrates coherent natural vision",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 10.527986, 2023",
            "author": "Lixiang Chen and Radoslaw Martin Cichy* and Daniel Kaiser*",
            "journal": "bioRxiv",
            "pages": "2023.02. 10.527986",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "For coherent visual experience to emerge, the brain needs to spatially integrate the complex and dynamic information it receives from the environment. To meet this challenge, the visual system uses contextual information from one part of the visual field to create feedback signals that guide analysis in other parts of the visual field. Here, we set out to characterize the nature of this feedback across brain rhythms and cortical regions. In EEG and fMRI experiments, we experimentally recreated the spatially distributed nature of visual inputs by presenting natural videos at different visual field locations. Critically, we manipulated the spatiotemporal congruency of the videos, so that they did or did not demand integration into a coherent percept. Decoding stimulus information from frequency-specific EEG patterns revealed a shift from representations in feedforward-related gamma activity for spatiotemporally inconsistent videos to representations in feedback-related alpha activity for spatiotemporally consistent videos. Our fMRI data suggest high-level scene-selective areas as the putative source of this feedback. Combining the EEG data with spatially resolved fMRI recordings, we demonstrate that alpha-frequency feedback is directly associated with representations in early visual cortex. Together this demonstrates how the human brain orchestrates coherent visual experience across space: it uses feedback to integrate information from high-level to early visual cortex through a dedicated rhythmic code in the alpha frequency range."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:-_dYPAW6P2MC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=14263805169901092477",
        "cites_id": [
            "14263805169901092477"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.10.527986.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:fb5VqTo388UJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Imaginary scenes are represented in cortical alpha activity",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.10. 23.563249, 2023",
            "author": "Rico Stecher and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.10. 23.563249",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Imagining natural scenes enables us to engage with a myriad of simulated environments. How do our brains generate such complex mental images? Recent research suggests that cortical alpha activity carries information about individual objects during visual imagery. However, it remains unclear if more complex imagined contents such as natural scenes are similarly represented in alpha activity. Here, we answer this question by decoding the contents of imagined scenes from rhythmic cortical activity patterns. In an EEG experiment, participants imagined natural scenes based on detailed written descriptions, which conveyed four complementary scene properties: openness, naturalness, clutter level and brightness. By conducting classification analyses on EEG power patterns across neural frequencies, we were able to decode both individual imagined scenes as well as their properties from the alpha band, showing that also the contents of complex visual images are represented in alpha rhythms. An additional cross-classification analysis between alpha power patterns during the imagery task and during a perception task, in which participants were presented images of the described scenes, showed that scene representations in the alpha band are shared between imagery and late stages of perception. This suggests that alpha activity mediates the top-down re-activation of scene-related visual contents during imagery."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:EkHepimYqZsC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.10.23.563249.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "An object numbering task reveals an underestimation of complexity for typically structured scenes",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Alex Carter and Daniel Kaiser",
            "publisher": "PsyArXiv",
            "abstract": "Our visual environments are composed of an abundance of individual objects. The efficiency with which we can parse such rich environments is remarkable. Previous work suggests that this efficiency is partly explained by grouping mechanisms, which allow the visual system to process the objects that surround us as meaningful groups rather than individual entities. Here, we show that the grouping of objects in typically and meaningfully structured environments directly relates to a reduction of perceived complexity. In an object numerosity discrimination task, we showed participants pairs of schematic scene miniatures, in which objects were structured in typical or atypical ways, and asked them to judge which scene consisted of more individual objects. We obtained two key results: First, participants were less accurate in comparing numerosities between typically structured scenes than between atypically structured scenes, suggesting that grouping processes hinder the effective individuation of separate objects in typically structured scenes. Second, participants underestimated the number of objects in typically structured, compared to atypically structured, scenes, suggesting that grouping based on typical object configurations reduces the perceived numerical complexity of a scene. In a control experiment, we show that this overestimation is specific to upright scenes, indicating that is not related to basic visual feature differences between typically and atypically structured scenes. Together, our results suggest that our visual surroundings appear less complex to the visual system than the number of objects in them makes us believe."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:ipzZ9siozwsC",
        "num_citations": 0,
        "pub_url": "https://osf.io/preprints/psyarxiv/n8hvr/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Individual differences in internal models explain idiosyncrasies in scene perception",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Matthew J Foxwell* and Gongting Wang* and Radoslaw M Cichy and David Pitcher and Daniel Kaiser",
            "publisher": "PsyArXiv",
            "abstract": "According to predictive processing theories, vision is facilitated by predictions derived from our internal models of what the world should look like. However, the contents of these models and how they vary across people remains unclear. Here, we use drawing to directly access the contents of the internal models of individual participants. Participants were first asked to draw typical versions of scene categories, as descriptors of their internal models. These drawings were converted into standardized 3d renders, which we used as stimuli in subsequent scene categorization experiments. Across two experiments, participants\u2019 scene categorization was more accurate for renders tailored to their own drawings compared to renders based on others\u2019 drawings or copies of scene photographs, suggesting that scene perception is determined by a match with idiosyncratic internal models. These results demonstrate that visual perception can only be fully understood through the lens of our personally unique models of the world."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:nrtMV_XWKgEC",
        "num_citations": 0,
        "pub_url": "https://psyarxiv.com/98wt7/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jCng5_Qk1GYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "EEG decoding reveals neural predictions for naturalistic material behaviors",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 15.528640, 2023",
            "author": "Daniel Kaiser and Rico Stecher and Katja Doerschner",
            "journal": "bioRxiv",
            "pages": "2023.02. 15.528640",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Material properties like softness or stickiness determine how an object can be used. Based on our real-life experience, we form strong expectations about how objects should behave under force, given their typical material properties. Such expectations have been shown to modulate perceptual processes, but we currently do not know how expectation influences the temporal dynamics of the cortical visual analysis for objects and their materials. Here, we tracked the neural representations of expected and unexpected material behaviors using time-resolved EEG decoding in a violation-of-expectation paradigm, where objects fell to the ground and deformed in expected or unexpected ways. Participants were 25 men and women. Our study yielded three key results: First, both objects and materials were represented rapidly and in a temporally sustained fashion. Second, objects exhibiting unexpected material behaviors were more successfully decoded than objects exhibiting expected behaviors within 190ms after the impact, which might indicate additional processing demands when expectations are unmet. Third, general signals of expectation fulfillment that generalize across specific objects and materials were found within the first 150ms after the impact. Together, our results provide new insights into the temporal neural processing cascade that underlies the analysis of real-world material behaviors. They reveal a sequence of predictions, with cortical signals progressing from a general signature of expectation fulfillment towards increased processing of unexpected material behaviors.In the real world, we can make accurate \u2026"
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:uJ-U7cs_P_0C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.15.528640.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VQbNSel9v64J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Your place or mine? The neural dynamics of personally familiar scene recognition suggests category independent familiarity encoding",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.06. 29.547012, 2023",
            "author": "Hannah Klink and Daniel Kaiser and Rico Stecher and Geza Gergely Ambrus and Gyula Kov\u00e1cs",
            "journal": "bioRxiv",
            "pages": "2023.06. 29.547012",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Recognizing a stimulus as familiar is an important capacity in our everyday life. Recent investigation of visual processes has led to important insights into the nature of the neural representations of familiarity for human faces. Still, little is known about how familiarity affects the neural dynamics of non-face stimulus processing. Here we report the results of an EEG study, examining the representational dynamics of personally familiar scenes. Participants viewed highly variable images of their own apartments and unfamiliar ones, as well as personally familiar and unfamiliar faces. Multivariate pattern analyses were used to examine the time course of differential processing of familiar and unfamiliar stimuli. Time resolved classification revealed that familiarity is decodable from the EEG data similarly for scenes and faces. The temporal dynamics showed delayed onsets and peaks for scenes as compared to faces. Familiarity information, starting at 200 ms, generalized across stimulus categories and led to a robust familiarity effect. In addition, familiarity enhanced category representations in early (250 to 300 ms) and later (>400 ms) processing stages. Our results extend previous face familiarity results to another stimulus category and suggest that familiarity as a construct can be understood as a general, stimulus-independent processing step during recognition."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:0KyAp5RtaNEC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.06.29.547012.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Integrative processing in artificial and biological vision predicts the perceived beauty of natural images",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.05. 05.539579, 2023",
            "author": "Sanjeev Nara and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.05. 05.539579",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Previous research indicates that the beauty of natural images is already determined during perceptual analysis. However, it is still largely unclear which perceptual computations give rise to the perception of beauty. Theories of processing fluency suggest that the ease of processing for an image determines its perceived beauty. Here, we tested whether perceived beauty is related to the amount of spatial integration across an image, a perceptual computation that reduces processing demands by aggregating image elements into more efficient representations of the whole. We hypothesized that higher degrees of integration reduce processing demands in the visual system and thereby predispose the perception of beauty. We quantified integrative processing in an artificial deep neural network model of vision: We compared activations between parts of the image and the whole image, where the degree of integration was determined by the amount of deviation between activations for the whole image and its constituent parts. This quantification of integration predicted the beauty ratings for natural images across four studies, which featured different stimuli and task demands. In a complementary fMRI study, we show that integrative processing in human visual cortex predicts perceived beauty in a similar way as in artificial neural networks. Together, our results establish integration as a computational principle that facilitates perceptual analysis and thereby mediates the perception of beauty."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:yB1At4FlUx8C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.05.05.539579.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:5WVMrLwMM9MJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "TMS disruption of the lateral prefrontal cortex increases neural activity in the default mode network when naming facial expressions",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.03. 09.531897, 2023",
            "author": "David Pitcher and Magdalena Sliwinska and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.03. 09.531897",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Recognizing facial expressions is dependent on multiple brain networks specialized for different cognitive functions. In the current study participants (N=20) were scanned using functional magnetic resonance imaging (fMRI) while they performed a covert facial expression naming task. Immediately prior to scanning thetaburst transcranial magnetic stimulation (TMS) was delivered over the right lateral prefrontal cortex (PFC), or the vertex control site. A group whole-brain analysis revealed that TMS induced opposite effects in the neural responses across different brain networks. Stimulation of the right PFC (compared to stimulation of the vertex) decreased neural activity in the left lateral PFC but increased neural activity in three nodes of the default mode network (DMN): the right superior frontal gyrus (SFG), right angular gyrus and the bilateral middle cingulate gyrus. A region of interest (ROI) analysis showed that TMS delivered over the right PFC reduced neural activity across all functionally localised face areas (including in the PFC) compared to TMS delivered over the vertex. These results causally demonstrate that visually recognizing facial expressions is dependent on the dynamic interaction of the face processing network and the DMN. Our study also demonstrates the utility of combined TMS / fMRI studies for revealing the dynamic interactions between different functional brain networks."
        },
        "filled": true,
        "author_pub_id": "v4CvWHgAAAAJ:t7zJ5fGR-2UC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.03.09.531897.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hVnPRsAA4AQJ:scholar.google.com/",
        "cites_per_year": {}
    }
]