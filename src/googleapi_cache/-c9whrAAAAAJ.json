[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Semantic object processing is modulated by prior scene context",
            "pub_year": 2023,
            "citation": "Language, Cognition and Neuroscience, 1-10, 2023",
            "author": "Alexandra Krugliak and Dejan Draschkow and Melissa L-H V\u00f5 and Alex Clarke",
            "journal": "Language, Cognition and Neuroscience",
            "pages": "1-10",
            "publisher": "Routledge",
            "abstract": "Objects that are congruent with a scene are recognised more efficiently than objects that are incongruent. Further, semantic integration of incongruent objects elicits a stronger N300/N400 EEG component. Yet, the time course and mechanisms of how contextual information supports access to semantic object information is unclear. We used computational modelling and EEG to test how context influences semantic object processing. Using representational similarity analysis, we established that EEG patterns dissociated between objects in congruent or incongruent scenes from around 300\u2005ms. By modelling the semantic processing of objects using independently normed properties, we confirm that the onset of semantic processing of both congruent and incongruent objects is similar (\u223c150 ms). Critically, after \u223c275 ms, we discover a difference in the duration of semantic integration, lasting longer for incongruent \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:eq2jaN3J8jMC",
        "num_citations": 0,
        "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/23273798.2023.2279083",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mFXj7jGbVfkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Access to meaning from visual input: Object and word frequency effects in categorization behavior.",
            "pub_year": 2023,
            "citation": "Journal of Experimental Psychology: General, 2023",
            "author": "Klara Gregorov\u00e1 and Jacopo Turini and Benjamin Gagl and Melissa Le-Hoa V\u00f5",
            "journal": "Journal of Experimental Psychology: General",
            "publisher": "American Psychological Association",
            "abstract": "Object and word recognition are both cognitive processes that transform visual input into meaning. When reading words, the frequency of their occurrence (\u201cword frequency,\u201d WF) strongly modulates access to their meaning, as seen in recognition performance. Does the frequency of objects in our world also affect access to their meaning? With object labels available in real-world image datasets, one can now estimate the frequency of occurrence of objects in scenes (\u201cobject frequency,\u201d OF). We explored frequency effects in word and object recognition behavior by employing a natural versus man-made categorization task (Experiment 1) and a matching\u2013mismatching priming task (Experiments 2\u20133). In Experiment 1, we found a WF effect for both words and objects but no OF effect. In Experiment 2, we replicated the WF effect for both stimulus types during cross-modal priming but not uni-modal priming. Moreover, in \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:q3oQSFYPqjQC",
        "num_citations": 4,
        "citedby_url": "/scholar?hl=en&cites=8341788289603668324",
        "cites_id": [
            "8341788289603668324"
        ],
        "pub_url": "https://psycnet.apa.org/record/2023-69316-001",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZAlXsUD8w3MJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 2,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Disentangling diagnostic object properties for human scene categorization",
            "pub_year": 2023,
            "citation": "Scientific Reports 13 (1), 5912, 2023",
            "author": "Sandro L Wiesmann and Melissa L-H V\u00f5",
            "journal": "Scientific Reports",
            "volume": "13",
            "number": "1",
            "pages": "5912",
            "publisher": "Nature Publishing Group UK",
            "abstract": "It usually only takes a single glance to categorize our environment into different scene categories (e.g. a kitchen or a highway). Object information has been suggested to play a crucial role in this process, and some proposals even claim that the recognition of a single object can be sufficient to categorize the scene around it. Here, we tested this claim in four behavioural experiments by having participants categorize real-world scene photographs that were reduced to a single, cut-out object. We show that single objects can indeed be sufficient for correct scene categorization and that scene category information can be extracted within 50 ms of object presentation. Furthermore, we identified object frequency and specificity for the target scene category as the most important object properties for human scene categorization. Interestingly, despite the statistical definition of specificity and frequency, human ratings of these \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:Fu2w8maKXqMC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=13498392154162091884",
        "cites_id": [
            "13498392154162091884"
        ],
        "pub_url": "https://www.nature.com/articles/s41598-023-32385-y",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:bFfAiyXsU7sJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Entwicklung und Einsatz von VR-Lernszenarien f\u00fcr den Lehrkompetenzaufbau: Klassenraumsimulationen mit Virtual Reality",
            "pub_year": 2023,
            "citation": "Lehr-Lern-Labore und Digitalisierung, 211-224, 2023",
            "author": "Laura Glocker and Sebastian Breitenbach and Miriam Hansen and Julia Mendzheritskaya and Melissa L\u00ea-Hoa V\u00f5",
            "pages": "211-224",
            "publisher": "Springer Fachmedien Wiesbaden",
            "abstract": "In diesem Beitrag werden Entwicklung, Einsatz und Evaluation einer selbst entwickelten VR-Klassensimulation \u201aCLASIVIR 1.0\u2018 vorgestellt, in der den Studierenden eine virtuelle Situation pr\u00e4sentiert wurde. Den theoretischen Rahmen f\u00fcr Die Entwicklung und der Einsatz von VR-Lernszenarien innerhalb einer Klassenraumsimulation zum Lehrkompetenzerwerb f\u00fcr die vorliegende Studie wurde theoretisch abgeleitet, unter anderem basierend auf den Basisdimensionen guten Unterrichts. Die Pilotierung des ersten VR-Lernszenarios wurde in ein bildungswissenschaftliches Seminar f\u00fcr Lehramtsstudierende zur F\u00f6rderung von digitalen Kompetenzen durch Virtual Reality als ein VR-Lehr-Lern-Labor integriert. F\u00fcr Evaluationszwecke wurden die Studierenden im Rahmen von Pr\u00e4-Post-Befragungen zum emotionalen Erleben sowie zu Einstellungen zu digitalen befragt. Im Beitrag werden sowohl VR-Lehr-Lern-Labore \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:_Ybze24A_UAC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=1535785716765454004",
        "cites_id": [
            "1535785716765454004"
        ],
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-658-40109-2_22",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:tG7s8vg0UBUJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Using XR (Extended Reality) for Behavioral, Clinical, and Learning Sciences Requires Updates in Infrastructure and Funding",
            "pub_year": 2023,
            "citation": "Policy Insights from the Behavioral and Brain Sciences 10 (2), 317-323, 2023",
            "author": "Dejan Draschkow and Nicola C Anderson and Erwan David and Nathan Gauge and Alan Kingstone and Levi Kumle and Xavier Laurent and Anna C Nobre and Sally Shiels and Melissa L-H V\u00f5",
            "journal": "Policy Insights from the Behavioral and Brain Sciences",
            "volume": "10",
            "number": "2",
            "pages": "317-323",
            "publisher": "SAGE Publications",
            "abstract": "Extended reality (XR, including augmented and virtual reality) creates a powerful intersection between information technology and cognitive, clinical, and education sciences. XR technology has long captured the public imagination, and its development is the focus of major technology companies. This article demonstrates the potential of XR to (1) deliver behavioral insights, (2) transform clinical treatments, and (3) improve learning and education. However, without appropriate policy, funding, and infrastructural investment, many research institutions will struggle to keep pace with the advances and opportunities of XR. To realize the full potential of XR for basic and translational research, funding should incentivize (1) appropriate training, (2) open software solutions, and (3) collaborations between complementary academic and industry partners. Bolstering the XR research infrastructure with the right investments and \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:uJ-U7cs_P_0C",
        "num_citations": 0,
        "pub_url": "https://journals.sagepub.com/doi/abs/10.1177/23727322231196305",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Aging attenuates the memory advantage for unexpected objects in real-world scenes",
            "pub_year": 2023,
            "citation": "Heliyon 9 (9), 2023",
            "author": "Lena Klever and Jasmin Islam and Melissa Le-Hoa V\u00f5 and Jutta Billino",
            "journal": "Heliyon",
            "volume": "9",
            "number": "9",
            "publisher": "Elsevier",
            "abstract": "Across the adult lifespan memory processes are subject to pronounced changes. Prior knowledge and expectations might critically shape functional differences; however, corresponding findings have remained ambiguous so far. Here, we chose a tailored approach to scrutinize how schema (in-)congruencies affect older and younger adults' memory for objects embedded in real-world scenes, a scenario close to everyday life memory demands. A sample of 23 older (52\u201381 years) and 23 younger adults (18\u201338 years) freely viewed 60 photographs of scenes in which target objects were included that were either congruent or incongruent with the given context information. After a delay, recognition performance for those objects was determined. In addition, recognized objects had to be matched to the scene context in which they were previously presented. While we found schema violations beneficial for object \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:j8SEvjWlNXcC",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/heliyon/pdf/S2405-8440(23)07449-2.pdf",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Viewpoint dependence and scene context effects generalize to depth rotated three-dimensional objects",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (10), 9-9, 2023",
            "author": "Aylin Kallmayer and Melissa L-H V\u00f5 and Dejan Draschkow",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "10",
            "pages": "9-9",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Viewpoint effects on object recognition interact with object-scene consistency effects. While recognition of objects seen from \u201cnoncanonical\u201d viewpoints (eg, a cup from below) is typically impeded compared to processing of objects seen from canonical viewpoints (eg, the string-side of a guitar), this effect is reduced by meaningful scene context information. In the present study we investigated if these findings established by using photographic images, generalize to strongly noncanonical orientations of three-dimensional (3D) models of objects. Using 3D models allowed us to probe a broad range of viewpoints and empirically establish viewpoints with very strong noncanonical and canonical orientations. In Experiment 1, we presented 3D models of objects from six different viewpoints (0, 60, 120, 180 240, 300) in color (1a) and grayscaled (1b) in a sequential matching task. Viewpoint had a significant effect on accuracy and response times. Based on the viewpoint effect in Experiments 1a and 1b, we could empirically determine the most canonical and noncanonical viewpoints from our set of viewpoints to use in Experiment 2. In Experiment 2, participants again performed a sequential matching task, however now the objects were paired with scene backgrounds which could be either consistent (eg, a cup in the kitchen) or inconsistent (eg, a guitar in the bathroom) to the object. Viewpoint interacted significantly with scene consistency in that object recognition was less affected by viewpoint when consistent scene information was provided, compared to inconsistent information. Our results show that scene context supports object recognition even \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:fEOibwPWpKIC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792757",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Vowel World 2.0. Implicit learning of artificial scene grammar. Exp 1",
            "pub_year": 2023,
            "citation": "OSF, 2023",
            "author": "Yuri Markov and Jeremy Wolfe and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "Scene guidance can be quite difficult to investigate in real scenes: Not all assets of a complex scene can be manipulated or controlled, editing/manipulating scenes is very time consuming and creates artifacts, and the number of scenes is often limited by labor intense manipulations. Using artificial scenes that mimic characteristics of real scenes and which are created according to artificial rules could help. In VW 2.0 we control various parameters using simplified scenes (basically a 16x16 grid onto which vowels and consonants are overlaid) which contain different \u201cobjects\u201d and targets, which follow different rules. We used a new version of \u201cVowelWorld\u201d(VW), a paradigm that allows to control for different types of guidance rules in artificial displays. The previous version had a 10x10 grid with colored cells and letters placed on it and followed three rules: color rule (certain targets were placed on certain color regions), structure rule (vowels were placed on circles), and letter rule (vowels were close to neighboring consonants of the alphabet). In the new version, the grid has one color tone and contains artificial \u201cobjects\u201d, these are connect groups of cells, distinguished from the background and other objects by use of shadow cues to imply stratification in depth. Importantly, this version tried to incorporate so-called \u201canchor objects\u201d(Boettcher, Draschkow, Dienhart, & V\u00f5, 2018; V\u00f5, 2021) in the scene, which are important for guiding search in real-world scenes."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:ZfRJV9d4-WMC",
        "num_citations": 0,
        "pub_url": "https://osf.io/382e9/resources",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Just look away: Could attention allocation to scene grammar violations during unrelated object searches be modulated by individual differences in language experience?",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5289-5289, 2023",
            "author": "Naomi Vingron and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5289-5289",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Disengaging attention from task-irrelevant information is not only a process crucial to navigating the world around us but may also be a skill that bilinguals are particularly well-practiced at as they are constantly juggling information from multiple languages. The current study investigates the extent to which individual differences in language experience are associated with increased object search efficiency in scenes containing mismatches between objects and scene context. Efficient search in real-world scenes relies on a set of rules (\u201cscene grammar\u201d), which support object localization and identification (eg, knowing that a spatula goes neither in a toaster nor a bathroom). Violations to these rules may impair processing as viewers need to resolve ambiguity resulting from the unexpected element of the scene and have been shown to modulate ongoing eye-movements, even when they are irrelevant to the task at \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:-_dYPAW6P2MC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792099",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "How real can they get? Investigating neural responses to GAN generated scenes.",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5703-5703, 2023",
            "author": "Aylin Kallmayer and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5703-5703",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "To understand how we efficiently navigate real-world scenes, we need to unravel the underlying computations and structure of representations that afford efficient scene processing. One hypothesis is that we exploit scene structures by learning hierarchical object-to-object and scene-to-object relations captured by a scene grammar. But how can these high-level networks be learnt? Does unsupervised learning automatically lead to representations that reflect properties of scene grammar? To assess how well scenes generated by generative adversarial networks (GANs) capture real-world scene structure perceived over time we conducted an EEG experiment. Participants viewed 180 generated scenes across six categories (30 exemplars/category). Generated scenes varied in their \u201crealness\u201d as assessed by three different measures: realism ratings, false-alarm (FA) rates, and categorization performance for 50 and \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:35r97b3x0nAC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792570",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Artificial Scene Grammar Acquisition",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5656-5656, 2023",
            "author": "Maxim Spur and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5656-5656",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Statistical learning has been demonstrated to be a valid method of learning the structure of invented languages\u2014simple exposure to consecutive nonsensical syllables allows learning of the transitional probabilities reflecting the rules of languages. Does the acquisition of scene grammar, ie, the rules governing the structure of naturalistic scene environments, occur in a similar way, or are more interactive approaches more effective, which would more closely reflect the actual way we learn our environments? To answer this question, we attempted to create a number of \u201cinvented\u201d scenes with 12 \u201cnonsensical\u201d objects each. These objects were divided into \u201canchors\u201d(usually larger objects that predict the location of other objects), and \u201clocals,\u201d which were each assigned to an anchor, ie, appearing close to it 80% of the time. Participants were exposed to these objects in a virtual reality environment in two blocks (active \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:evX43VCCuoAC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792613",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Virtual reality protocol for decomposing complex behaviour into tractable subcomponents.",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4996-4996, 2023",
            "author": "Levi Kumle and Anna C Nobre and Melissa Vo and Dejan Draschkow",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4996-4996",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Understanding temporally extended natural behaviour requires studying the coordination of vision, action, and memory during free-flowing interactions with the environment. However, naturalistic experiments capturing the complexity of natural behaviour are often incompatible with the desire for tight experimental control. Therefore, sub-components of natural behaviour have been predominantly studied in isolation using desktop-based lab tasks\u2013missing potential interconnections between these sub-components that only emerge during continuous and temporally extended behaviour. Here, we share a unique open virtual reality (VR) resource for quantifying and segmenting continuous behaviour into tractable sub-units. Within a single temporally extended natural task, participants copied a Model display by selecting realistic objects from a Resource pool and placing them into a Workspace. We track head, hand \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:2KloaMYe4IUC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791518",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dynamics of gaze and body while viewing omnidirectional stimuli",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5123-5123, 2023",
            "author": "Erwan David and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5123-5123",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Recent vision studies conducted in virtual reality (VR) are helping to understand what roles the eye and the head play while we observe scenes that surround us. Unfortunately, very few studies have shown interest in the contributions of the rest of the body to gaze movements. To shed some light on this subject we have designed a protocol to gather tracking data about torso and leg movements, in addition to the eye and head tracking. Wearing a VR headset and trackers on their torso and leg, our participants observed scenes that were either simple (Gabor patches, 3D shapes) or complex (360 photos, 3D rooms). Stimuli were either flat on the surface of a sphere or fully 3D. Additionally, half of the trials were either free-viewing or followed by a recall task to study a potential effect of goal-direction (all trials lasted 10s). We show that under the impetus of a goal participants made longer saccades (and shorter fixations \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:tzM49s52ZIMC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792248",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Investigating the effects of a virtual reality vs. screen-based testing setup on incidental memory after visual search through scenes",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5126-5126, 2023",
            "author": "Julia Beitner and Jason Helbing and Erwan J David and Melissa L-H Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5126-5126",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Many experiments investigating visual search are performed on a computer screen under highly controlled settings. However, the generalizability of the investigated search mechanisms to the real world is unclear. Here, we tested whether the formation of incidental object memories during visual search follows similar patterns in virtual reality (VR) and on a computer screen. We present two studies that were identical in the administered tasks, but differed in the setup they were tested in. In both experiments, participants searched for ten out of 20 objects in indoor scenes, either in full illumination or with constrained visual input via a controller-contingent 8-degree window (flashlight condition). After the search task, participants\u2019 incidental memory was tested with two surprise tasks: an object recognition and a location memory task (scene rebuilding). Critically, the first experiment took place in an immersive virtual \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:_Re3VWB3Y0AC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792245",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Recollection and familiarity in the search superiority effect: ERPs, ROCs, and remember\u2013know judgements of search targets and intentionally memorized objects in scenes",
            "pub_year": 2023,
            "citation": "OSF, 2023",
            "author": "Jason Helbing and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "We intend to combine electroencephalography (EEG) and eye tracking in an experiment to investigate recognition memory representations of objects in scenes which are either (a) searched for or (b) intentionally memorized. This is motivated by the surprising finding that search targets tend to be memorized better than intentionally memorized objects (search superiority effect). We intend to answer the question whether this effect is purely quantitative (stronger memory) or whether we can also find indicators of different cognitive processes related to the recognition of search targets and intentionally memorized objects (stronger and different memory). The two processes we hypothesize to be related differentially to search and memorization are recollection (knowing precisely that something has been encountered, typically with detailed information on the context; hard threshold signal of yes vs. no) and familiarity (vaguely knowing something has been encountered before without details about the encoding context; continuous strength signal) as described by dual-process models of recognition memory. We aim to analyze event-related potentials (ERPs), receiver operating characteristics (ROCs), and remember\u2013know-judgements to investigate this potential link."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:tKAzc9rXhukC",
        "num_citations": 0,
        "pub_url": "https://osf.io/hvfpq/resources",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The Salient360! Toolbox: Processing, Visualising and Comparing Gaze Data in 3D",
            "pub_year": 2023,
            "citation": "Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, 1-8, 2023",
            "author": "Erwan David and Jes\u00fas Guti\u00e9rrez and Melissa Le-Hoa Vo and Antoine Coutrot and Matthieu Perreira Da Silva and Patrick Le Callet",
            "pages": "1-8",
            "abstract": " Eye tracking can serve as a gateway to studying the mind. For this reason it has been adopted by a diverse range of scientific communities. With the improvement of the quality of head-mounted virtual reality devices (HMDs) over the past 10 years, eye tracking has been added to capture gaze in immersive environments. The use of HMDs with eye tracking is increasing significantly and so is the need for a toolbox enabling consensus about eye tracking methods in 3D. We present the Salient360! toolbox: it implements functions to identify saccades and fixations and output gaze characteristics (e.g., fixation duration or saccade directions), to generate saliency maps, fixation maps, and scanpath data. It also implements routines made to compare gaze data that were adapted to 3D. We hope that this toolbox will spark discussions about the methodology of 3D gaze processing, facilitate running experiments, and improve \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:kzcrU_BdoSEC",
        "num_citations": 0,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3588015.3588406",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Y96wLbrMZvgJ:scholar.google.com/",
        "cites_per_year": {}
    }
]