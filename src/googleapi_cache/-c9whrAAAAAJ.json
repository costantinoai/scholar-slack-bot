[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Recollection and familiarity in the search superiority effect: Event-related potentials, receiver operating characteristics, and remember\u2013know judgments of incidental and intentional memory of objects in scenes",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Jason Helbing and Dejan Draschkow and Melissa L-H Vo",
            "publisher": "OSF"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:epqYDVWIO7EC",
        "num_citations": 0,
        "pub_url": "https://osf.io/yd729/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:eSNYeH8ReqMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "More than meets the eye: Neural evidence for scene grammar representations during individual object processing.",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Aylin Kallmayer and Leila Zacharias and Luisa Jetter and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "Objects in real-world scenes adhere to regular arrangements. The resulting compositions can be described by a scene grammar\u2013a framework that captures hierarchically structured relationships between objects in real-world scenes wherein phrases refer to clusters of frequently co-occurring objects. Within such phrases, anchor objects (eg, sink) are predictive of surrounding local objects (eg, toothbrush). Do neural representations of objects follow the structure suggested by scene grammar? In this EEG study we characterize the temporal dynamics of phrase specific shared representations quantified via crossclassification analysis. That is, training classifiers on an object categorization task based on neural data from one type of object (anchor or local) and testing generalizations to the held out set of objects. We find an early cluster of timepoints between 130 and 160 ms which carry phrase specific shared representations. Next, we predict the format of shared representations from a range of encoded features in a generalized linear model using representational similarity analysis (RSA). We find that, in general, classifiers use similarity in high-level visual and semantic features when generalizing between anchor and local objects and not just similarity in low-level visual features between stimuli. Crucially,\u201cupward\u201d generalization from local to anchor objects was driven by co-occurrence statistics and high-level visual features.\u201cDownward\u201d generalization was driven by high-level semantic features, action similarity and co-occurrence statistics. We offer novel insights into the dynamics and format of neural representations underlying the intricate \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:ipzZ9siozwsC",
        "num_citations": 0,
        "pub_url": "https://osf.io/hs835/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:zWh-TvNI0VQJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Using a flashlight-contingent window paradigm to investigate visual search and object memory in virtual reality and on computer screens",
            "pub_year": 2024,
            "citation": "Scientific Reports 14 (1), 8596, 2024",
            "author": "Julia Beitner and Jason Helbing and Erwan Jo\u00ebl David and Melissa L\u00ea-Hoa V\u00f5",
            "journal": "Scientific Reports",
            "volume": "14",
            "number": "1",
            "pages": "8596",
            "publisher": "Nature Publishing Group UK",
            "abstract": "A popular technique to modulate visual input during search is to use gaze-contingent windows. However, these are often rather discomforting, providing the impression of visual impairment. To counteract this, we asked participants in this study to search through illuminated as well as dark three-dimensional scenes using a more naturalistic flashlight with which they could illuminate the rooms. In a surprise incidental memory task, we tested the identities and locations of objects encountered during search. Importantly, we tested this study design in both immersive virtual reality (VR; Experiment 1) and on a desktop-computer screen (Experiment 2). As hypothesized, searching with a flashlight increased search difficulty and memory usage during search. We found a memory benefit for identities of distractors in the flashlight condition in VR but not in the computer screen experiment. Surprisingly, location memory was \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:uc_IGeMz5qoC",
        "num_citations": 0,
        "pub_url": "https://www.nature.com/articles/s41598-024-58941-8",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:3g4tj8yg3zUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Multifaceted consequences of visual distraction during natural behaviour",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Levi Kumle and Melissa L-H Vo and Kia Nobre and Dejan Draschkow",
            "publisher": "OSF",
            "abstract": "Visual distraction is a ubiquitous aspect of everyday life. Studying the consequences of distraction during temporally extended tasks, however, is not tractable with traditional methods. Here we developed a virtual reality approach that segments complex behaviour into cognitive subcomponents, including encoding, visual search, working memory usage, and decision-making. Participants copied a model display by selecting objects from a resource pool and placing them into a workspace. By manipulating the distractibility of objects in the resource pool, we discovered interfering effects of distraction across the different cognitive subcomponents. We successfully traced the consequences of distraction all the way from overall task performance to the decision-making processes that gate memory usage. Distraction slowed down behaviour and increased costly body movements. Critically, distraction increased encoding demands, slowed visual search, and decreased reliance on working memory. Our findings illustrate that effects of visual distraction during natural behaviour can be rather focal but nevertheless have cascading consequences."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:0KyAp5RtaNEC",
        "num_citations": 0,
        "pub_url": "https://osf.io/v7dwp/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:JM29R96_FycJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "More is not always better: Temporal neural signatures of object-driven versus scene-driven human scene categorization",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Elia Samuel Rothenberg and Aylin Kallmayer and Sandro Wiesmann and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "Scene categorization is an impressively rapid process (Potter et al., 2014). Examining the question which type of visual information is crucial for scene categorization to operate efficiently in the early stages of perception has sparked an ongoing debate among cognitive psychologists. Explanatory propositions have emphasized the importance of either object-centered (eg, De Graef et al. 1990) or scene-centered (eg, Greene & Oliva, 2009) processing routes with many also acknowledging integrative dual pathways to scene recognition (eg, MacEvoy & Epstein, 2011). While there is ample evidence for the importance of both local and global scene information, recent work by our research group (Wiesmann & V\u00f5, 2022; Wiesmann & V\u00f5, 2023a; Wiesmann & V\u00f5, 2023b) has suggested that object information is generally more useful to observers for rapid scene categorization, especially if these objects exhibit high specificity to a given scene category and demonstrate frequent occurrence within scenes belonging to that particular category. With increasing stimulus onset asynchrony and decreasing object diagnosticity, utility of global scene information is thought to increase. In this experiment, we now intend to use electroencephalography (EEG) to investigate the temporal dynamics and neural signatures of local and global scene information in the scene categorization process."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:yB1At4FlUx8C",
        "num_citations": 0,
        "pub_url": "https://osf.io/zm97y/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:QuLjFr4u5zcJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Development of Scene Hierarchy",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Dilara Deniz T\u00fcrk and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "The arrangement of visual scenes enables us to anticipate the typical spatial placement of objects within them (Biederman, Mezzanotte & Rabinowitz, 1982). Just as the distinction between semantics and syntax exists in language, we possess knowledge of object locations within a scene. We designate this configuration as scene grammar (for a review, see V\u00f5, 2021), which adheres to physical principles and customary usage. Studies have demonstrated that scene grammar can be hierarchically structured (V\u00f5, Boettcher & Draschkow, 2019). Building upon these findings, objects linked by spatial proximity create clusters termed\" phrases.\" Each phrase is centered around an anchor object (eg, toilet), indicating the identity and location of other objects (local objects, eg, toilet paper) within that cluster. In a prior study within our research group (Turini & V\u00f5, 2022), this hierarchy was examined across two modalities (verbal and non-verbal) using the odd-one-out task which was previously employed to investigate the perceptual and conceptual dimensions that underlie the cognitive representation of objects (Hebart et al., 2019). Participants were instructed to choose the \u201codd\u201d object from triplets without receiving additional clarification regarding the criteria for the oddness of the objects. Stronger similarity judgements were observed for object pairs belonging to the same scene and pairs belonging to the same phrase of a scene compared to those in different phrases of the same scene, indicating the influence of spatial context on perceptual judgements. The influence of the scene hierarchy on similarity ratings manifested itself similarly across both \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:nrtMV_XWKgEC",
        "num_citations": 0,
        "pub_url": "https://osf.io/tzxcu/resources",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:YFIYYH98d7kJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The Salient360! toolbox: Handling gaze data in 3D made easy",
            "pub_year": 2024,
            "citation": "Computers & Graphics, 103890, 2024",
            "author": "Erwan David and Jes\u00fas Guti\u00e9rrez and Melissa L\u00e8-Hoa V\u00f5 and Antoine Coutrot and Matthieu Perreira Da Silva and Patrick Le Callet",
            "journal": "Computers & Graphics",
            "pages": "103890",
            "publisher": "Pergamon",
            "abstract": "Eye tracking has historically been a very popular tool. The data it records allow us to understand how people behave and what they attend to within our visual world; under this perspective the experiments, applications and use-cases are endless. Therefore, it is not surprising to witness a strong rise in the use of eXtended Reality (XR) devices with embedded eye trackers in research. These devices allow for less obtrusive experimenting conditions, and a significantly higher experimental control compared to traditional desktop testing. The use of eye tracking in XR is increasing and so is the need for a toolbox enabling consensus about eye tracking methods in 3D. We present the Salient360! toolbox: it implements functions to identify saccades and fixations and output gaze features (e.g., saccade directions) to generate saliency maps, fixation maps, and scanpath data. It implements comparisons of gaze data with \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:9Nmd_mFXekcC",
        "num_citations": 0,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0097849324000177",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:PzT6cEfNYrcJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Making a scene\u2013using GAN generated scenes to test the role of real-world co-occurence statistics and hierarchical feature spaces in scene understanding.",
            "pub_year": 2024,
            "citation": "",
            "author": "Aylin Kallmayer and Melissa V\u00f5",
            "abstract": "Our visual surroundings are highly complex. Despite this, we understand and navigate them effortlessly. This requires a complex series of transformations resulting in representations that not only span low-to high-level visual features (eg, contours, textures, object parts and objects), but likely also reflect co-occurrence statistics of objects in real-world scenes. Here, so-called anchor objects reflect clustering statistics in real-world scenes, anchoring predictions towards frequently co-occuring smaller objects, while so-called diagnostic objects predict the larger semantic context. We investigate which of these properties underly scene understanding across two dimensions\u2013realism and categorisation\u2013using scenes generated from Generative Adversarial Networks (GANs) which naturally vary along these dimensions. We show that anchor objects and mainly high-level features extracted from a range of pre-trained deep neural networks (DNNs) drove realism both at first glance and after initial processing. Categorisation performance was mainly determined by diagnostic objects, regardless of realism and DNN features, also at first glance and after initial processing. Our results are testament to the visual system\u2019s ability to pick up on reliable, category specific sources of information that are flexible towards disturbances across the visual feature hierarchy."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:t7zJ5fGR-2UC",
        "num_citations": 0,
        "pub_url": "https://www.researchsquare.com/article/rs-3786230/latest",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "Using XR (extended reality) for behavioral, clinical, and learning sciences requires updates in infrastructure and funding",
            "pub_year": 2023,
            "citation": "Policy Insights from the Behavioral and Brain Sciences 10 (2), 317-323, 2023",
            "author": "Dejan Draschkow and Nicola C Anderson and Erwan David and Nathan Gauge and Alan Kingstone and Levi Kumle and Xavier Laurent and Anna C Nobre and Sally Shiels and Melissa L-H V\u00f5",
            "journal": "Policy Insights from the Behavioral and Brain Sciences",
            "volume": "10",
            "number": "2",
            "pages": "317-323",
            "publisher": "SAGE Publications",
            "abstract": "Extended reality (XR, including augmented and virtual reality) creates a powerful intersection between information technology and cognitive, clinical, and education sciences. XR technology has long captured the public imagination, and its development is the focus of major technology companies. This article demonstrates the potential of XR to (1) deliver behavioral insights, (2) transform clinical treatments, and (3) improve learning and education. However, without appropriate policy, funding, and infrastructural investment, many research institutions will struggle to keep pace with the advances and opportunities of XR. To realize the full potential of XR for basic and translational research, funding should incentivize (1) appropriate training, (2) open software solutions, and (3) collaborations between complementary academic and industry partners. Bolstering the XR research infrastructure with the right investments and \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:uJ-U7cs_P_0C",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=9364903938429561602",
        "cites_id": [
            "9364903938429561602"
        ],
        "pub_url": "https://journals.sagepub.com/doi/abs/10.1177/23727322231196305",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Semantic object processing is modulated by prior scene context",
            "pub_year": 2023,
            "citation": "Language, Cognition and Neuroscience, 1-10, 2023",
            "author": "Alexandra Krugliak and Dejan Draschkow and Melissa L-H V\u00f5 and Alex Clarke",
            "journal": "Language, Cognition and Neuroscience",
            "pages": "1-10",
            "publisher": "Routledge",
            "abstract": "Objects that are congruent with a scene are recognised more efficiently than objects that are incongruent. Further, semantic integration of incongruent objects elicits a stronger N300/N400 EEG component. Yet, the time course and mechanisms of how contextual information supports access to semantic object information is unclear. We used computational modelling and EEG to test how context influences semantic object processing. Using representational similarity analysis, we established that EEG patterns dissociated between objects in congruent or incongruent scenes from around 300\u2005ms. By modelling the semantic processing of objects using independently normed properties, we confirm that the onset of semantic processing of both congruent and incongruent objects is similar (\u223c150 ms). Critically, after \u223c275 ms, we discover a difference in the duration of semantic integration, lasting longer for incongruent \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:eq2jaN3J8jMC",
        "num_citations": 0,
        "pub_url": "https://www.tandfonline.com/doi/abs/10.1080/23273798.2023.2279083",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mFXj7jGbVfkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Access to meaning from visual input: Object and word frequency effects in categorization behavior.",
            "pub_year": 2023,
            "citation": "Journal of Experimental Psychology: General, 2023",
            "author": "Klara Gregorov\u00e1 and Jacopo Turini and Benjamin Gagl and Melissa Le-Hoa V\u00f5",
            "journal": "Journal of Experimental Psychology: General",
            "publisher": "American Psychological Association",
            "abstract": "Object and word recognition are both cognitive processes that transform visual input into meaning. When reading words, the frequency of their occurrence (\u201cword frequency,\u201d WF) strongly modulates access to their meaning, as seen in recognition performance. Does the frequency of objects in our world also affect access to their meaning? With object labels available in real-world image datasets, one can now estimate the frequency of occurrence of objects in scenes (\u201cobject frequency,\u201d OF). We explored frequency effects in word and object recognition behavior by employing a natural versus man-made categorization task (Experiment 1) and a matching\u2013mismatching priming task (Experiments 2\u20133). In Experiment 1, we found a WF effect for both words and objects but no OF effect. In Experiment 2, we replicated the WF effect for both stimulus types during cross-modal priming but not uni-modal priming. Moreover, in \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:q3oQSFYPqjQC",
        "num_citations": 4,
        "citedby_url": "/scholar?hl=en&cites=8341788289603668324",
        "cites_id": [
            "8341788289603668324"
        ],
        "pub_url": "https://psycnet.apa.org/record/2023-69316-001",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZAlXsUD8w3MJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 2,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Disentangling diagnostic object properties for human scene categorization",
            "pub_year": 2023,
            "citation": "Scientific Reports 13 (1), 5912, 2023",
            "author": "Sandro L Wiesmann and Melissa L-H V\u00f5",
            "journal": "Scientific Reports",
            "volume": "13",
            "number": "1",
            "pages": "5912",
            "publisher": "Nature Publishing Group UK",
            "abstract": "It usually only takes a single glance to categorize our environment into different scene categories (e.g. a kitchen or a highway). Object information has been suggested to play a crucial role in this process, and some proposals even claim that the recognition of a single object can be sufficient to categorize the scene around it. Here, we tested this claim in four behavioural experiments by having participants categorize real-world scene photographs that were reduced to a single, cut-out object. We show that single objects can indeed be sufficient for correct scene categorization and that scene category information can be extracted within 50 ms of object presentation. Furthermore, we identified object frequency and specificity for the target scene category as the most important object properties for human scene categorization. Interestingly, despite the statistical definition of specificity and frequency, human ratings of these \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:Fu2w8maKXqMC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=13498392154162091884",
        "cites_id": [
            "13498392154162091884"
        ],
        "pub_url": "https://www.nature.com/articles/s41598-023-32385-y",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:bFfAiyXsU7sJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Entwicklung und Einsatz von VR-Lernszenarien f\u00fcr den Lehrkompetenzaufbau: Klassenraumsimulationen mit Virtual Reality",
            "pub_year": 2023,
            "citation": "Lehr-Lern-Labore und Digitalisierung, 211-224, 2023",
            "author": "Laura Glocker and Sebastian Breitenbach and Miriam Hansen and Julia Mendzheritskaya and Melissa L\u00ea-Hoa V\u00f5",
            "pages": "211-224",
            "publisher": "Springer Fachmedien Wiesbaden",
            "abstract": "In diesem Beitrag werden Entwicklung, Einsatz und Evaluation einer selbst entwickelten VR-Klassensimulation \u201aCLASIVIR 1.0\u2018 vorgestellt, in der den Studierenden eine virtuelle Situation pr\u00e4sentiert wurde. Den theoretischen Rahmen f\u00fcr Die Entwicklung und der Einsatz von VR-Lernszenarien innerhalb einer Klassenraumsimulation zum Lehrkompetenzerwerb f\u00fcr die vorliegende Studie wurde theoretisch abgeleitet, unter anderem basierend auf den Basisdimensionen guten Unterrichts. Die Pilotierung des ersten VR-Lernszenarios wurde in ein bildungswissenschaftliches Seminar f\u00fcr Lehramtsstudierende zur F\u00f6rderung von digitalen Kompetenzen durch Virtual Reality als ein VR-Lehr-Lern-Labor integriert. F\u00fcr Evaluationszwecke wurden die Studierenden im Rahmen von Pr\u00e4-Post-Befragungen zum emotionalen Erleben sowie zu Einstellungen zu digitalen befragt. Im Beitrag werden sowohl VR-Lehr-Lern-Labore \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:_Ybze24A_UAC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=1535785716765454004",
        "cites_id": [
            "1535785716765454004"
        ],
        "pub_url": "https://link.springer.com/chapter/10.1007/978-3-658-40109-2_22",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:tG7s8vg0UBUJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Using XR (Extended Reality) for Behavioral, Clinical, and Learning Sciences Requires Updates in Infrastructure and Funding",
            "pub_year": 2023,
            "citation": "Policy Insights from the Behavioral and Brain Sciences 10 (2), 317-323, 2023",
            "author": "Dejan Draschkow and Nicola C Anderson and Erwan David and Nathan Gauge and Alan Kingstone and Levi Kumle and Xavier Laurent and Anna C Nobre and Sally Shiels and Melissa L-H V\u00f5",
            "journal": "Policy Insights from the Behavioral and Brain Sciences",
            "volume": "10",
            "number": "2",
            "pages": "317-323",
            "publisher": "SAGE Publications",
            "abstract": "Extended reality (XR, including augmented and virtual reality) creates a powerful intersection between information technology and cognitive, clinical, and education sciences. XR technology has long captured the public imagination, and its development is the focus of major technology companies. This article demonstrates the potential of XR to (1) deliver behavioral insights, (2) transform clinical treatments, and (3) improve learning and education. However, without appropriate policy, funding, and infrastructural investment, many research institutions will struggle to keep pace with the advances and opportunities of XR. To realize the full potential of XR for basic and translational research, funding should incentivize (1) appropriate training, (2) open software solutions, and (3) collaborations between complementary academic and industry partners. Bolstering the XR research infrastructure with the right investments and \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:uJ-U7cs_P_0C",
        "num_citations": 0,
        "pub_url": "https://journals.sagepub.com/doi/abs/10.1177/23727322231196305",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Aging attenuates the memory advantage for unexpected objects in real-world scenes",
            "pub_year": 2023,
            "citation": "Heliyon 9 (9), 2023",
            "author": "Lena Klever and Jasmin Islam and Melissa Le-Hoa V\u00f5 and Jutta Billino",
            "journal": "Heliyon",
            "volume": "9",
            "number": "9",
            "publisher": "Elsevier",
            "abstract": "Across the adult lifespan memory processes are subject to pronounced changes. Prior knowledge and expectations might critically shape functional differences; however, corresponding findings have remained ambiguous so far. Here, we chose a tailored approach to scrutinize how schema (in-)congruencies affect older and younger adults' memory for objects embedded in real-world scenes, a scenario close to everyday life memory demands. A sample of 23 older (52\u201381 years) and 23 younger adults (18\u201338 years) freely viewed 60 photographs of scenes in which target objects were included that were either congruent or incongruent with the given context information. After a delay, recognition performance for those objects was determined. In addition, recognized objects had to be matched to the scene context in which they were previously presented. While we found schema violations beneficial for object \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:j8SEvjWlNXcC",
        "num_citations": 0,
        "pub_url": "https://www.cell.com/heliyon/pdf/S2405-8440(23)07449-2.pdf",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Viewpoint dependence and scene context effects generalize to depth rotated three-dimensional objects",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (10), 9-9, 2023",
            "author": "Aylin Kallmayer and Melissa L-H V\u00f5 and Dejan Draschkow",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "10",
            "pages": "9-9",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Viewpoint effects on object recognition interact with object-scene consistency effects. While recognition of objects seen from \u201cnoncanonical\u201d viewpoints (eg, a cup from below) is typically impeded compared to processing of objects seen from canonical viewpoints (eg, the string-side of a guitar), this effect is reduced by meaningful scene context information. In the present study we investigated if these findings established by using photographic images, generalize to strongly noncanonical orientations of three-dimensional (3D) models of objects. Using 3D models allowed us to probe a broad range of viewpoints and empirically establish viewpoints with very strong noncanonical and canonical orientations. In Experiment 1, we presented 3D models of objects from six different viewpoints (0, 60, 120, 180 240, 300) in color (1a) and grayscaled (1b) in a sequential matching task. Viewpoint had a significant effect on accuracy and response times. Based on the viewpoint effect in Experiments 1a and 1b, we could empirically determine the most canonical and noncanonical viewpoints from our set of viewpoints to use in Experiment 2. In Experiment 2, participants again performed a sequential matching task, however now the objects were paired with scene backgrounds which could be either consistent (eg, a cup in the kitchen) or inconsistent (eg, a guitar in the bathroom) to the object. Viewpoint interacted significantly with scene consistency in that object recognition was less affected by viewpoint when consistent scene information was provided, compared to inconsistent information. Our results show that scene context supports object recognition even \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:fEOibwPWpKIC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792757",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Vowel World 2.0. Implicit learning of artificial scene grammar. Exp 1",
            "pub_year": 2023,
            "citation": "OSF, 2023",
            "author": "Yuri Markov and Jeremy Wolfe and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "Scene guidance can be quite difficult to investigate in real scenes: Not all assets of a complex scene can be manipulated or controlled, editing/manipulating scenes is very time consuming and creates artifacts, and the number of scenes is often limited by labor intense manipulations. Using artificial scenes that mimic characteristics of real scenes and which are created according to artificial rules could help. In VW 2.0 we control various parameters using simplified scenes (basically a 16x16 grid onto which vowels and consonants are overlaid) which contain different \u201cobjects\u201d and targets, which follow different rules. We used a new version of \u201cVowelWorld\u201d(VW), a paradigm that allows to control for different types of guidance rules in artificial displays. The previous version had a 10x10 grid with colored cells and letters placed on it and followed three rules: color rule (certain targets were placed on certain color regions), structure rule (vowels were placed on circles), and letter rule (vowels were close to neighboring consonants of the alphabet). In the new version, the grid has one color tone and contains artificial \u201cobjects\u201d, these are connect groups of cells, distinguished from the background and other objects by use of shadow cues to imply stratification in depth. Importantly, this version tried to incorporate so-called \u201canchor objects\u201d(Boettcher, Draschkow, Dienhart, & V\u00f5, 2018; V\u00f5, 2021) in the scene, which are important for guiding search in real-world scenes."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:ZfRJV9d4-WMC",
        "num_citations": 0,
        "pub_url": "https://osf.io/382e9/resources",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Just look away: Could attention allocation to scene grammar violations during unrelated object searches be modulated by individual differences in language experience?",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5289-5289, 2023",
            "author": "Naomi Vingron and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5289-5289",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Disengaging attention from task-irrelevant information is not only a process crucial to navigating the world around us but may also be a skill that bilinguals are particularly well-practiced at as they are constantly juggling information from multiple languages. The current study investigates the extent to which individual differences in language experience are associated with increased object search efficiency in scenes containing mismatches between objects and scene context. Efficient search in real-world scenes relies on a set of rules (\u201cscene grammar\u201d), which support object localization and identification (eg, knowing that a spatula goes neither in a toaster nor a bathroom). Violations to these rules may impair processing as viewers need to resolve ambiguity resulting from the unexpected element of the scene and have been shown to modulate ongoing eye-movements, even when they are irrelevant to the task at \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:-_dYPAW6P2MC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792099",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "How real can they get? Investigating neural responses to GAN generated scenes.",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5703-5703, 2023",
            "author": "Aylin Kallmayer and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5703-5703",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "To understand how we efficiently navigate real-world scenes, we need to unravel the underlying computations and structure of representations that afford efficient scene processing. One hypothesis is that we exploit scene structures by learning hierarchical object-to-object and scene-to-object relations captured by a scene grammar. But how can these high-level networks be learnt? Does unsupervised learning automatically lead to representations that reflect properties of scene grammar? To assess how well scenes generated by generative adversarial networks (GANs) capture real-world scene structure perceived over time we conducted an EEG experiment. Participants viewed 180 generated scenes across six categories (30 exemplars/category). Generated scenes varied in their \u201crealness\u201d as assessed by three different measures: realism ratings, false-alarm (FA) rates, and categorization performance for 50 and \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:35r97b3x0nAC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792570",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Artificial Scene Grammar Acquisition",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5656-5656, 2023",
            "author": "Maxim Spur and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5656-5656",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Statistical learning has been demonstrated to be a valid method of learning the structure of invented languages\u2014simple exposure to consecutive nonsensical syllables allows learning of the transitional probabilities reflecting the rules of languages. Does the acquisition of scene grammar, ie, the rules governing the structure of naturalistic scene environments, occur in a similar way, or are more interactive approaches more effective, which would more closely reflect the actual way we learn our environments? To answer this question, we attempted to create a number of \u201cinvented\u201d scenes with 12 \u201cnonsensical\u201d objects each. These objects were divided into \u201canchors\u201d(usually larger objects that predict the location of other objects), and \u201clocals,\u201d which were each assigned to an anchor, ie, appearing close to it 80% of the time. Participants were exposed to these objects in a virtual reality environment in two blocks (active \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:evX43VCCuoAC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792613",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Virtual reality protocol for decomposing complex behaviour into tractable subcomponents.",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4996-4996, 2023",
            "author": "Levi Kumle and Anna C Nobre and Melissa Vo and Dejan Draschkow",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4996-4996",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Understanding temporally extended natural behaviour requires studying the coordination of vision, action, and memory during free-flowing interactions with the environment. However, naturalistic experiments capturing the complexity of natural behaviour are often incompatible with the desire for tight experimental control. Therefore, sub-components of natural behaviour have been predominantly studied in isolation using desktop-based lab tasks\u2013missing potential interconnections between these sub-components that only emerge during continuous and temporally extended behaviour. Here, we share a unique open virtual reality (VR) resource for quantifying and segmenting continuous behaviour into tractable sub-units. Within a single temporally extended natural task, participants copied a Model display by selecting realistic objects from a Resource pool and placing them into a Workspace. We track head, hand \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:2KloaMYe4IUC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791518",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dynamics of gaze and body while viewing omnidirectional stimuli",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5123-5123, 2023",
            "author": "Erwan David and Melissa Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5123-5123",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Recent vision studies conducted in virtual reality (VR) are helping to understand what roles the eye and the head play while we observe scenes that surround us. Unfortunately, very few studies have shown interest in the contributions of the rest of the body to gaze movements. To shed some light on this subject we have designed a protocol to gather tracking data about torso and leg movements, in addition to the eye and head tracking. Wearing a VR headset and trackers on their torso and leg, our participants observed scenes that were either simple (Gabor patches, 3D shapes) or complex (360 photos, 3D rooms). Stimuli were either flat on the surface of a sphere or fully 3D. Additionally, half of the trials were either free-viewing or followed by a recall task to study a potential effect of goal-direction (all trials lasted 10s). We show that under the impetus of a goal participants made longer saccades (and shorter fixations \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:tzM49s52ZIMC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792248",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Investigating the effects of a virtual reality vs. screen-based testing setup on incidental memory after visual search through scenes",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5126-5126, 2023",
            "author": "Julia Beitner and Jason Helbing and Erwan J David and Melissa L-H Vo",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5126-5126",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Many experiments investigating visual search are performed on a computer screen under highly controlled settings. However, the generalizability of the investigated search mechanisms to the real world is unclear. Here, we tested whether the formation of incidental object memories during visual search follows similar patterns in virtual reality (VR) and on a computer screen. We present two studies that were identical in the administered tasks, but differed in the setup they were tested in. In both experiments, participants searched for ten out of 20 objects in indoor scenes, either in full illumination or with constrained visual input via a controller-contingent 8-degree window (flashlight condition). After the search task, participants\u2019 incidental memory was tested with two surprise tasks: an object recognition and a location memory task (scene rebuilding). Critically, the first experiment took place in an immersive virtual \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:_Re3VWB3Y0AC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792245",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Recollection and familiarity in the search superiority effect: ERPs, ROCs, and remember\u2013know judgements of search targets and intentionally memorized objects in scenes",
            "pub_year": 2023,
            "citation": "OSF, 2023",
            "author": "Jason Helbing and Melissa L-H Vo",
            "publisher": "OSF",
            "abstract": "We intend to combine electroencephalography (EEG) and eye tracking in an experiment to investigate recognition memory representations of objects in scenes which are either (a) searched for or (b) intentionally memorized. This is motivated by the surprising finding that search targets tend to be memorized better than intentionally memorized objects (search superiority effect). We intend to answer the question whether this effect is purely quantitative (stronger memory) or whether we can also find indicators of different cognitive processes related to the recognition of search targets and intentionally memorized objects (stronger and different memory). The two processes we hypothesize to be related differentially to search and memorization are recollection (knowing precisely that something has been encountered, typically with detailed information on the context; hard threshold signal of yes vs. no) and familiarity (vaguely knowing something has been encountered before without details about the encoding context; continuous strength signal) as described by dual-process models of recognition memory. We aim to analyze event-related potentials (ERPs), receiver operating characteristics (ROCs), and remember\u2013know-judgements to investigate this potential link."
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:tKAzc9rXhukC",
        "num_citations": 0,
        "pub_url": "https://osf.io/hvfpq/resources",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The Salient360! Toolbox: Processing, Visualising and Comparing Gaze Data in 3D",
            "pub_year": 2023,
            "citation": "Proceedings of the 2023 Symposium on Eye Tracking Research and Applications, 1-8, 2023",
            "author": "Erwan David and Jes\u00fas Guti\u00e9rrez and Melissa Le-Hoa Vo and Antoine Coutrot and Matthieu Perreira Da Silva and Patrick Le Callet",
            "pages": "1-8",
            "abstract": " Eye tracking can serve as a gateway to studying the mind. For this reason it has been adopted by a diverse range of scientific communities. With the improvement of the quality of head-mounted virtual reality devices (HMDs) over the past 10 years, eye tracking has been added to capture gaze in immersive environments. The use of HMDs with eye tracking is increasing significantly and so is the need for a toolbox enabling consensus about eye tracking methods in 3D. We present the Salient360! toolbox: it implements functions to identify saccades and fixations and output gaze characteristics (e.g., fixation duration or saccade directions), to generate saliency maps, fixation maps, and scanpath data. It also implements routines made to compare gaze data that were adapted to 3D. We hope that this toolbox will spark discussions about the methodology of 3D gaze processing, facilitate running experiments, and improve \u2026"
        },
        "filled": true,
        "author_pub_id": "-c9whrAAAAAJ:kzcrU_BdoSEC",
        "num_citations": 0,
        "pub_url": "https://dl.acm.org/doi/abs/10.1145/3588015.3588406",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Y96wLbrMZvgJ:scholar.google.com/",
        "cites_per_year": {}
    }
]