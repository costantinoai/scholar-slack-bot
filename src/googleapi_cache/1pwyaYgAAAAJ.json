[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Learning functional transduction",
            "pub_year": 2024,
            "citation": "Advances in Neural Information Processing Systems 36, 2024",
            "author": "Mathieu Chalvidal and Thomas Serre and Rufin VanRullen",
            "journal": "Advances in Neural Information Processing Systems",
            "volume": "36",
            "abstract": "Research in statistical learning has polarized into two general approaches to perform regression analysis: Transductive methods construct estimates directly based on exemplar data using generic relational principles which might suffer from the curse of dimensionality. Conversely, inductive methods can potentially fit highly complex functions at the cost of compute-intensive solution searches. In this work, we leverage the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS) to propose a hybrid approach: We show that transductive regression systems can be meta-learned with gradient descent to form efficient in-context neural approximators of function defined over both finite and infinite-dimensional spaces (operator regression). Once trained, our Transducer can almost instantaneously capture new functional relationships and produce original image estimates, given a few pairs of input and output examples. We demonstrate the benefit of our meta-learned transductive approach to model physical systems influenced by varying external factors with little data at a fraction of the usual deep learning training costs for partial differential equations and climate modeling applications."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:gVv57TyPmFsC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=12991008176689008541",
        "cites_id": [
            "12991008176689008541"
        ],
        "pub_url": "https://proceedings.neurips.cc/paper_files/paper/2023/hash/e9b8a3362a6d9a7f9f842bd2d919e1a0-Abstract-Conference.html",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:nU--fRtVSbQJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Latent Representation Matters: Human-like Sketches in One-shot Drawing Tasks",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2406.06079, 2024",
            "author": "Victor Boutin and Rishav Mukherji and Aditya Agrawal and Sabine Muzellec and Thomas Fel and Thomas Serre and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2406.06079",
            "abstract": "Humans can effortlessly draw new categories from a single exemplar, a feat that has long posed a challenge for generative models. However, this gap has started to close with recent advances in diffusion models. This one-shot drawing task requires powerful inductive biases that have not been systematically investigated. Here, we study how different inductive biases shape the latent space of Latent Diffusion Models (LDMs). Along with standard LDM regularizers (KL and vector quantization), we explore supervised regularizations (including classification and prototype-based representation) and contrastive inductive biases (using SimCLR and redundancy reduction objectives). We demonstrate that LDMs with redundancy reduction and prototype-based regularizations produce near-human-like drawings (regarding both samples' recognizability and originality) -- better mimicking human perception (as evaluated psychophysically). Overall, our results suggest that the gap between humans and machines in one-shot drawings is almost closed."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:CaZNVDsoPx4C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2406.06079",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:QuPKEkbUXmMJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A recurrent CNN for online object detection on raw radar frames",
            "pub_year": 2024,
            "citation": "IEEE Transactions on Intelligent Transportation Systems, 2024",
            "author": "Colin Decourt and Rufin VanRullen and Didier Salle and Thomas Oberlin",
            "journal": "IEEE Transactions on Intelligent Transportation Systems",
            "publisher": "IEEE",
            "abstract": "Automotive radar sensors provide valuable information for advanced driving assistance systems (ADAS). Radars can reliably estimate the distance to an object and the relative velocity, regardless of weather and light conditions. However, radar sensors suffer from low resolution and huge intra-class variations in the shape of objects. Exploiting the time information (e.g, multiple frames) has been shown to help to capture better the dynamics of objects and, therefore, the variation in the shape of objects. Most temporal radar object detectors use 3D convolutions to learn spatial and temporal information. However, these methods are often non-causal and unsuitable for real-time applications. This work presents RECORD, a new recurrent CNN architecture for online radar object detection. We propose an end-to-end trainable architecture mixing convolutions and ConvLSTMs to learn spatio-temporal dependencies \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:nVrZBo8bIpAC",
        "num_citations": 6,
        "citedby_url": "/scholar?hl=en&cites=13715660132531667729",
        "cites_id": [
            "13715660132531667729"
        ],
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10547638/",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:EYOJyzjQV74J:scholar.google.com/",
        "cites_per_year": {
            "2023": 4,
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Through their eyes: multi-subject Brain Decoding with simple alignment techniques",
            "pub_year": 2024,
            "citation": "Imaging Neuroscience, 2024",
            "author": "Matteo Ferrante and Tommaso Boccato and Furkan Ozcelik and Rufin VanRullen and Nicola Toschi",
            "journal": "Imaging Neuroscience",
            "publisher": "MIT Press",
            "abstract": "To-date, brain decoding literature has focused on single-subject studies, i.e. reconstructing stimuli presented to a subject under fMRI acquisition from the fMRI activity of the same subject. The objective of this study is to introduce a generalization technique that enables the decoding of a subject\u2019s brain based on fMRI activity of another subject, i.e. cross-subject brain decoding. To this end, we also explore cross-subject data alignment techniques. Data alignment is the attempt to register different subjects in a common anatomical or functional space for further and more general analysis.We utilized the Natural Scenes Dataset, a comprehensive 7T fMRI experiment focused on vision of natural images. The dataset contains fMRI data from multiple subjects exposed to 9841 images, where 982 images have been viewed by all subjects. Our method involved training a decoding model on one subject\u2019s data \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:otzGkya1bYkC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=5811987928613068171",
        "cites_id": [
            "5811987928613068171"
        ],
        "pub_url": "https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00170/120739",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ix3XUzpUqFAJ:scholar.google.com/",
        "cites_per_year": {
            "2024": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Towards Generalized Brain Decoding of visual stimuli: Cross-Subject and Dataset Perspectives with a simple alignment technique",
            "pub_year": 2024,
            "citation": "",
            "author": "Matteo Ferrante and Tommaso Boccato and Furkan Ozcelik and Rufin VanRullen and Nicola Toschi",
            "abstract": "To-date, brain decoding literature has focused on singlesubject studies, ie reconstructing stimuli presented to a subject under fMRI acquisition from the fMRI activity of the same subject. The objective of this study is to introduce a generalization technique that enables the decoding of a subject\u2019s brain based on fMRI activity of another subject, ie crosssubject brain decoding. To this end, we also explore crosssubject data alignment techniques. Data alignment is the attempt to register different subjects in a common anatomical or functional space for further and more general analysis. We worked with the Natural Scenes Dataset, a comprehensive 7T fMRI experiment focused on vision of natural images. The dataset contains fMRI data from multiple subjects exposed to 9841 images, where 982 images have been viewed by all subjects. Our method involved training a decoding model on one subject\u2019s data, aligning new data from other subjects to this space, and testing the decoding on the second subject based on information aligned to first subject. We found that crosssubject brain decoding is possible, even with a small subset of the dataset, specifically, using the common data, which are around 10% of the total data, namely 982 images, with performances in decoding compararble to the ones achieved by single subject decoding. Ridge regression emerged as the best method for functional alignment in fine-grained information decoding, outperforming all other techniques. By aligning multiple subjects, we achieved high-quality brain decoding and a potential reduction in scan time by 90%. This substantial decrease in scan time could open up \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:wMgC3FpKEyYC",
        "num_citations": 0,
        "pub_url": "https://hcrl-workshop.github.io/2024/assets/papers/Towards%20Generalized%20Brain%20Decoding%20of%20visual%20stimuli-%20Cross-Subject%20and%20Dataset%20Perspectives%20with%20a%20simple%20alignment%20technique.pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:JBsU74iGNxQJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Zero-shot cross-modal transfer of Reinforcement Learning policies through a Global Workspace",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2403.04588, 2024",
            "author": "L\u00e9opold Mayti\u00e9 and Benjamin Devillers and Alexandre Arnold and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2403.04588",
            "abstract": "Humans perceive the world through multiple senses, enabling them to create a comprehensive representation of their surroundings and to generalize information across domains. For instance, when a textual description of a scene is given, humans can mentally visualize it. In fields like robotics and Reinforcement Learning (RL), agents can also access information about the environment through multiple sensors; yet redundancy and complementarity between sensors is difficult to exploit as a source of robustness (e.g. against sensor failure) or generalization (e.g. transfer across domains). Prior research demonstrated that a robust and flexible multimodal representation can be efficiently constructed based on the cognitive science notion of a 'Global Workspace': a unique representation trained to combine information across modalities, and to broadcast its signal back to each modality. Here, we explore whether such a brain-inspired multimodal representation could be advantageous for RL agents. First, we train a 'Global Workspace' to exploit information collected about the environment via two input modalities (a visual input, or an attribute vector representing the state of the agent and/or its environment). Then, we train a RL agent policy using this frozen Global Workspace. In two distinct environments and tasks, our results reveal the model's ability to perform zero-shot cross-modal transfer between input modalities, i.e. to apply to image inputs a policy previously trained on attribute vectors (and vice-versa), without additional training or fine-tuning. Variants and ablations of the full Global Workspace (including a CLIP-like multimodal representation \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:jL-93Qbq4QoC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2403.04588",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:iVbXxp7Tb_UJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Modality-Agnostic fMRI Decoding of Vision and Language",
            "pub_year": 2024,
            "citation": "ICLR 2024 Workshop on Representational Alignment, 2024",
            "author": "Mitja Nikolaus and Milad Mozafari and Nicholas Asher and Leila Reddy and Rufin VanRullen",
            "conference": "ICLR 2024 Workshop on Representational Alignment",
            "abstract": "Previous studies have shown that it is possible to map brain activation data of subjects viewing images onto the feature representation space of not only vision models (modality-specific decoding) but also language models (cross-modal decoding). In this work, we introduce and use a new large-scale fMRI dataset ( trials per subject) of people watching both images and text descriptions of such images. This novel dataset enables the development of modality-agnostic decoders: a single decoder that can predict which stimulus a subject is seeing, irrespective of the modality (image or text) in which the stimulus is presented.  We train and evaluate such decoders to map brain signals onto stimulus representations from a large range of publicly available vision, language and multimodal (vision+language) models. Our findings reveal that (1) modality-agnostic decoders perform as well as (and sometimes even better than) modality-specific decoders (2) modality-agnostic decoders mapping brain data onto representations from unimodal models perform as well as decoders relying on multimodal representations (3) while language and low-level visual (occipital) brain regions are best at decoding text and image stimuli, respectively, high-level visual (temporal) regions perform well on both stimulus types."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:4xDN1ZYqzskC",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=7gWQL0hTrX",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:tMNycASFBawJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A traveling waves perspective on temporal binding",
            "pub_year": 2024,
            "citation": "Journal of Cognitive Neuroscience, 1-9, 2024",
            "author": "Andrea Alamia and Rufin VanRullen",
            "journal": "Journal of Cognitive Neuroscience",
            "pages": "1-9",
            "publisher": "MIT Press",
            "abstract": "Brain oscillations are involved in many cognitive processes, and several studies have investigated their role in cognition. In particular, the phase of certain oscillations has been related to temporal binding and integration processes, with some authors arguing that perception could be an inherently rhythmic process. However, previous research on oscillations mostly overlooked their spatial component: how oscillations propagate through the brain as traveling waves, with systematic phase delays between brain regions. Here, we argue that interpreting oscillations as traveling waves is a useful paradigm shift to understand their role in temporal binding and address controversial results. After a brief definition of traveling waves, we propose an original view on temporal integration that considers this new perspective. We first focus on cortical dynamics, then speculate about the role of thalamic nuclei in modulating \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:tH6gc1N1XXoC",
        "num_citations": 9,
        "citedby_url": "/scholar?hl=en&cites=8910395417058135892",
        "cites_id": [
            "8910395417058135892"
        ],
        "pub_url": "https://direct.mit.edu/jocn/article-abstract/doi/10.1162/jocn_a_02004/115974",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VM8EKmwVqHsJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 4,
            "2024": 5
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Leveraging Self-Supervised Instance Contrastive Learning for Radar Object Detection",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2402.08427, 2024",
            "author": "Colin Decourt and Rufin VanRullen and Didier Salle and Thomas Oberlin",
            "journal": "arXiv preprint arXiv:2402.08427",
            "abstract": "In recent years, driven by the need for safer and more autonomous transport systems, the automotive industry has shifted toward integrating a growing number of Advanced Driver Assistance Systems (ADAS). Among the array of sensors employed for object recognition tasks, radar sensors have emerged as a formidable contender due to their abilities in adverse weather conditions or low-light scenarios and their robustness in maintaining consistent performance across diverse environments. However, the small size of radar datasets and the complexity of the labelling of those data limit the performance of radar object detectors. Driven by the promising results of self-supervised learning in computer vision, this paper presents RiCL, an instance contrastive learning framework to pre-train radar object detectors. We propose to exploit the detection from the radar and the temporal information to pre-train the radar object detection model in a self-supervised way using contrastive learning. We aim to pre-train an object detector's backbone, head and neck to learn with fewer data. Experiments on the CARRADA and the RADDet datasets show the effectiveness of our approach in learning generic representations of objects in range-Doppler maps. Notably, our pre-training strategy allows us to use only 20% of the labelled data to reach a similar mAP@0.5 than a supervised approach using the whole training set."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:PVgj2kMGcgYC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2402.08427",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:6LPXbzzChDUJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "Multimodal decoding of human brain activity into images and text",
            "pub_year": 2023,
            "citation": "UniReps: the First Workshop on Unifying Representations in Neural Models, 2023",
            "author": "Matteo Ferrante and Tommaso Boccato and Furkan Ozcelik and Rufin VanRullen and Nicola Toschi",
            "conference": "UniReps: the First Workshop on Unifying Representations in Neural Models",
            "abstract": "Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation.  We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline based on latent diffusion models. The method involves training regularized linear regression models between brain activity and extracted features. Additionally, we incorporated depth maps from the ControlNet model to further guide the reconstruction process. We propose a multimodal based approach that leverage similarities between neural and deep learning presentation and by learning alignment between these spaces, we produce textual description and image reconstruction from brain activity. We evaluate our methods using quantitative metrics for both generated captions and images. Our brain captioning approach outperforms existing \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:6yz0xqPARnAC",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=rGCabZfV3d",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Consciousness in artificial intelligence: insights from the science of consciousness",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2308.08708, 2023",
            "author": "Patrick Butlin and Robert Long and Eric Elmoznino and Yoshua Bengio and Jonathan Birch and Axel Constant and George Deane and Stephen M Fleming and Chris Frith and Xu Ji and Ryota Kanai and Colin Klein and Grace Lindsay and Matthias Michel and Liad Mudrik and Megan AK Peters and Eric Schwitzgebel and Jonathan Simon and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2308.08708",
            "abstract": "Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive \"indicator properties\" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:F1b5ZUV5XREC",
        "num_citations": 24,
        "citedby_url": "/scholar?hl=en&cites=8239061011717183910",
        "cites_id": [
            "8239061011717183910"
        ],
        "pub_url": "https://arxiv.org/abs/2308.08708",
        "cites_per_year": {
            "2023": 23
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Generative Multimodal Decoding: Reconstructing Images and Text from Human fMRI",
            "pub_year": 2023,
            "citation": "Deep Generative Models for Health Workshop NeurIPS 2023, 2023",
            "author": "Matteo Ferrante and Tommaso Boccato and Furkan Ozcelik and Rufin VanRullen and Nicola Toschi",
            "conference": "Deep Generative Models for Health Workshop NeurIPS 2023",
            "abstract": "The human brain adeptly processes immense visual information using complex neural mechanisms. Recent advances in functional MRI (fMRI) enable decoding this visual information from recorded brain activity patterns. In this work, we present an innovative approach for reconstructing meaningful images and captions directly from fMRI data, with a focus on brain captioning due to its enhanced flexibility over image decoding. We utilize the Natural Scenes fMRI dataset containing brain recordings from subjects viewing images. Our method leverages state-of-the-art image captioning and diffusion models for multimodal decoding. We train regression models between fMRI data and textual/visual features and incorporate depth estimation to guide image reconstruction. Our key innovation is a multimodal framework aligning neural and deep learning representations to generate both semantic captions and photorealistic images from brain activity. We demonstrate quantitative improvements in captioning over prior art and in image spatial relationships through our reconstruction pipeline. In conclusion, this work significantly advances brain decoding capabilities through an integrated vision-language approach. Our flexible decoding platform combining high-level semantic text and low-level visual depth information provides new insights into human visual cognition. The proposed methods could enable future applications in brain-computer interfaces, neuroscience, and AI."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:sJsF-0ZLhtgC",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=XUgIZQvxg4",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Consciousness in artificial intelligence: Insights from the science of consciousness",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2308.08708, 2023",
            "author": "Patrick Butlin and Robert Long and Eric Elmoznino and Yoshua Bengio and Jonathan Birch and Axel Constant and George Deane and Stephen M Fleming and Chris Frith and Xu Ji and Ryota Kanai and Colin Klein and Grace Lindsay and Matthias Michel and Liad Mudrik and Megan AK Peters and Eric Schwitzgebel and Jonathan Simon and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2308.08708",
            "abstract": "Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive \"indicator properties\" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:F1b5ZUV5XREC",
        "num_citations": 13,
        "citedby_url": "/scholar?hl=en&cites=8239061011717183910",
        "cites_id": [
            "8239061011717183910"
        ],
        "pub_url": "https://arxiv.org/abs/2308.08708",
        "cites_per_year": {
            "2023": 12
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Brain-diffuser: Natural scene reconstruction from fmri signals using generative latent diffusion",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2303.05334, 2023",
            "author": "Furkan Ozcelik and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2303.05334",
            "abstract": "In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called ``Brain-Diffuser''. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second stage, we use the image-to-image framework of a latent diffusion model (Versatile Diffusion) conditioned on predicted multimodal (text and visual) features, to generate final reconstructed images. On the publicly available Natural Scenes Dataset benchmark, our method outperforms previous models both qualitatively and quantitatively. When applied to synthetic fMRI patterns generated from individual ROI (region-of-interest) masks, our trained model creates compelling ``ROI-optimal'' scenes consistent with neuroscientific knowledge. Thus, the proposed methodology can have an impact on both applied (e.g. brain-computer interface) and fundamental neuroscience."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:5icHVeHT4IsC",
        "num_citations": 12,
        "citedby_url": "/scholar?hl=en&cites=5828055210287727240",
        "cites_id": [
            "5828055210287727240"
        ],
        "pub_url": "https://arxiv.org/abs/2303.05334",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:iIKQ3FVp4VAJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 12
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Distinct roles of forward and backward alpha-band waves in spatial visual attention",
            "pub_year": 2023,
            "citation": "Elife 12, e85035, 2023",
            "author": "Andrea Alamia and Lucie Terral and Malo Renaud D'ambra and Rufin VanRullen",
            "journal": "Elife",
            "volume": "12",
            "pages": "e85035",
            "publisher": "eLife Sciences Publications Limited",
            "abstract": "Previous research has associated alpha-band [8\u201312 Hz] oscillations with inhibitory functions: for instance, several studies showed that visual attention increases alpha-band power in the hemisphere ipsilateral to the attended location. However, other studies demonstrated that alpha oscillations positively correlate with visual perception, hinting at different processes underlying their dynamics. Here, using an approach based on traveling waves, we demonstrate that there are two functionally distinct alpha-band oscillations propagating in different directions. We analyzed EEG recordings from three datasets of human participants performing a covert visual attention task (one new dataset with N= 16, two previously published datasets with N= 16 and N= 31). Participants were instructed to detect a brief target by covertly attending to the screen\u2019s left or right side. Our analysis reveals two distinct processes: allocating attention to one hemifield increases top-down alpha-band waves propagating from frontal to occipital regions ipsilateral to the attended location, both with and without visual stimulation. These top-down oscillatory waves correlate positively with alpha-band power in frontal and occipital regions. Yet, different alpha-band waves propagate from occipital to frontal regions and contralateral to the attended location. Crucially, these forward waves were present only during visual stimulation, suggesting a separate mechanism related to visual processing. Together, these results reveal two distinct processes reflected by different propagation directions, demonstrating the importance of considering oscillations as traveling waves when characterizing \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:ZzlSgRqYykMC",
        "num_citations": 7,
        "citedby_url": "/scholar?hl=en&cites=5854450739036230356",
        "cites_id": [
            "5854450739036230356"
        ],
        "pub_url": "https://elifesciences.org/articles/85035",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:1BJHz-0vP1EJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 7
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Brain Captioning: Decoding human brain activity into images and text",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2305.11560, 2023",
            "author": "Matteo Ferrante and Furkan Ozcelik and Tommaso Boccato and Rufin VanRullen and Nicola Toschi",
            "journal": "arXiv preprint arXiv:2305.11560",
            "abstract": "Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation. We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline based on latent diffusion models. The method involves training regularized linear regression models between brain activity and extracted features. Additionally, we incorporated depth maps from the ControlNet model to further guide the reconstruction process. We evaluate our methods using quantitative metrics for both generated captions and images. Our brain captioning approach outperforms existing methods, while our image reconstruction pipeline generates plausible images with improved spatial relationships. In conclusion, we demonstrate significant progress in brain decoding, showcasing the enormous potential of integrating vision and \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:artPoR2Yc-kC",
        "num_citations": 4,
        "citedby_url": "/scholar?hl=en&cites=3766769288517777644",
        "cites_id": [
            "3766769288517777644"
        ],
        "pub_url": "https://arxiv.org/abs/2305.11560",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:7Pjz8LVARjQJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 4
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Natural scene reconstruction from fMRI signals using generative latent diffusion",
            "pub_year": 2023,
            "citation": "Scientific Reports 13 (1), 15666, 2023",
            "author": "Furkan Ozcelik and Rufin VanRullen",
            "journal": "Scientific Reports",
            "volume": "13",
            "number": "1",
            "pages": "15666",
            "publisher": "Nature Publishing Group UK",
            "abstract": "In neural decoding research, one of the most intriguing topics is the reconstruction of perceived natural images based on fMRI signals. Previous studies have succeeded in re-creating different aspects of the visuals, such as low-level properties (shape, texture, layout) or high-level features (category of objects, descriptive semantics of scenes) but have typically failed to reconstruct these properties together for complex scene images. Generative AI has recently made a leap forward with latent diffusion models capable of generating high-complexity images. Here, we investigate how to take advantage of this innovative technology for brain decoding. We present a two-stage scene reconstruction framework called \u201cBrain-Diffuser\u201d. In the first stage, starting from fMRI signals, we reconstruct images that capture low-level properties and overall layout using a VDVAE (Very Deep Variational Autoencoder) model. In the second \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:AHdEip9mkN0C",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=10102413494633237114",
        "cites_id": [
            "10102413494633237114"
        ],
        "pub_url": "https://www.nature.com/articles/s41598-023-42891-8",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "On the role of feedback in image recognition under noise and adversarial attacks: A predictive coding perspective",
            "pub_year": 2023,
            "citation": "Neural Networks 157, 280-287, 2023",
            "author": "Andrea Alamia and Milad Mozafari and Bhavin Choksi and Rufin VanRullen",
            "journal": "Neural Networks",
            "volume": "157",
            "pages": "280-287",
            "publisher": "Pergamon",
            "abstract": "Brain-inspired machine learning is gaining increasing consideration, particularly in computer vision. Several studies investigated the inclusion of top-down feedback connections in convolutional networks; however, it remains unclear how and when these connections are functionally helpful. Here we address this question in the context of object recognition under noisy conditions. We consider deep convolutional networks (CNNs) as models of feed-forward visual processing and implement Predictive Coding (PC) dynamics through feedback connections (predictive feedback) trained for reconstruction or classification of clean images. First, we show that the accuracy of the network implementing PC dynamics is significantly larger compared to its equivalent forward network. Importantly, to directly assess the computational role of predictive feedback in various experimental situations, we optimize and interpret the hyper \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:u-coK7KVo8oC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=7270655545322486933",
        "cites_id": [
            "7270655545322486933"
        ],
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0893608022004166",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:lbwrZ7KO5mQJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A Traveling Waves Perspective on Temporal Binding",
            "pub_year": 2023,
            "citation": "Journal of Cognitive Neuroscience, 1-9, 2023",
            "author": "Andrea Alamia and Rufin VanRullen",
            "journal": "Journal of Cognitive Neuroscience",
            "pages": "1-9",
            "abstract": "Brain oscillations are involved in many cognitive processes, and several studies have investigated their role in cognition. In particular, the phase of certain oscillations has been related to temporal binding and integration processes, with some authors arguing that perception could be an inherently rhythmic process. However, previous research on oscillations mostly overlooked their spatial component: how oscillations propagate through the brain as traveling waves, with systematic phase delays between brain regions. Here, we argue that interpreting oscillations as traveling waves is a useful paradigm shift to understand their role in temporal binding and address controversial results. After a brief definition of traveling waves, we propose an original view on temporal integration that considers this new perspective. We first focus on cortical dynamics, then speculate about the role of thalamic nuclei in modulating the \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:tH6gc1N1XXoC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=8910395417058135892",
        "cites_id": [
            "8910395417058135892"
        ],
        "pub_url": "https://direct.mit.edu/jocn/article-abstract/doi/10.1162/jocn_a_02004/115974",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:VM8EKmwVqHsJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Random Tactile Noise Stimulation Reveals Beta-Rhythmic Impulse Response Function of the Somatosensory System",
            "pub_year": 2023,
            "citation": "Journal of Neuroscience 43 (17), 3107-3119, 2023",
            "author": "Samson Chota and Rufin VanRullen and Rasa Gulbinaite",
            "journal": "Journal of Neuroscience",
            "volume": "43",
            "number": "17",
            "pages": "3107-3119",
            "publisher": "Society for Neuroscience",
            "abstract": "Both passive tactile stimulation and motor actions result in dynamic changes in beta band (15\u201330 Hz Hz) oscillations over somatosensory cortex. Similar to alpha band (8\u201312 Hz) power decrease in the visual system, beta band power also decreases following stimulation of the somatosensory system. This relative suppression of \u03b1 and \u03b2 oscillations is generally interpreted as an increase in cortical excitability. Here, next to traditional single-pulse stimuli, we used a random intensity continuous right index finger tactile stimulation (white noise), which enabled us to uncover an impulse response function of the somatosensory system. Contrary to previous findings, we demonstrate a burst-like initial increase rather than decrease of beta activity following white noise stimulation (human participants, N = 18, 8 female). These \u03b2 bursts, on average, lasted for 3 cycles, and their frequency was correlated with resonant frequency \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:Aul-kAQHnToC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=9228367352374605559",
        "cites_id": [
            "9228367352374605559"
        ],
        "pub_url": "https://www.jneurosci.org/content/43/17/3107.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:97YZhza_EYAJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Learning Functional Transduction",
            "pub_year": 2023,
            "citation": "Thirty-seventh Conference on Neural Information Processing Systems, 2023",
            "author": "Mathieu Chalvidal and Thomas Serre and Rufin VanRullen",
            "conference": "Thirty-seventh Conference on Neural Information Processing Systems",
            "abstract": "Research in statistical learning has polarized into two general approaches to perform regression analysis: Transductive methods construct estimates directly based on exemplar data using generic relational principles which might suffer from the curse of dimensionality. Conversely, inductive methods can potentially fit highly complex functions at the cost of compute-intensive solution searches. In this work, we leverage the theory of vector-valued Reproducing Kernel Banach Spaces (RKBS) to propose a hybrid approach: We show that transductive regression systems can be meta-learned with gradient descent to form efficient _in-context_ neural approximators of function defined over both finite and infinite-dimensional spaces (operator regression). Once trained, our _Transducer_ can almost instantaneously capture new functional relationships and produce original image estimates, given a few pairs of input and output examples. We demonstrate the benefit of our meta-learned transductive approach to model physical systems influenced by varying external factors with little data at a fraction of the usual deep learning training costs for partial differential equations and climate modeling applications."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:gVv57TyPmFsC",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=2BFZ8cPIf6",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Gradient strikes back: How filtering out high frequencies improves explanations",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2307.09591, 2023",
            "author": "Sabine Muzellec and Leo Andeol and Thomas Fel and Rufin VanRullen and Thomas Serre",
            "journal": "arXiv preprint arXiv:2307.09591",
            "abstract": "Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthermore, our analysis reveals that the operations used in Convolutional Neural Networks (CNNs) for downsampling appear to be a significant source of this high-frequency content -- suggesting aliasing as a possible underlying basis. We then apply an optimal low-pass filter for attribution maps and demonstrate that it improves gradient-based attribution methods. We show that (i) removing high-frequency noise yields significant improvements in the explainability scores obtained with gradient-based methods across multiple models -- leading to (ii) a novel ranking of state-of-the-art methods with gradient-based methods at the top. We believe that our \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:LgRImbQfgY4C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2307.09591",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "CLIP-based image captioning via unsupervised cycle-consistency in the latent space",
            "pub_year": 2023,
            "citation": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP \u2026, 2023",
            "author": "Romain Bielawski and Rufin Vanrullen",
            "conference": "Proceedings of the 8th Workshop on Representation Learning for NLP (RepL4NLP 2023)",
            "pages": "266-275",
            "abstract": "Image captioning typically involves an image encoder to extract meaningful image features, and a text decoder to generate appropriate sentences. Powerful pretrained models can be used for both image encoding and text decoding; but in this case, a separate multimodal translation stage between image-encoder output features and text-decoder input features must be learned. One exception is when image and text features are already aligned by construction, as in the CLIP model (Contrastive Language and Image Pretraining\u2013a bimodal network pretrained on 400M image-text pairs). Pretrained CLIP-image features can be directly fed to a textdecoder trained to reconstruct captions from their pretrained CLIP-text features. Here we show that this direct captioning method is in fact sub-optimal. Instead, we propose an alternative method to translate CLIP-image features into CLIP-text features in a strictly unsupervised way, using the CycleGAN architecture\u2013originally designed for unpaired imageto-image translation. Our Latent CycleGAN, optimized solely for an unsupervised cycleconsistency objective, generates CLIP-text latent features conditioned on CLIP-image latent features and vice-versa. Using these CLIP-text latent features as input to the text decoder, our method largely outperforms the direct captioning method that uses CLIP-image features\u2013despite the fact that CLIP\u2019s large-scale pretraining should have already aligned the two feature spaces. This implies that cycle-consistency on unmatched multimodal data can be efficiently implemented in a bimodal latent space, and that CLIP-based image captioning can be improved without \u2026"
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:Ak0FvsSvgGUC",
        "num_citations": 0,
        "pub_url": "https://aclanthology.org/2023.repl4nlp-1.22.pdf",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Semi-supervised Multimodal Representation Learning through a Global Workspace",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2306.15711, 2023",
            "author": "Benjamin Devillers and L\u00e9opold Mayti\u00e9 and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2306.15711",
            "abstract": "Recent deep learning models can efficiently combine inputs from different modalities (e.g., images and text) and learn to align their latent representations, or to translate signals from one domain to another (as in image captioning, or text-to-image generation). However, current approaches mainly rely on brute-force supervised training over large multimodal datasets. In contrast, humans (and other animals) can learn useful multimodal representations from only sparse experience with matched cross-modal data. Here we evaluate the capabilities of a neural network architecture inspired by the cognitive notion of a \"Global Workspace\": a shared representation for two (or more) input modalities. Each modality is processed by a specialized system (pretrained on unimodal data, and subsequently frozen). The corresponding latent representations are then encoded to and decoded from a single shared workspace. Importantly, this architecture is amenable to self-supervised training via cycle-consistency: encoding-decoding sequences should approximate the identity function. For various pairings of vision-language modalities and across two datasets of varying complexity, we show that such an architecture can be trained to align and translate between two modalities with very little need for matched data (from 4 to 7 times less than a fully supervised approach). The global workspace representation can be used advantageously for downstream classification tasks and for robust transfer learning. Ablation studies reveal that both the shared workspace and the self-supervised cycle-consistency training are critical to the system's performance."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:Bg7qf7VwUHIC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2306.15711",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Mathematical derivation of wave propagation properties in hierarchical neural networks with predictive coding feedback dynamics",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2304.05676, 2023",
            "author": "Gr\u00e9gory Faye and Guilhem Fouilh\u00e9 and Rufin VanRullen",
            "journal": "arXiv preprint arXiv:2304.05676",
            "abstract": "Sensory perception (e.g. vision) relies on a hierarchy of cortical areas, in which neural activity propagates in both directions, to convey information not only about sensory inputs but also about cognitive states, expectations and predictions. At the macroscopic scale, neurophysiological experiments have described the corresponding neural signals as both forward and backward-travelling waves, sometimes with characteristic oscillatory signatures. It remains unclear, however, how such activity patterns relate to specific functional properties of the perceptual apparatus. Here, we present a mathematical framework, inspired by neural network models of predictive coding, to systematically investigate neural dynamics in a hierarchical perceptual system. We show that stability of the system can be systematically derived from the values of hyper-parameters controlling the different signals (related to bottom-up inputs, top-down prediction and error correction). Similarly, it is possible to determine in which direction, and at what speed neural activity propagates in the system. Different neural assemblies (reflecting distinct eigenvectors of the connectivity matrices) can simultaneously and independently display different properties in terms of stability, propagation speed or direction. We also derive continuous-limit versions of the system, both in time and in neural space. Finally, we analyze the possible influence of transmission delays between layers, and reveal the emergence of oscillations at biologically plausible frequencies."
        },
        "filled": true,
        "author_pub_id": "1pwyaYgAAAAJ:gKiMpY-AVTkC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2304.05676",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:f8cmTB-ifQwJ:scholar.google.com/",
        "cites_per_year": {}
    }
]