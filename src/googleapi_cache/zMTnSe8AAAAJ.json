[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "A phone in a basket looks like a knife in a cup: Role-filler independence in visual processing",
            "pub_year": 2024,
            "citation": "Open Mind 8, 766-794, 2024",
            "author": "Alon Hafri and Michael F Bonner and Barbara Landau and Chaz Firestone",
            "journal": "Open Mind",
            "volume": "8",
            "pages": "766-794",
            "publisher": "MIT Press",
            "abstract": "When a piece of fruit is in a bowl, and the bowl is on a table, we appreciate not only the individual objects and their features, but also the relations containment and support, which abstract away from the particular objects involved. Independent representation of roles (e.g., containers vs. supporters) and \u201cfillers\u201d of those roles (e.g., bowls vs. cups, tables vs. chairs) is a core principle of language and higher-level reasoning. But does such role-filler independence also arise in automatic visual processing? Here, we show that it does, by exploring a surprising error that such independence can produce. In four experiments, participants saw a stream of images containing different objects arranged in force-dynamic relations\u2014e.g., a phone contained in a basket, a marker resting on a garbage can, or a knife sitting in a cup. Participants had to respond to a single target image (e.g., a phone in a basket) within a stream of \u2026"
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:FsLZdJ3BAzkC",
        "num_citations": 0,
        "pub_url": "https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00146/123216",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:oJvv9WV-sFsJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Modeling dynamic social vision highlights gaps between deep learning and humans",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Kathy Garcia and Emalie McMahon and Colin Conwell and Michael F Bonner and Leyla Isik",
            "publisher": "OSF",
            "abstract": "Deep learning models trained on computer vision tasks are widely considered the most successful models of human vision to date. The majority of work that supports this idea evaluates how accurately these models predict brain and behavioral responses to static images of objects and natural scenes. Real-world vision, however, is highly dynamic, and far less work has focused on evaluating the accuracy of deep learning models in predicting responses to stimuli that move, and that involve more complicated, higher-order phenomena like social interactions. Here, we present a dataset of natural videos and captions involving complex multi-agent interactions, and we benchmark 350+ image, video, and language models on behavioral and neural responses to the videos. As with prior work, we find that many vision models reach the noise ceiling in predicting visual scene features and responses along the ventral visual stream (often considered the primary neural substrate of object and scene recognition). In contrast, image models poorly predict human action and social interaction ratings and neural responses in the lateral stream (a neural pathway increasingly theorized as specializing in dynamic, social vision). Language models (given human sentence captions of the videos) predict action and social ratings better than either image or video models, but they still perform poorly at predicting neural responses in the lateral stream. Together these results identify a major gap in AI\u2019s ability to match human social vision and highlight the importance of studying vision in dynamic, natural contexts."
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:PuOEWVtPfzwC",
        "num_citations": 0,
        "pub_url": "https://osf.io/4mpd9/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uZSPgUvyUNYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Convolutional architectures are cortex-aligned de novo",
            "pub_year": 2024,
            "citation": "bioRxiv, 2024.05. 10.593623, 2024",
            "author": "Atlas Kazemian and Eric Elmoznino and Michael F Bonner",
            "journal": "bioRxiv",
            "pages": "2024.05. 10.593623",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "What underlies the emergence of cortex-aligned representations in deep neural network models of vision? We examined networks with varied architectures but no pre-training and quantified their ability to predict image representations in the visual cortices of both monkeys and humans. We found that cortex-aligned representations emerge in convolutional architectures that combine two key manipulations of dimensionality: compression in the spatial domain and expansion in the feature domain. These dimensionality manipulations yielded encoding models of the monkey ventral stream that rivaled a classic goal-driven network. Interestingly, in the human ventral stream, there remained a substantial performance advantage for the goal-driven network, suggesting a stronger contribution from high-level semantic processing in humans. We further show that the inductive biases of convolutional architectures are critical for obtaining performance gains from feature expansion-dimensionality manipulations were relatively ineffective in other architectures and in convolutional models with targeted lesions. Our findings suggest that the architectural constraints of convolutional networks are sufficiently close to the constraints of biological vision to yield cortex-aligned representations without the need for goal-driven optimization."
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:BAanoTsO0WEC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2024.05.10.593623.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:1GXNMauDbxsJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "High-performing neural network models of visual cortex benefit from high latent dimensionality",
            "pub_year": 2024,
            "citation": "PLOS Computational Biology 20 (1), e1011792, 2024",
            "author": "Eric Elmoznino and Michael F Bonner",
            "journal": "PLOS Computational Biology",
            "volume": "20",
            "number": "1",
            "pages": "e1011792",
            "publisher": "Public Library of Science",
            "abstract": "Geometric descriptions of deep neural networks (DNNs) have the potential to uncover core representational principles of computational models in neuroscience. Here we examined the geometry of DNN models of visual cortex by quantifying the latent dimensionality of their natural image representations. A popular view holds that optimal DNNs compress their representations onto low-dimensional subspaces to achieve invariance and robustness, which suggests that better models of visual cortex should have lower dimensional geometries. Surprisingly, we found a strong trend in the opposite direction\u2014neural networks with high-dimensional image subspaces tended to have better generalization performance when predicting cortical responses to held-out stimuli in both monkey electrophysiology and human fMRI data. Moreover, we found that high dimensionality was associated with better performance when learning new categories of stimuli, suggesting that higher dimensional representations are better suited to generalize beyond their training domains. These findings suggest a general principle whereby high-dimensional geometry confers computational benefits to DNN models of visual cortex."
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:wyM6WWKXmoIC",
        "num_citations": 14,
        "citedby_url": "/scholar?hl=en&cites=13589683078237540538",
        "cites_id": [
            "13589683078237540538"
        ],
        "pub_url": "https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1011792",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ulB8KMJAmLwJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 10
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "A sparse null code emerges in deep neural networks",
            "pub_year": 2023,
            "citation": "UniReps: the First Workshop on Unifying Representations in Neural Models, 2023",
            "author": "Brian S Robinson and Nathan Drenkow and Colin Conwell and Michael Bonner",
            "conference": "UniReps: the First Workshop on Unifying Representations in Neural Models",
            "abstract": "The internal representations of deep vision models are often assumed to encode specific image features, such as contours, textures, and object parts. However, it is possible for deep networks to learn highly abstract representations that may not be linked to any specific image feature. Here we present evidence for one such abstract representation in transformers and modern convolutional architectures that appears to serve as a null code, indicating image regions that are non-diagnostic for the object class. These null codes are both statistically and qualitatively distinct from the more commonly reported feature-related codes of vision models. Specifically, these null codes have several distinct characteristics: they are highly sparse, they have a single unique activation pattern for each network, they emerge abruptly at intermediate network depths, and they are activated in a feature-independent manner by weakly informative image regions, such as backgrounds. Together, these findings reveal a new class of highly abstract representations in deep vision models: sparse null codes that seem to indicate the absence of features rather than serving as feature detectors."
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:6ZzL7HXColQC",
        "num_citations": 0,
        "pub_url": "https://bonnerlab.org/s/74_a_sparse_null_code_emerges_in_.pdf",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Canonical Dimensions of Neural Visual Representation",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4937-4937, 2023",
            "author": "Zirui Chen and Michael Bonner",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4937-4937",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "What key factors of deep neural networks (DNNs) account for their representational similarity to visual cortex? Many properties that neuroscientists proposed to be critical, such as architecture or training task, have turned out to have surprisingly little explanatory power. Instead, there appears to be a high degree of \u201cdegeneracy,\u201d as many DNNs with distinct designs yield equally good models of visual cortex. Here, we suggest that a more global perspective is needed to understand the relationship between DNNs and the brain. We reasoned that the most essential visual representations are general-purpose and thus naturally emerge from systems with diverse architectures or neuroanatomies. This leads to a specific hypothesis: it should be possible to identify a set of canonical dimensions, extensively learned by many DNNs, that best explain cortical visual representations. To test this hypothesis, we developed a \u2026"
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:g8uWPOAv7ggC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791573",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Hierarchical organization of social action features along the lateral visual pathway",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Emalie McMahon and Michael F Bonner and Leyla Isik",
            "publisher": "PsyArXiv",
            "abstract": "Recent theoretical work has argued that in addition to the classical ventral (what) and dorsal (where/how) visual streams, there is a third visual stream on the lateral surface of the brain specialized for processing social information. Like visual representations in the ventral and dorsal streams, representations in the lateral stream are thought to be hierarchically organized. However, no prior studies have comprehensively investigated the organization of naturalistic, social visual content in the lateral stream. Here we used a data-rich approach to test the hypothesis that social action information is extracted hierarchically along this pathway. Social scenes pose several computational challenges beyond those of object recognition: they are often highly dynamic and involve relations between two or more people. To address these challenges, we curated a naturalistic stimulus set of 250 three-second videos of two people engaged in everyday actions. Each clip was richly annotated for its low-level visual features, mid-level scene & object properties, visual social primitives (including the distance between people and the extent to which they were facing), and high-level information about social interactions and affective content. Using a condition-rich fMRI experiment and a within-subject encoding model approach, we found that low-level visual features are represented in early visual cortex (EVC) and middle temporal area (MT), mid-level visual-social features in extrastriate body area (EBA) and lateral occipital complex (LOC), and high-level social interaction information along the superior temporal sulcus (STS). Communicative interactions, in particular \u2026"
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:VRfTbSk87rEC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=16072428916932549311",
        "cites_id": [
            "16072428916932549311"
        ],
        "pub_url": "https://psyarxiv.com/x3avb/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vxp7z_G8DN8J:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Scene context is predictive of unconstrained object similarity judgments",
            "pub_year": 2023,
            "citation": "Cognition 239, 105535, 2023",
            "author": "Caterina Magri and Eric Elmoznino and Michael F Bonner",
            "journal": "Cognition",
            "volume": "239",
            "pages": "105535",
            "publisher": "Elsevier",
            "abstract": "What makes objects alike in the human mind? Computational approaches for characterizing object similarity have largely focused on the visual forms of objects or their linguistic associations. However, intuitive notions of object similarity may depend heavily on contextual reasoning\u2014that is, objects may be grouped together in the mind if they occur in the context of similar scenes or events. Using large-scale analyses of natural scene statistics and human behavior, we found that a computational model of the associations between objects and their scene contexts is strongly predictive of how humans spontaneously group objects by similarity. Specifically, we learned contextual prototypes for a diverse set of object categories by taking the average response of a convolutional neural network (CNN) to the scene contexts in which the objects typically occurred. In behavioral experiments, we found that contextual \u2026"
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:kUhpeDhEZMUC",
        "num_citations": 0,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010027723001695",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The spatiotemporal dynamics of social scene perception in the human brain",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4628-4628, 2023",
            "author": "Emalie McMahon and Taylor Abel and Jorge Gonzalez-Martinez and Michael F Bonner and Avniel Ghuman and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4628-4628",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Social perception is an important part of everyday life that develops early and is shared with non-human primates. To understand the spatiotemporal dynamics of naturalistic social perception in the human brain, we first curated a dataset of 250 500-ms video clips of two people performing everyday actions. We densely labeled these videos with features of visual social scene, including scene and object features, visual social primitives, and higher-level social/affective features. To investigate when and where these features are represented in the brain, patients with implanted stereoelectroencephalography electrodes viewed the videos. We used time-resolved encoding models in individual channels to investigate the time course of representations across the human brain. We find that an encoding model based on all of our social scene features predicts responses in a subset of channels around 400 ms after video \u2026"
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:lbI08cpqPnQC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791837",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Toward a computational neuroscience of visual cortex without deep learning",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5455-5455, 2023",
            "author": "Atlas Kazemian and Eric Elmoznino and Michael Bonner",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5455-5455",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "The performance of convolutional neural networks (CNNs) as representational models of visual cortex is thought to be associated with their optimization on ethologically relevant tasks. Here, we show that this view is incorrect and that there are other architectural and statistical factors that primarily account for their performance. We show this by developing a novel statistically inspired neural network that yields accurate predictions of cortical image representation without the need for optimization on supervised or self-supervised tasks. Our architecture is characterized by a core module of convolutions and max pooling, which can be stacked in a deep hierarchy. An important characteristic of our model is the use of thousands of random filters to sample the high-dimensional space of natural image statistics. These filters can be mapped to cortical responses through a simple linear-regression procedure, which we \u2026"
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:I858iXPj1OkC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791949",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Contextual coherence increases perceived numerosity independent of semantic content",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Chuyan Qu and Michael F Bonner and Nicholas K DeWind and Elizabeth M Brannon",
            "publisher": "PsyArXiv",
            "abstract": "Understanding if and how visual features systematically bias numerosity perception is central to understanding the processes that give rise to our visual number sense. Recent work demonstrated that reducing coherence of low-level visual attributes such as color and orientation systematically reduces perceived number. Here we ask when in the visual processing hierarchy coherence affects numerosity perception and specifically whether the coherence effect is exclusive to low-level visual features or instead whether it can be driven by contextual or semantic relationships. We tested adults in an ordinal numerical comparison task with contextual coherence mathematically manipulated using a statistical model of visual object co-occurrence. Across several experiments, we found that arrays with high contextual coherence were perceived as numerically larger than arrays with low contextual coherence. This contextual coherence effect was not attenuated even when we reduced objects to texforms (unrecognizable images that preserve mid-level visual features) or removed semantic content from the images through box scrambling and diffeomorphic warping. Together, these results suggest that visual coherence derived from natural statistics of object co-occurrence systematically alters perceived numerosity at low-level visual processing, even before later stages at which items can be explicitly categorized and identified."
        },
        "filled": true,
        "author_pub_id": "zMTnSe8AAAAJ:U0iAMwwPxtsC",
        "num_citations": 0,
        "pub_url": "https://psyarxiv.com/tcn8q/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:H4ULqxOMXEsJ:scholar.google.com/",
        "cites_per_year": {}
    }
]