[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Position: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience",
            "pub_year": 2024,
            "citation": "Forty-first International Conference on Machine Learning, 2024",
            "author": "Martina G Vilas and Federico Adolfi and David Poeppel and Gemma Roig",
            "conference": "Forty-first International Conference on Machine Learning",
            "abstract": "Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:AXPGKjj_ei8C",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=66KmnMhGU5",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:pKAMPZ0qGV0J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Position Paper: An Inner Interpretability Framework for AI Inspired by Lessons from Cognitive Neuroscience",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2406.01352, 2024",
            "author": "Martina G Vilas and Federico Adolfi and David Poeppel and Gemma Roig",
            "journal": "arXiv preprint arXiv:2406.01352",
            "abstract": "Inner Interpretability is a promising emerging field tasked with uncovering the inner mechanisms of AI systems, though how to develop these mechanistic theories is still much debated. Moreover, recent critiques raise issues that question its usefulness to advance the broader goals of AI. However, it has been overlooked that these issues resemble those that have been grappled with in another field: Cognitive Neuroscience. Here we draw the relevant connections and highlight lessons that can be transferred productively between fields. Based on these, we propose a general conceptual framework and give concrete methodological strategies for building mechanistic explanations in AI inner interpretability research. With this conceptual framework, Inner Interpretability can fend off critiques and position itself on a productive path to explain AI systems."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:XiVPGOgt02cC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2406.01352",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:pKAMPZ0qGV0J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Learning Object Semantic Similarity with Self-Supervision",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2405.05143, 2024",
            "author": "Arthur Aubret and Timothy Schauml\u00f6ffel and Gemma Roig and Jochen Triesch",
            "journal": "arXiv preprint arXiv:2405.05143",
            "abstract": "Humans judge the similarity of two objects not just based on their visual appearance but also based on their semantic relatedness. However, it remains unclear how humans learn about semantic relationships between objects and categories. One important source of semantic knowledge is that semantically related objects frequently co-occur in the same context. For instance, forks and plates are perceived as similar, at least in part, because they are often experienced together in a ``kitchen\" or ``eating'' context. Here, we investigate whether a bio-inspired learning principle exploiting such co-occurrence statistics suffices to learn a semantically structured object representation {\\em de novo} from raw visual or combined visual and linguistic input. To this end, we simulate temporal sequences of visual experience by binding together short video clips of real-world scenes showing objects in different contexts. A bio-inspired neural network model aligns close-in-time visual representations while also aligning visual and category label representations to simulate visuo-language alignment. Our results show that our model clusters object representations based on their context, e.g. kitchen or bedroom, in particular in high-level layers of the network, akin to humans. In contrast, lower-level layers tend to better reflect object identity or category. To achieve this, the model exploits two distinct strategies: the visuo-language alignment ensures that different objects of the same category are represented similarly, whereas the temporal alignment leverages that objects from the same context are frequently seen in succession to make their representations more similar \u2026"
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:bnK-pcrLprsC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2405.05143",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Rrh2pJS9U5gJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Optimized Financial Planning: Integrating Individual and Cooperative Budgeting Models with LLM Recommendations",
            "pub_year": 2024,
            "citation": "",
            "author": "Irene de Zarz\u00e0 i Cubero and Joaquim de Curt\u00f2 i D\u00edaz and Gemma Roig and Carlos T Calafate",
            "abstract": "In today\u2019s complex economic environment, individuals and households alike grapple with the challenge of financial planning. This paper introduces novel methodologies for both individual and cooperative (household) financial budgeting. We firstly propose an optimization framework for individual budget allocation, aiming to maximize savings by efficiently distributing monthly income among various expense categories. We then extend this model to households, wherein the complexity of handling multiple incomes and shared expenses is addressed. The cooperative model prioritizes not only maximized savings but also the preferences and needs of each member, fostering a harmonious financial environment, whether they are short-term needs or long-term aspirations. A notable innovation in our approach is the integration of recommendations from a large language model (LLM). Given its vast training data and potent inferential capabilities, the LLM provides initial feasible solutions to our optimization problems, acting as a guiding beacon for individuals and households unfamiliar with the nuances of financial planning. Our preliminary results indicate that the LLM-recommended solutions result in budget plans that are both economically sound, meaning that they are consistent with established financial management principles and promote fiscal resilience and stability, and aligned with the financial goals and preferences of the concerned parties. This integration of AI-driven recommendations with econometric models, as an instantiation of an extended coevolutionary (EC) theory, paves the way for a new era in financial planning, making it more \u2026"
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:V3AGJWp-ZtQC",
        "num_citations": 0,
        "pub_url": "https://repositorio.comillas.edu/xmlui/handle/11531/88114",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:4M1W_D36BLEJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Large Language Model-Informed X-ray Photoelectron Spectroscopy Data Analysis",
            "pub_year": 2024,
            "citation": "Signals 5 (2), 181-201, 2024",
            "author": "J de Curt\u00f2 and I de Zarz\u00e0 and Gemma Roig and Carlos T Calafate",
            "journal": "Signals",
            "volume": "5",
            "number": "2",
            "pages": "181-201",
            "publisher": "MDPI",
            "abstract": "X-ray photoelectron spectroscopy (XPS) remains a fundamental technique in materials science, offering invaluable insights into the chemical states and electronic structure of a material. However, the interpretation of XPS spectra can be complex, requiring deep expertise and often sophisticated curve-fitting methods. In this study, we present a novel approach to the analysis of XPS data, integrating the utilization of large language models (LLMs), specifically OpenAI\u2019s GPT-3.5/4 Turbo to provide insightful guidance during the data analysis process. Working in the framework of the CIRCE-NAPP beamline at the CELLS ALBA Synchrotron facility where data are obtained using ambient pressure X-ray photoelectron spectroscopy (APXPS), we implement robust curve-fitting techniques on APXPS spectra, highlighting complex cases including overlapping peaks, diverse chemical states, and noise presence. Post curve fitting, we engage the LLM to facilitate the interpretation of the fitted parameters, leaning on its extensive training data to simulate an interaction corresponding to expert consultation. The manuscript presents also a real use case utilizing GPT-4 and Meta\u2019s LLaMA-2 and describes the integration of the functionality into the TANGO control system. Our methodology not only offers a fresh perspective on XPS data analysis, but also introduces a new dimension of artificial intelligence (AI) integration into scientific research. It showcases the power of LLMs in enhancing the interpretative process, particularly in scenarios wherein expert knowledge may not be immediately available. Despite the inherent limitations of LLMs, their potential in the realm \u2026"
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:q3oQSFYPqjQC",
        "num_citations": 0,
        "pub_url": "https://www.mdpi.com/2624-6120/5/2/10",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:vXCqmAezQ0gJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Generative Adversarial Collaborations: A practical guide for conference organizers and participating scientists",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2402.12604, 2024",
            "author": "Gunnar Blohm and Benjamin Peters and Ralf Haefner and Leyla Isik and Nikolaus Kriegeskorte and Jennifer S Lieberman and Carlos R Ponce and Gemma Roig and Megan AK Peters",
            "journal": "arXiv preprint arXiv:2402.12604",
            "abstract": "Generative adversarial collaborations (GACs) are a form of formal teamwork between groups of scientists with diverging views. The goal of GACs is to identify and ultimately resolve the most important challenges, controversies, and exciting theoretical and empirical debates in a given research field. A GAC team would develop specific, agreed-upon avenues to resolve debates in order to move a field of research forward in a collaborative way. Such adversarial collaborations have many benefits and opportunities but also come with challenges. Here, we use our experience from (1) creating and running the GAC program for the Cognitive Computational Neuroscience (CCN) conference and (2) implementing and leading GACs on particular scientific problems to provide a practical guide for future GAC program organizers and leaders of individual GACs."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:BrmTIyaxlBUC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2402.12604",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:uxFImCCXViEJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic Play",
            "pub_year": 2023,
            "citation": "Proceedings of the 2023 IEEE International Conference on Development and \u2026, 2023",
            "author": "Jochen Triesch Timothy Schauml\u00f6ffel and Arthur Aubret and Gemma Roig",
            "journal": "Proceedings of the 2023 IEEE International Conference on Development and Learning (ICDL)"
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:VOx2b1Wkg3QC",
        "num_citations": 0,
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "PEACS: Prefix encoding for auditory caption synthesis",
            "pub_year": 2023,
            "citation": "Proc. Conf. Detection Classification Acoust. Scenes Events Challenge, 1-3, 2023",
            "author": "Timothy Schauml\u00f6ffel and Martina G Vilas and Gemma Roig",
            "journal": "Proc. Conf. Detection Classification Acoust. Scenes Events Challenge",
            "pages": "1-3",
            "abstract": "This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:XiSMed-E-HIC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=13148287201322677619",
        "cites_id": [
            "13148287201322677619"
        ],
        "pub_url": "https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:czWnB4wZeLYJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "LLM multimodal traffic accident forecasting",
            "pub_year": 2023,
            "citation": "Sensors 23 (22), 9225, 2023",
            "author": "I de Zarz\u00e0 and J de Curt\u00f2 and Gemma Roig and Carlos T Calafate",
            "journal": "Sensors",
            "volume": "23",
            "number": "22",
            "pages": "9225",
            "publisher": "MDPI",
            "abstract": "With the rise in traffic congestion in urban centers, predicting accidents has become paramount for city planning and public safety. This work comprehensively studied the efficacy of modern deep learning (DL) methods in forecasting traffic accidents and enhancing Level-4 and Level-5 (L-4 and L-5) driving assistants with actionable visual and language cues. Using a rich dataset detailing accident occurrences, we juxtaposed the Transformer model against traditional time series models like ARIMA and the more recent Prophet model. Additionally, through detailed analysis, we delved deep into feature importance using principal component analysis (PCA) loadings, uncovering key factors contributing to accidents. We introduce the idea of using real-time interventions with large language models (LLMs) in autonomous driving with the use of lightweight compact LLMs like LLaMA-2 and Zephyr-7b-\u03b1. Our exploration extends to the realm of multimodality, through the use of Large Language-and-Vision Assistant (LLaVA)\u2014a bridge between visual and linguistic cues by means of a Visual Language Model (VLM)\u2014in conjunction with deep probabilistic reasoning, enhancing the real-time responsiveness of autonomous driving systems. In this study, we elucidate the advantages of employing large multimodal models within DL and deep probabilistic programming for enhancing the performance and usability of time series forecasting and feature weight importance, particularly in a self-driving scenario. This work paves the way for safer, smarter cities, underpinned by data-driven decision making."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:fQNAKQ3IYiAC",
        "num_citations": 0,
        "pub_url": "https://www.mdpi.com/1424-8220/23/22/9225",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "LLM Multimodal Traffic Accident Forecasting",
            "pub_year": 2023,
            "citation": "Sensors 23 (22), 9225, 2023",
            "author": "I de Zarz\u00e0 and J de Curt\u00f2 and Gemma Roig and Carlos T Calafate",
            "journal": "Sensors",
            "volume": "23",
            "number": "22",
            "pages": "9225",
            "publisher": "MDPI",
            "abstract": "With the rise in traffic congestion in urban centers, predicting accidents has become paramount for city planning and public safety. This work comprehensively studied the efficacy of modern deep learning (DL) methods in forecasting traffic accidents and enhancing Level-4 and Level-5 (L-4 and L-5) driving assistants with actionable visual and language cues. Using a rich dataset detailing accident occurrences, we juxtaposed the Transformer model against traditional time series models like ARIMA and the more recent Prophet model. Additionally, through detailed analysis, we delved deep into feature importance using principal component analysis (PCA) loadings, uncovering key factors contributing to accidents. We introduce the idea of using real-time interventions with large language models (LLMs) in autonomous driving with the use of lightweight compact LLMs like LLaMA-2 and Zephyr-7b-\u03b1. Our exploration extends to the realm of multimodality, through the use of Large Language-and-Vision Assistant (LLaVA)\u2014a bridge between visual and linguistic cues by means of a Visual Language Model (VLM)\u2014in conjunction with deep probabilistic reasoning, enhancing the real-time responsiveness of autonomous driving systems. In this study, we elucidate the advantages of employing large multimodal models within DL and deep probabilistic programming for enhancing the performance and usability of time series forecasting and feature weight importance, particularly in a self-driving scenario. This work paves the way for safer, smarter cities, underpinned by data-driven decision making."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:fQNAKQ3IYiAC",
        "num_citations": 0,
        "pub_url": "https://www.mdpi.com/1424-8220/23/22/9225",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Learning Class and Domain Augmentations for Single-Source Open-Domain Generalization",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2311.02599, 2023",
            "author": "Prathmesh Bele and Valay Bundele and Avigyan Bhattacharya and Ankit Jha and Gemma Roig and Biplab Banerjee",
            "journal": "arXiv preprint arXiv:2311.02599",
            "abstract": "Single-source open-domain generalization (SS-ODG) addresses the challenge of labeled source domains with supervision during training and unlabeled novel target domains during testing. The target domain includes both known classes from the source domain and samples from previously unseen classes. Existing techniques for SS-ODG primarily focus on calibrating source-domain classifiers to identify open samples in the target domain. However, these methods struggle with visually fine-grained open-closed data, often misclassifying open samples as closed-set classes. Moreover, relying solely on a single source domain restricts the model's ability to generalize. To overcome these limitations, we propose a novel framework called SODG-Net that simultaneously synthesizes novel domains and generates pseudo-open samples using a learning-based objective, in contrast to the ad-hoc mixing strategies commonly found in the literature. Our approach enhances generalization by diversifying the styles of known class samples using a novel metric criterion and generates diverse pseudo-open samples to train a unified and confident multi-class classifier capable of handling both open and closed-set data. Extensive experimental evaluations conducted on multiple benchmarks consistently demonstrate the superior performance of SODG-Net compared to the literature."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:LPZeul_q3PIC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2311.02599",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The algonauts project 2023 challenge: How the human brain makes sense of natural scenes",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2301.03198, 2023",
            "author": "Alessandro T Gifford and Benjamin Lahner and Sari Saba-Sadiya and Martina G Vilas and Alex Lascelles and Aude Oliva and Kendrick Kay and Gemma Roig and Radoslaw M Cichy",
            "journal": "arXiv preprint arXiv:2301.03198",
            "abstract": "The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to ~73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:KxtntwgDAa4C",
        "num_citations": 9,
        "citedby_url": "/scholar?hl=en&cites=1174422521076802658",
        "cites_id": [
            "1174422521076802658"
        ],
        "pub_url": "https://arxiv.org/abs/2301.03198",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:YmQAgQ9jTBAJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 9
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "EmoMV: Affective Music-Video Correspondence Learning Datasets for Classification and Retrieval",
            "pub_year": 2023,
            "citation": "Information Fusion 91, 64-79, 2023",
            "author": "Ha Thi Phuong Thao and Dorien Herremans and Gemma Roig",
            "journal": "Information Fusion",
            "volume": "91",
            "pages": "64-79",
            "publisher": "Elsevier",
            "abstract": "Studies in affective audio\u2013visual correspondence learning require ground-truth data to train, validate, and test models. The number of available datasets together with benchmarks, however, is still limited. In this paper, we create a collection of three datasets (called EmoMV) for affective correspondence learning between music and video modalities. The first two datasets (called EmoMV-A, and EmoMV-B, respectively) are constructed by making use of music video segments from other available datasets. The third one called EmoMV-C is created from music videos that we self-collected from YouTube. The music-video pairs in our datasets are annotated as matched or mismatched in terms of the emotions they are conveying. The emotions are annotated by humans in the EmoMV-A dataset, while in the EmoMV-B and EmoMV-C datasets they are predicted using a pretrained deep neural network. A user study is carried \u2026"
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:CHSYGLWDkRkC",
        "num_citations": 8,
        "citedby_url": "/scholar?hl=en&cites=10100485155605414894",
        "cites_id": [
            "10100485155605414894"
        ],
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1566253522001725",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:7oP3_rshLIwJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 7
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Signature and log-signature for the study of empirical distributions generated with GANs",
            "pub_year": 2023,
            "citation": "Electronics 12 (10), 2192, 2023",
            "author": "Joaquim de Curt\u00f2 and Irene de Zarz\u00e0 and Gemma Roig and Carlos T Calafate",
            "journal": "Electronics",
            "volume": "12",
            "number": "10",
            "pages": "2192",
            "publisher": "MDPI",
            "abstract": "In this paper, we address the research gap in efficiently assessing Generative Adversarial Network (GAN) convergence and goodness of fit by introducing the application of the Signature Transform to measure similarity between image distributions. Specifically, we propose the novel use of Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) Signature, along with Log-Signature, as alternatives to existing methods such as Fr\u00e9chet Inception Distance (FID) and Multi-Scale Structural Similarity Index Measure (MS-SSIM). Our approach offers advantages in terms of efficiency and effectiveness, providing a comprehensive understanding and extensive evaluations of GAN convergence and goodness of fit. Furthermore, we present innovative analytical measures based on statistics by means of Kruskal\u2013Wallis to evaluate the goodness of fit of GAN sample distributions. Unlike existing GAN measures, which are based on deep neural networks and require extensive GPU computations, our approach significantly reduces computation time and is performed on the CPU while maintaining the same level of accuracy. Our results demonstrate the effectiveness of the proposed method in capturing the intrinsic structure of the generated samples, providing meaningful insights into GAN performance. Lastly, we evaluate our approach qualitatively using Principal Component Analysis (PCA) and adaptive t-Distributed Stochastic Neighbor Embedding (t-SNE) for data visualization, illustrating the plausibility of our method."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:WbkHhVStYXYC",
        "num_citations": 4,
        "citedby_url": "/scholar?hl=en&cites=14142471096293437390",
        "cites_id": [
            "14142471096293437390"
        ],
        "pub_url": "https://www.mdpi.com/2079-9292/12/10/2192",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:zq_MNIgmRMQJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 2,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Emergent Cooperation and Strategy Adaptation in Multi-Agent Systems: An Extended Coevolutionary Theory with LLMs",
            "pub_year": 2023,
            "citation": "Electronics 12 (12), 2722, 2023",
            "author": "I de Zarz\u00e0 and J de Curt\u00f2 and Gemma Roig and Pietro Manzoni and Carlos T Calafate",
            "journal": "Electronics",
            "volume": "12",
            "number": "12",
            "pages": "2722",
            "publisher": "MDPI",
            "abstract": "The increasing complexity of Multi-Agent Systems (MASs), coupled with the emergence of Artificial Intelligence (AI) and Large Language Models (LLMs), have highlighted significant gaps in our understanding of the behavior and interactions of diverse entities within dynamic environments. Traditional game theory approaches have often been employed in this context, but their utility is limited by the static and homogenous nature of their models. With the transformative influence of AI and LLMs on business and society, a more dynamic and nuanced theoretical framework is necessary to guide the design and management of MASs. In response to this pressing need, we propose an Extended Coevolutionary (EC) Theory in this paper. This alternative framework incorporates key aspects of coevolutionary dynamics, adaptive learning, and LLM-based strategy recommendations to model and analyze the strategic interactions among heterogeneous agents in MASs. It goes beyond game theory by acknowledging and addressing the diverse interactions (economic transactions, social relationships, information exchange) and the variability in risk aversion, social preferences, and learning capabilities among entities. To validate the effectiveness of the EC framework, we developed a simulation environment that enabled us to explore the emergence of cooperation and defection patterns in MASs. The results demonstrated the potential of our framework to promote cooperative behavior and maintain robustness in the face of disruptions. The dynamics and evolution of the Multi-Agent System over time were also visualized using advanced techniques. Our \u2026"
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:u9iWguZQMMsC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=10624256355472113045",
        "cites_id": [
            "10624256355472113045"
        ],
        "pub_url": "https://www.mdpi.com/2079-9292/12/12/2722",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:lf2Zlt3wcJMJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Summarization of Videos with the Signature Transform",
            "pub_year": 2023,
            "citation": "Electronics 12 (7), 1735, 2023",
            "author": "Joaquim de Curt\u00f2 and Irene de Zarz\u00e0 and Gemma Roig and Carlos T Calafate",
            "journal": "Electronics",
            "volume": "12",
            "number": "7",
            "pages": "1735",
            "publisher": "MDPI",
            "abstract": "This manuscript presents a new benchmark for assessing the quality of visual summaries without the need for human annotators. It is based on the Signature Transform, specifically focusing on the RMSE and the MAE Signature and Log-Signature metrics, and builds upon the assumption that uniform random sampling can offer accurate summarization capabilities. We provide a new dataset comprising videos from Youtube and their corresponding automatic audio transcriptions. Firstly, we introduce a preliminary baseline for automatic video summarization, which has at its core a Vision Transformer, an image\u2013text model pre-trained with Contrastive Language\u2013Image Pre-training (CLIP), as well as a module of object detection. Following that, we propose an accurate technique grounded in the harmonic components captured by the Signature Transform, which delivers compelling accuracy. The analytical measures are extensively evaluated, and we conclude that they strongly correlate with the notion of a good summary."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:p2g8aNsByqUC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=11618654879579509463",
        "cites_id": [
            "11618654879579509463"
        ],
        "pub_url": "https://www.mdpi.com/2079-9292/12/7/1735",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:1xIyAQ7BPaEJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 3
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "LLM-Informed Multi-Armed Bandit Strategies for Non-Stationary Environments",
            "pub_year": 2023,
            "citation": "Electronics 12 (13), 2814, 2023",
            "author": "J de Curt\u00f2 and I de Zarz\u00e0 and Gemma Roig and Juan Carlos Cano and Pietro Manzoni and Carlos T Calafate",
            "journal": "Electronics",
            "volume": "12",
            "number": "13",
            "pages": "2814",
            "publisher": "MDPI",
            "abstract": "In this paper, we introduce an innovative approach to handling the multi-armed bandit (MAB) problem in non-stationary environments, harnessing the predictive power of large language models (LLMs). With the realization that traditional bandit strategies, including epsilon-greedy and upper confidence bound (UCB), may struggle in the face of dynamic changes, we propose a strategy informed by LLMs that offers dynamic guidance on exploration versus exploitation, contingent on the current state of the bandits. We bring forward a new non-stationary bandit model with fluctuating reward distributions and illustrate how LLMs can be employed to guide the choice of bandit amid this variability. Experimental outcomes illustrate the potential of our LLM-informed strategy, demonstrating its adaptability to the fluctuating nature of the bandit problem, while maintaining competitive performance against conventional strategies. This study provides key insights into the capabilities of LLMs in enhancing decision-making processes in dynamic and uncertain scenarios."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:738O_yMBCRsC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=14955449359568500186",
        "cites_id": [
            "14955449359568500186"
        ],
        "pub_url": "https://www.mdpi.com/2079-9292/12/13/2814",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "LLM Adaptive PID Control for B5G Truck Platooning Systems",
            "pub_year": 2023,
            "citation": "Sensors 23 (13), 5899, 2023",
            "author": "I de Zarz\u00e0 and J de Curt\u00f2 and Gemma Roig and Carlos T Calafate",
            "journal": "Sensors",
            "volume": "23",
            "number": "13",
            "pages": "5899",
            "publisher": "MDPI",
            "abstract": "This paper presents an exploration into the capabilities of an adaptive PID controller within the realm of truck platooning operations, situating the inquiry within the context of Cognitive Radio and AI-enhanced 5G and Beyond 5G (B5G) networks. We developed a Deep Learning (DL) model that emulates an adaptive PID controller, taking into account the implications of factors such as communication latency, packet loss, and communication range, alongside considerations of reliability, robustness, and security. Furthermore, we harnessed a Large Language Model (LLM), GPT-3.5-turbo, to deliver instantaneous performance updates to the PID system, thereby elucidating its potential for incorporation into AI-enabled radio and networks. This research unveils crucial insights for augmenting the performance and safety parameters of vehicle platooning systems within B5G networks, concurrently underlining the prospective applications of LLMs within such technologically advanced communication environments."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:Tiz5es2fbqcC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=8511345799558055843",
        "cites_id": [
            "8511345799558055843"
        ],
        "pub_url": "https://www.mdpi.com/1424-8220/23/13/5899",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Peacs: Prefix encoding for auditory caption synthesis",
            "pub_year": 2023,
            "citation": "DCASE2023 Challenge, Tech. Rep, 2023",
            "author": "Timothy Schauml\u00f6ffel and Martina G Vilas and Gemma Roig",
            "publisher": "DCASE2023 Challenge, Tech. Rep",
            "abstract": "This technical report describes an Automated Audio Captioning system for the Detection and Classification of Acoustic Scenes and Events (DCASE) 2023 Challenge, Task 6a (automated audio captioning). Our approach employs an encoder-decoder architecture, with the encoder utilizing a large contrastive pre-trained HTS-AT capable of handling variable-length audio segments. The decoder is based on the GPT2 model. To incorporate audio into the decoding process, we employ a light mapping network that translates audio representations into a prefix, effectively guiding the decoder\u2019s generation process. Given the limited data availability, we pre-train our model on various audio captioning datasets and fine-tune it on Clotho. We reach a SPIDERr-FL score of 29.3 on the evaluation split of the Clotho-v2 dataset."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:XiSMed-E-HIC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=13148287201322677619",
        "cites_id": [
            "13148287201322677619"
        ],
        "pub_url": "https://dcase.community/documents/challenge2023/technical_reports/DCASE2023_Schaumloeffel_107_t6a.pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:czWnB4wZeLYJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Analyzing Vision Transformers for Image Classification in Class Embedding Space",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2310.18969, 2023",
            "author": "Martina G Vilas and Timothy Schauml\u00f6ffel and Gemma Roig",
            "journal": "arXiv preprint arXiv:2310.18969",
            "abstract": "Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:8AbLer7MMksC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2310.18969",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "DynamicsDiffusion: Generating and Rare Event Sampling of Molecular Dynamic Trajectories Using Diffusion Models",
            "pub_year": 2023,
            "citation": "NeurIPS 2023 AI for Science Workshop, 2023",
            "author": "Magnus Petersen and Gemma Roig and Roberto Covino",
            "conference": "NeurIPS 2023 AI for Science Workshop",
            "abstract": "Molecular dynamics simulations are fundamental tools for quantitative molecular sciences. However, these simulations are computationally demanding and often struggle to sample rare events crucial for understanding spontaneous organization and reconfiguration in complex systems. To improve general speed and the ability to sample rare events in a directed fashion, we propose a method called  based on denoising diffusion probabilistic models (DDPM) to generate molecular dynamics trajectories from noise. The generative model can then serve as a surrogate to sample rare events. We leverage the properties of DDPMs, such as conditional generation, the ability to generate variations of trajectories, and those with certain conditions, such as crossing from one state to another, using the 'inpainting' property of DDPMs, which became only applicable when generating whole trajectories and not just individual conformations. To our knowledge, this is the first deep generative modeling for generating molecular dynamics trajectories. We hope this work will motivate a new generation of generative modeling for the study of molecular dynamics."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:5Ul4iDaHHb8C",
        "num_citations": 0,
        "pub_url": "https://openreview.net/forum?id=pwYCCq4xAf",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Visual features are processed before navigational affordances in the human brain",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.06. 27.546695, 2023",
            "author": "Kshitij Dwivedi and Sari Sadiya and Marta P Balode and Gemma Roig and Radoslaw Cichy",
            "journal": "bioRxiv",
            "pages": "2023.06. 27.546695",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "To navigate through their immediate environment humans process scene information rapidly. How does the cascade of neural processing elicited by scene viewing to facilitate navigational planning unfold over time? To investigate, we recorded human brain responses to visual scenes with electroencephalography (EEG) and related those to computational models that operationalize three aspects of scene processing (2D, 3D, and semantic information), as well as to a behavioral model capturing navigational affordances. We found a temporal processing hierarchy: navigational affordance is processed later than the other scene features (2D, 3D, and semantic) investigated. This reveals the temporal order with which the human brain computes complex scene information and suggests that the brain leverages these pieces of information to plan navigation."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:K3LRdlH-MEoC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.06.27.546695.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "BOLD Moments: modeling short visual events through a video fMRI dataset and metadata",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.03. 12.530887, 2023",
            "author": "Benjamin Lahner and Kshitij Dwivedi and Polina Iamshchinina and Monika Graumann and Alex Lascelles and Gemma Roig and Alessandro Thomas Gifford and Bowen Pan and SouYoung Jin and N Apurva Ratan Murty and Kendrick Kay and Aude Oliva and Radoslaw Cichy",
            "journal": "bioRxiv",
            "pages": "2023.03. 12.530887",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Grasping the meaning of everyday visual events is a fundamental feat of human intelligence that hinges on diverse neural processes ranging from vision to higher-level cognition. Deciphering the neural basis of visual event understanding requires rich, extensive, and appropriately designed experimental data. However, this type of data is hitherto missing. To fill this gap, we introduce the BOLD Moments Dataset (BMD), a large dataset of whole-brain fMRI responses to over 1,000 short (3s) naturalistic video clips and accompanying metadata. We show visual events interface with an array of processes, extending even to memory, and we reveal a match in hierarchical processing between brains and video-computable deep neural networks. Furthermore, we showcase that BMD successfully captures temporal dynamics of visual events at second resolution. BMD thus establishes a critical groundwork for investigations of the neural basis of visual event understanding."
        },
        "filled": true,
        "author_pub_id": "6MjMhT4AAAAJ:OU6Ihb5iCvQC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.03.12.530887.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jESFCRR-e_wJ:scholar.google.com/",
        "cites_per_year": {}
    }
]