[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Category-selective representation of relationships in visual cortex",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Etienne Abassi and Liuba Papeo",
            "publisher": "PsyArXiv",
            "abstract": "Understanding social interaction requires processing social agents and their relationship. Latest results show that much of this process is visually solved: visual areas can represent multiple people encoding emergent information about their interaction that is not explained by the response to the individuals alone. A neural signature of this process is an increased response in visual areas, to face-to-face (seemingly interacting) people, relative to people presented as unrelated (back-to-back). This effect highlighted a network of visual areas for representing relational information. How is this network organized? Using functional MRI, we measured brain activity of healthy female and male humans (N= 42), in response to images of two faces or two (head-blurred) bodies, facing toward or away from each other. Taking the facing> non-facing effect as signature of relation perception, we found that relations between faces and between bodies were coded in distinct areas, mirroring the categorical representation of faces and bodies in visual cortex. Additional analyses suggest the existence of a third network encoding relations between (non-social) objects. Finally, a separate occipitotemporal network showed generalization of relational information across body, face and non-social object dyads (multivariate-pattern classification analysis), revealing shared properties of relations across categories. In sum, beyond single entities, visual cortex encodes the relations that bind multiple entities into relationships; it does so in a category-selective fashion, thus respecting a general organizing principle of representation in high-level vision. Visual areas encoding \u2026"
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:J_g5lzvAfSwC",
        "num_citations": 0,
        "pub_url": "https://osf.io/preprints/psyarxiv/w9c8x/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Evidence for a role of synchrony but not common fate in the perception of biological group movements",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023",
            "author": "Emiel Cracco and Liuba Papeo and Jan R Wiersema",
            "journal": "bioRxiv",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Extensive research has shown that observers are able to efficiently extract summary information from groups of people. However, little is known about the cues that determine whether multiple people are represented as a social group or as independent individuals. Initial research on this topic has primarily focused on the role of static cues. Here, we instead investigate the role of dynamic cues. In two experiments with male and female human participants, we use EEG frequency tagging to investigate the influence of two fundamental Gestalt principles - synchrony and common fate - on the grouping of biological movements. In Experiment 1, we find that brain responses coupled to four point-light figures walking together are enhanced when they move in sync vs. out of sync, but only when they are presented upright. In contrast, we found no effect of movement direction (i.e., common fate). In Experiment 2, we rule out \u2026"
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:RYcK_YlVTxYC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.10.21.563400",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A perceptual cue-based mechanism for automatic assignment of thematic agent and patient roles",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Sofie Vettori and Catherine Odin and Jean-R\u00e9my Hochmann and Liuba Papeo",
            "publisher": "PsyArXiv",
            "abstract": "Understanding a social event requires assigning the participating entities to roles such as agent and patient, a mental operation that is reportedly effortless. We investigated whether, in processing visual scenes, role assignment is accomplished automatically (ie, when the task does not require it), based on visuo-spatial information alone. Participants (male and female human adults) saw a series of images featuring the same male and female actors next to each other, one in an agent-like (more dynamic, leaning forward) and the other in a patient-like (static/less dynamic) posture. They had to indicate the side (left/right) of a target-actor (ie, the female). From trial to trial, body postures changed, but the roles, defined by the type of posture, sometimes changed, sometimes not. If participants spontaneously saw the actors as agent and patient, they should be slower to respond when roles switched from trial n-1 to trial n, than when they stayed the same (role-switch cost). Results confirmed this hypothesis (Experiments 1-3). A role-switch cost was also found when roles were defined by another visual relational cue, the relative positioning (where one actor stands relative to another), but not when actors were presented in isolation (Experiments 4-6). These findings reveal a mechanism for automatic role assignment based on encoding of visual relational information in social (multiple-person) scenes. Since we found that role assignment in one trial affected the same process in the subsequent trial, this mechanism must be one that assigns entities in a scene, to the abstract categories of agent and patient."
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:NaGl4SEjCO4C",
        "num_citations": 0,
        "pub_url": "https://osf.io/preprints/psyarxiv/ydgca/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Intermodulation responses show integration of interacting bodies in a new whole",
            "pub_year": 2023,
            "citation": "Cortex 165, 129-140, 2023",
            "author": "Nicolas Goupil and Jean-R\u00e9my Hochmann and Liuba Papeo",
            "journal": "Cortex",
            "volume": "165",
            "pages": "129-140",
            "publisher": "Elsevier",
            "abstract": "People are often seen among other people, relating to and interacting with one another. Recent studies suggest that socially relevant spatial relations between bodies, such as the face-to-face positioning, or facingness, change the visual representation of those bodies, relative to when the same items appear unrelated (e.g., back-to-back) or in isolation. The current study addresses the hypothesis that face-to-face bodies give rise to a new whole, an integrated representation of individual bodies in a new perceptual unit. Using frequency-tagging EEG, we targeted, as a measure of integration, an EEG correlate of the non-linear combination of the neural responses to each of two individual bodies presented either face-to-face as if interacting, or back-to-back. During EEG recording, participants (N = 32) viewed two bodies, either face-to-face or back-to-back, flickering at two different frequencies (F1 and F2), yielding two \u2026"
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:NMxIlDl6LWMC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=11379490925539417874",
        "cites_id": [
            "11379490925539417874"
        ],
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010945223001156",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Et-OpbQS7J0J:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Category-specific effects of high-level relations in visual search",
            "pub_year": 2023,
            "citation": "OSF Preprints, 2023",
            "author": "Nicolas Goupil and Daniel Kaiser and Liuba Papeo",
            "publisher": "OSF Preprints",
            "abstract": "Recent empirical findings demonstrate that, in visual search for a target in an array of distractors, observers exploit information about object relations to increase search efficiency. We investigated how people searched for interacting people in a crowd, and how the eccentricity of the target affected the search (Experiments 1-3). Participants briefly viewed crowded arrays and had to search for an interacting dyad (two bodies face-to-face) among non-interacting dyads (back-to-back distractors) or vice versa, with the target presented in the attended central location or at peripheral locations. With central targets, we found a search asymmetry, whereby interacting people among non-interacting people were detected better than non-interacting people among interacting people. With peripheral targets, non-interacting targets were detected better than interacting targets. In Experiment 4, we asked whether these asymmetries generalized to object pairs whose spatial relations did or did not form functionally interacting sets (computer screen above keyboard). Results showed that non-interacting targets were detected better than interacting targets, whether presented in central or peripheral locations. Thus, the effect of relational information on visual search is contingent on both stimulus category and attentional focus. Across both stimulus categories (bodies and objects), search is facilitated when individual distractor-items can be organized in larger structured units (social interaction or functional set), effectively reducing the number of distractors. The presentation of social interaction at the attended (central) location breaks this search pattern by readily \u2026"
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:hMod-77fHWUC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=9316523349721484676",
        "cites_id": [
            "9316523349721484676"
        ],
        "pub_url": "https://osf.io/n7uvk/download",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:hA3IIaDwSoEJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Visual preference for socially relevant spatial relations in humans and monkeys",
            "pub_year": 2023,
            "citation": "OSF Preprints, 2023",
            "author": "Nicolas Goupil and Holly Rayson and Emilie Serraille and Alice Massera and Pier Francesco Ferrari and Jean-R\u00e9my Hochmann and Liuba Papeo",
            "publisher": "OSF Preprints",
            "abstract": "As an extremely relevant social signal, a body/face/gaze facing towards oneself holds an individual\u2019s attention. We asked whether, going beyond an egocentric stance, facingness between others has a similar effect, and why. Using a preferential-looking time paradigm, we showed that human adults looked preferentially at two bodies facing each other, relative to the same bodies presented back-to-back. Moreover, facing dyads were rated higher in social semantic dimensions, showing that facingness adds social value to stimuli. The preference for facing dyads thus reflects the ability to understand the social content of facingness beyond first-person interactions. In showing the same preference in macaques and in young children, anticipated by particularly efficient processing of facing dyads in early infancy, we propose that this ability is part of the fabric of a social mind."
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:RGFaLdJalmkC",
        "num_citations": 0,
        "pub_url": "https://osf.io/preprints/8zsc9/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Converging evidence that left extrastriate body area supports visual sensitivity to social interactions",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.05. 23.541943, 2023",
            "author": "Marco Gandolfo and Etienne Abassi and Eva Balgova and Paul E Downing and Liuba Papeo and Kami Koldewyn",
            "journal": "bioRxiv",
            "pages": "2023.05. 23.541943",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Navigating our complex social world requires processing the interactions we observe. Recent psychophysical and neuroimaging studies provide parallel evidence that the human visual system may be attuned to efficiently perceive dyadic interactions. This work implies, but has not yet demonstrated, that activity in body-selective cortical regions causally supports efficient visual perception of interactions. We adopt a multi-method approach to close this important gap. First, using a large fMRI dataset (N=92), we found that the left-hemisphere Extrastriate Body Area (EBA) responds more to face-to-face than non-facing dyads. Second, we replicated a behavioural marker of visual sensitivity to interactions: categorisation of facing dyads is more impaired by inversion than non-facing dyads. Third, we used fMRI-guided transcranial magnetic stimulation to show that online stimulation of the left EBA, but not a nearby control region, abolishes this selective inversion effect. Activity in left EBA, thus, causally supports the efficient perception of social interactions."
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:BqipwSGYUEgC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.05.23.541943.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:a_ezTuPJ_xoJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The effect of ostensive communication on immediate and delayed memory of novel and familiar action patterns",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Cristina Ioana Galusca and Liuba Papeo and Luca Lorenzo Bonatti",
            "publisher": "PsyArXiv",
            "abstract": "Actions are often learnt incidentally by observing other individuals. How aspects inherent to the social context in which an action is seen affect action learning remains poorly understood. Here we study the effect of a special social signal, the eye gaze of the demonstrator, on the immediate and delayed memory and execution of observed actions. In Experiment 1, healthy young adult volunteers watched short videos of a demonstrator performing novel actions with novel objects only once, and were tested for immediate and delayed recall of those actions through action execution. Overall, novel actions on novel objects were recalled more accurately if, during demonstration, the demonstrator had repeatedly gazed towards the participant (direct gaze) vs. towards an object (control condition). Experiment 2 investigated the immediate and delayed recall of novel and familiar object-use actions performed on familiar objects. In this condition, which involved familiar information, direct gaze had no effect or was even detrimental for recall. These findings show that direct gaze benefits incidental learning and retention of novel information only (novel actions paired with novel objects in Experiment 1), and interferes with the retention of familiar information (familiar objects paired with familiar actions in Experiment 2). This study suggests a role of ostensive eye contact in incidentally learning new actions. At the same time, it limits its beneficial role to a specific type of learning scenario, one involving only novel (as opposed to familiar) information, unveiling the selective function of ostensive eye contact in the transmission of novel cultural behaviors."
        },
        "filled": true,
        "author_pub_id": "isdiwE8AAAAJ:maZDTaKrznsC",
        "num_citations": 0,
        "pub_url": "https://psyarxiv.com/vaegk/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:AkKrVnfIxz0J:scholar.google.com/",
        "cites_per_year": {}
    }
]