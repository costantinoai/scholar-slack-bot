[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Multidimensional neural representations of social features during movie viewing",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023",
            "author": "Haemy Lee Masson and Lucy Chang and Leyla Isik",
            "journal": "bioRxiv",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "The social world is dynamic and contextually embedded. Yet, most studies utilize simple stimuli that do not capture the complexity of everyday social episodes. To address this, we implemented a movie viewing paradigm and investigated how the everyday social episodes are processed in the brain. Participants watched one of two movies during an MRI scan. Neural patterns from brain regions involved in social perception, mentalization, action observation, and sensory processing were extracted. Representational similarity analysis results revealed that several labeled social features (including social interaction, mentalization, the actions of others, characters talking about themselves, talking about others, and talking about objects) were represented in superior temporal gyrus (STG) and middle temporal gyrus (MTG). The mentalization feature was also represented throughout the theory of mind network, and \u2026"
        },
        "filled": true,
        "author_pub_id": "U4i0WGsAAAAJ:TFP_iSt0sucC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.11.22.568258",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Rapid processing of observed touch through social perceptual brain regions: an EEG-fMRI fusion study",
            "pub_year": 2023,
            "citation": "Journal of Neuroscience, 2023",
            "author": "Haemy Lee Masson and Leyla Isik",
            "journal": "Journal of Neuroscience",
            "publisher": "Society for Neuroscience",
            "abstract": "Seeing social touch triggers a strong social-affective response that involves multiple brain networks, including visual, social perceptual, and somatosensory systems. Previous studies have identified the specific functional role of each system, but little is known about the speed and directionality of the information flow. Is this information extracted via the social perceptual system or from simulation from somatosensory cortex? To address this, we examined the spatiotemporal neural processing of observed touch. Twenty-one human participants (7 males) watched 500 ms video clips showing social and non-social touch during EEG recording. Visual and social-affective features were rapidly extracted in the brain, beginning at 90 and 150 ms after video onset, respectively. Combining the EEG data with fMRI data from our prior study with the same stimuli reveals that neural information first arises in early visual cortex (EVC \u2026"
        },
        "filled": true,
        "author_pub_id": "U4i0WGsAAAAJ:j3f4tGmQtD8C",
        "num_citations": 0,
        "pub_url": "https://www.jneurosci.org/content/early/2023/10/13/JNEUROSCI.0995-23.2023.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:BEMs7WNFzAwJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Word-timestamped transcripts of two spoken narrative recall functional neuroimaging datasets",
            "pub_year": 2023,
            "citation": "Data in Brief 50, 109490, 2023",
            "author": "Savannah J Born and Kathy Shi and Haemy Lee Masson and Hongmi Lee and Yoonjung Lee and Janice Chen",
            "journal": "Data in Brief",
            "volume": "50",
            "pages": "109490",
            "publisher": "Elsevier",
            "abstract": "After watching audiovisual movies, human participants produced spoken narrative recollections during functional magnetic resonance imaging (fMRI); presented here are word-level timestamps of their speech, temporally aligned to the publicly shared fMRI data. For the \u201cFilmFestival\u201d dataset, twenty participants watched ten short audiovisual movies, approximately 2-8 minutes each. For the \u201cSherlock\u201d dataset, seventeen participants watched the first half of the first episode of BBC's Sherlock (48 minutes). After viewing, participants then verbally described what they remembered about the movies in their own words. Participants\u2019 speech was recorded using an MR-compatible microphone. The audio recordings were transcribed, then timestamped by a forced aligner; missing timestamps were filled in manually by human transcriptionists referencing the audio recording. Each file contains the participant's recall word by \u2026"
        },
        "filled": true,
        "author_pub_id": "U4i0WGsAAAAJ:r0BpntZqJG4C",
        "num_citations": 0,
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S2352340923005905",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Observed social touch is processed in a rapid, feedforward manner: an EEG-fMRI fusion study",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4754-4754, 2023",
            "author": "Haemy Lee Masson and Leyla Isik",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4754-4754",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Observing social touch evokes a strong social-affective response. Our ability to extract the social-affective meaning of observed touch is supported by enhanced communication between brain networks, including social brain regions and somatosensory cortex. Yet, the direction of information flow across these networks and the overall neural dynamics of these processes remain unknown. The current study uses electroencephalography (EEG) to uncover how representations unfold spatial-temporally in the brain during touch observation. Twenty participants watched 500 ms video clips showing social and non-social touch during EEG recording. Representational similarity analysis reveals that EEG neural patterns are explained by visual features beginning at 90 ms post video onset. Social-affective features are processed shortly after, explaining neural patterns beginning at 150 ms. Next, we tracked the spatial \u2026"
        },
        "filled": true,
        "author_pub_id": "U4i0WGsAAAAJ:iH-uZ7U-co4C",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791745",
        "cites_per_year": {}
    }
]