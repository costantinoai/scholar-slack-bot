[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Empirically Identifying and Computationally Modeling the Brain\u2013Behavior Relationship for Human Scene Categorization",
            "pub_year": 2023,
            "citation": "Journal of Cognitive Neuroscience 35 (11), 1879-1897, 2023",
            "author": "Agnessa Karapetian and Antoniya Boyanova and Muthukumar Pandaram and Klaus Obermayer and Tim C Kietzmann and Radoslaw M Cichy",
            "journal": "Journal of Cognitive Neuroscience",
            "volume": "35",
            "number": "11",
            "pages": "1879-1897",
            "publisher": "MIT Press",
            "abstract": "Humans effortlessly make quick and accurate perceptual decisions about the nature of their immediate visual environment, such as the category of the scene they face. Previous research has revealed a rich set of cortical representations potentially underlying this feat. However, it remains unknown which of these representations are suitably formatted for decision-making. Here, we approached this question empirically and computationally, using neuroimaging and computational modeling. For the empirical part, we collected EEG data and RTs from human participants during a scene categorization task (natural vs. man-made). We then related EEG data to behavior to behavior using a multivariate extension of signal detection theory. We observed a correlation between neural data and behavior specifically between \u223c100 msec and \u223c200 msec after stimulus onset, suggesting that the neural scene \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:t7zJ5fGR-2UC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=6540615712862677034",
        "cites_id": [
            "6540615712862677034"
        ],
        "pub_url": "https://direct.mit.edu/jocn/article/doi/10.1162/jocn_a_02043/117201",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The algonauts project 2023 challenge: How the human brain makes sense of natural scenes",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2301.03198, 2023",
            "author": "Alessandro T Gifford and Benjamin Lahner and Sari Saba-Sadiya and Martina G Vilas and Alex Lascelles and Aude Oliva and Kendrick Kay and Gemma Roig and Radoslaw M Cichy",
            "journal": "arXiv preprint arXiv:2301.03198",
            "abstract": "The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD provides high-quality fMRI responses to ~73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:_Ybze24A_UAC",
        "num_citations": 7,
        "citedby_url": "/scholar?hl=en&cites=1174422521076802658",
        "cites_id": [
            "1174422521076802658"
        ],
        "pub_url": "https://arxiv.org/abs/2301.03198",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:YmQAgQ9jTBAJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 7
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Deep neural networks and visuo-semantic models explain complementary components of human ventral-stream representational dynamics",
            "pub_year": 2023,
            "citation": "Journal of Neuroscience 43 (10), 1731-1741, 2023",
            "author": "Kamila M Jozwik and Tim C Kietzmann and Radoslaw M Cichy and Nikolaus Kriegeskorte and Marieke Mur",
            "journal": "Journal of Neuroscience",
            "volume": "43",
            "number": "10",
            "pages": "1731-1741",
            "publisher": "Society for Neuroscience",
            "abstract": "Deep neural networks (DNNs) are promising models of the cortical computations supporting human object recognition. However, despite their ability to explain a significant portion of variance in neural data, the agreement between models and brain representational dynamics is far from perfect. We address this issue by asking which representational features are currently unaccounted for in neural time series data, estimated for multiple areas of the ventral stream via source-reconstructed magnetoencephalography data acquired in human participants (nine females, six males) during object viewing. We focus on the ability of visuo-semantic models, consisting of human-generated labels of object features and categories, to explain variance beyond the explanatory power of DNNs alone. We report a gradual reversal in the relative importance of DNN versus visuo-semantic features as ventral-stream object \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:HE397vMXCloC",
        "num_citations": 3,
        "citedby_url": "/scholar?hl=en&cites=16634044447513850",
        "cites_id": [
            "16634044447513850"
        ],
        "pub_url": "https://www.jneurosci.org/content/43/10/1731.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:-tx4RpMYOwAJ:scholar.google.com/",
        "cites_per_year": {
            "2022": 1,
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "An adversarial collaboration to critically evaluate theories of consciousness",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.06. 23.546249, 2023",
            "author": "Cogitate Consortium and Oscar Ferrante and Urszula Gorska-Klimowska and Simon Henin and Rony Hirschhorn and Aya Khalaf and Alex Lepauvre and Ling Liu and David Richter and Yamil Vidal and Niccolo Bonacchi and Tanya Brown and Praveen Sripad and Marcelo Armendariz and Katarina Bendtz and Tara Ghafari and Dorottya Hetenyi and Jay Jeschke and Csaba Kozma and David Rahul Mazumder and Stephanie Montenegro and Alia Seedat and Abdelrahman Sharafeldin and Shujun Yang and Sylvain Baillet and David J Chalmers and Radoslaw M Cichy and Francis Fallon and Theofanis I Panagiotaropoulos and Hal Blumenfeld and Sasha Devore and Ole Jensen and Gabriel Kreiman and Floris P de Lange and Huan Luo and Melanie Boly and Stanislas Dehaene and Christof Koch and Giulio Tononi and Michael Pitts and Liad Mudrik and Lucia Melloni",
            "journal": "bioRxiv",
            "pages": "2023.06. 23.546249",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Different theories explain how subjective experience arises from brain activity. These theories have independently accrued evidence, yet, confirmation bias and dependence on design choices hamper progress in the field. Here, we present an open science adversarial collaboration which directly juxtaposes Integrated Information Theory (IIT) and Global Neuronal Workspace Theory (GNWT), employing a theory-neutral consortium approach. We investigate neural correlates of the content and duration of visual experience. The theory proponents and the consortium developed and preregistered the experimental design, divergent predictions, expected outcomes, and their interpretation. 256 human subjects viewed suprathreshold stimuli for variable durations while neural activity was measured with functional magnetic resonance imaging, magnetoencephalography, and electrocorticography. We find information about conscious content in visual, ventro-temporal and inferior frontal cortex, with sustained responses in occipital and lateral temporal cortex reflecting stimulus duration, and content-specific synchronization between frontal and early visual areas. These results confirm some predictions of IIT and GNWT, while substantially challenging both theories: for IIT, a lack of sustained synchronization within posterior cortex contradicts the claim that network connectivity specifies consciousness. GNWT is challenged by the general lack of ignition at stimulus offset and limited representation of certain conscious dimensions in prefrontal cortex. Beyond challenging the theories themselves, we present an alternative approach to advance cognitive \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:fEOibwPWpKIC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=1934370548687779377",
        "cites_id": [
            "1934370548687779377"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.06.23.546249.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:MTaD6sND2BoJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Independent spatiotemporal effects of spatial attention and background clutter on human object location representations",
            "pub_year": 2023,
            "citation": "NeuroImage 272, 120053, 2023",
            "author": "Monika Graumann and Lara A Wallenwein and Radoslaw M Cichy",
            "journal": "NeuroImage",
            "volume": "272",
            "pages": "120053",
            "publisher": "Academic Press",
            "abstract": "Spatial attention helps us to efficiently localize objects in cluttered environments. However, the processing stage at which spatial attention modulates object location representations remains unclear. Here we investigated this question identifying processing stages in time and space in an EEG and fMRI experiment respectively. As both object location representations and attentional effects have been shown to depend on the background on which objects appear, we included object background as an experimental factor. During the experiments, human participants viewed images of objects appearing in different locations on blank or cluttered backgrounds while either performing a task on fixation or on the periphery to direct their covert spatial attention away or towards the objects. We used multivariate classification to assess object location information. Consistent across the EEG and fMRI experiment, we show that \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:SdhP9T11ey4C",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=13977593214783473349,6126056352524665008",
        "cites_id": [
            "13977593214783473349",
            "6126056352524665008"
        ],
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S1053811923001994",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:xUYU7vhi-sEJ:scholar.google.com/",
        "cites_per_year": {
            "2021": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The spatiotemporal neural dynamics of object recognition for natural images and line drawings",
            "pub_year": 2023,
            "citation": "Journal of Neuroscience 43 (3), 484-500, 2023",
            "author": "Johannes JD Singer and Radoslaw M Cichy and Martin N Hebart",
            "journal": "Journal of Neuroscience",
            "volume": "43",
            "number": "3",
            "pages": "484-500",
            "publisher": "Society for Neuroscience",
            "abstract": "Drawings offer a simple and efficient way to communicate meaning. While line drawings capture only coarsely how objects look in reality, we still perceive them as resembling real-world objects. Previous work has shown that this perceived similarity is mirrored by shared neural representations for drawings and natural images, which suggests that similar mechanisms underlie the recognition of both. However, other work has proposed that representations of drawings and natural images become similar only after substantial processing has taken place, suggesting distinct mechanisms. To arbitrate between those alternatives, we measured brain responses resolved in space and time using fMRI and MEG, respectively, while human participants (female and male) viewed images of objects depicted as photographs, line drawings, or sketch-like drawings. Using multivariate decoding, we demonstrate that object category \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:LjlpjdlvIbIC",
        "num_citations": 1,
        "citedby_url": "/scholar?hl=en&cites=14102090530843616330,16342086893956331921",
        "cites_id": [
            "14102090530843616330",
            "16342086893956331921"
        ],
        "pub_url": "https://www.jneurosci.org/content/43/3/484.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:SmRVKZ-wtMMJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 1
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2308.09431, 2023",
            "author": "Zejin Lu and Adrien Doerig and Victoria Bosch and Bas Krahmer and Daniel Kaiser and Radoslaw M Cichy and Tim C Kietzmann",
            "journal": "arXiv preprint arXiv:2308.09431",
            "abstract": "Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:XD-gHx7UXLsC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2308.09431",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Empirically Identifying and Computationally Modelling the Brain\u2013Behavior Relationship for Human Scene Categorization",
            "pub_year": 2023,
            "citation": "Journal of Cognitive Neuroscience, 1-19, 2023",
            "author": "Agnessa Karapetian and Antoniya Boyanova and Muthukumar Pandaram and Klaus Obermayer and Tim C Kietzmann and Radoslaw M Cichy",
            "journal": "Journal of Cognitive Neuroscience",
            "pages": "1-19",
            "abstract": "Humans effortlessly make quick and accurate perceptual decisions about the nature of their immediate visual environment, such as the category of the scene they face. Previous research has revealed a rich set of cortical representations potentially underlying this feat. However, it remains unknown which of these representations are suitably formatted for decision-making. Here, we approached this question empirically and computationally, using neuroimaging and computational modelling. For the empirical part, we collected EEG data and RTs from human participants during a scene categorization task (natural vs. man-made). We then related neural representations to behavior using a multivariate extension of signal detection theory. We observed a correlation specifically between\u223c 100 msec and\u223c 200 msec after stimulus onset, suggesting that the neural scene representations in this time period are suitably \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:t7zJ5fGR-2UC",
        "num_citations": 0,
        "pub_url": "https://direct.mit.edu/jocn/article/doi/10.1162/jocn_a_02043/117201",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "To View More...",
            "pub_year": 2023,
            "citation": "Journal of Vision 23, 4574, 2023",
            "author": "Kritika Lohia and Rijul Saurabh Soans and Dharam Raj and Rohit Saxena and Tapan Kumar Gandhi and Brady Roberts and Colin MacLeod and Myra Fernandes and Anne Yilmaz and John Wixted and Petra Borovska and Benjamin de Haas and Marlene Behrmann and Galia Avidan and Janita N Turchi and Fadila Hadj-Bouziane and Ning Liu and Alessandro T Gifford and Kshitij Dwivedi and Gemma Roig and Radoslaw M Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "pages": "4574",
            "abstract": "Youth is not wasted on the young: Late-in-life sight restoration in congenitally blind children leads to the emergence of some visual constructional skills but not others Open Access"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:EkHepimYqZsC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/issues.aspx?issueid=938645&journalid=178",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The effects of visual backward masking on visual spatiotemporal dynamics",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4711-4711, 2023",
            "author": "Siying Xie and Daniel Kaiser and Johannes Singer and Radoslaw Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4711-4711",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Disentangling the neural computations performed through feedforward and feedback information flow in visual processing is challenging. Backward masking is an efficient experimental approach for this purpose, as it effectively interferes with reentrant feedback processing but leaves feedforward processing relatively intact. We used backward masking to dissect the spatiotemporal flow of visual information in the human brain. We briefly presented natural objects which were followed by a dynamic visual mask to participants. The mask could appear shortly after the object (16.7 ms), resulting in low visibility, or with a substantial delay (600ms), resulting in high visibility of the object. We performed multivariate analysis on EEG (n= 32) and fMRI (n= 27) data to characterize temporal and spatial object representations in these two visibility conditions. While visual representations changed rapidly in time for both conditions \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:ipzZ9siozwsC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791786",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A large and rich EEG dataset for modeling human visual object recognition",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4579-4579, 2023",
            "author": "Alessandro T Gifford and Kshitij Dwivedi and Gemma Roig and Radoslaw M Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4579-4579",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "The human brain achieves visual object recognition through multiple stages of transformations operating at a millisecond scale. To predict and explain these rapid transformations, computational neuroscientists employ machine learning modeling techniques. However, state-of-the-art models require massive amounts of data to properly train, and to the present day there is a lack of vast brain datasets which extensively sample the temporal dynamics of visual object recognition. Here we collected a massive millisecond resolution electroencephalography (EEG) dataset of human brain responses to images of objects on a natural background from the THINGS database. We used a time-efficient rapid serial visual presentation paradigm to extensively sample 10 participants, each with 16,740 image conditions repeated over 82,160 trials. We then leveraged the unprecedented size and richness of our dataset to train and \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:uc_IGeMz5qoC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791858",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Revealing the locus and content of behaviorally relevant information about real-world scenes in human visual cortex",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4712-4712, 2023",
            "author": "Johannes Singer and Agnessa Karapetian and Martin Hebart and Radoslaw Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4712-4712",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Scene information can be rapidly categorized and translated into appropriate actions. While there has been substantial progress in understanding how scene information is represented in the brain, it remains unknown to what extent particular scene representations are relevant for decision behavior. To address this question, we recorded fMRI data while human participants (N= 29) viewed manmade and natural scenes and paired it with behavioral data recorded in a separate session from participants (N= 30) performing either a categorization task or an orthogonal task on the same stimuli. In order to identify behaviorally relevant information, we correlated the reaction times (RTs) of individual scenes with the distances of scene-specific fMRI responses to a hyperplane derived from a multivariate pattern classifier. Our findings are threefold. First, we found negative distance-RT correlations for the categorization task in \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:0KyAp5RtaNEC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791785",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Scene representations underlying categorization behaviour emerge 100 to 200 ms after stimulus onset",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4689-4689, 2023",
            "author": "Agnessa Karapetian and Antoniya Boyanova and Muthukumar Pandaram and Klaus Obermayer and Tim C Kietzmann and Radoslaw M Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4689-4689",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Humans are constantly processing scene information from their environment, requiring quick and accurate decision-making and behavioural responses. Despite the importance of this process, it remains unknown which cortical representations might underlie this function. Additionally, to date there is no unifying model of scene categorization which can predict both neural and behavioural correlates as well as their relationship. Here, we approached these questions empirically and via computational modelling using deep neural networks. First, to determine which scene representations are suitably formatted for behaviour, we collected electroencephalography (EEG) data and reaction times from human subjects during a scene categorization task (natural vs. man-made) and an orthogonal task (fixation cross colour discrimination). Then, we linked the neural representations with reaction times in a within-task or a \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:yB1At4FlUx8C",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791806",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The spatiotemporal neural dynamics of Braille letter representations in individuals with congenital blindness",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4710-4710, 2023",
            "author": "Marleen Haupt and Monika Graumann and Santani Teng and Radoslaw Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4710-4710",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Experience shapes the visual brain, but the degree to which its functional organization is plastic remains unknown. A unique opportunity to detect the boundaries of functional plasticity is by measuring brain plasticity in individuals born with congenital blindness. We used Braille letter reading to probe the brains of blind individuals. Driven by the analogy to visual processing that proceeds from viewing condition-dependent to viewing condition-independent representations in recognition, we investigated how the Braille letter processing proceeds from a hand-dependent to a hand-independent representation. For this we measured fMRI (N= 15) and EEG (N= 11) while congenitally blind participants read Braille letters with either their left or right index finger. For both imaging modalities, we applied equivalent multivariate classification schemes: a) to assess hand-dependent letter representations we classified between \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:nrtMV_XWKgEC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791787",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Laminar dissociation of feedforward and feedback signals in high-level ventral visual cortex during imagery and perception",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Tony Carricarte and Polina Iamshchinina and Robert Trampel and Denis Chaimow and Nikolaus Weiskopf and Radoslaw M Cichy",
            "publisher": "PsyArXiv",
            "abstract": "Visual imagery and veridical perception are phenomenologically related, suggesting that they share neural machinery. Previous research confirmed this assumption at the macroscale of human cortical organization in category-selective regions in ventral visual cortex. However, imagery and perception differ fundamentally in the information flow that underlies them: perception is driven by the integration of sensory feedforward and internally-generated feedback information, whereas imagery depends on feedback only. This suggests that while visual imagery and perception may activate common cortical regions, they do so in fundamentally distinctive ways. To investigate, we resolved neural activity during imagery and perception in the ventral visual cortex at the level of laminar organization that anatomically and functionally distinguishes feedforward from feedback information flow. We found distinctive laminar profiles for imagery and perception of scenes and faces in parahippocampal place area (PPA) and fusiform face area (FFA) respectively. Our findings clarify the neural basis of the phenomenology of visual imagery versus perception, and shed new light on how feedforward and feedback information processing in high-level ventral visual cortex orchestrates human object vision."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:9Nmd_mFXekcC",
        "num_citations": 0,
        "pub_url": "https://psyarxiv.com/7zcp8/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Visual features are processed before navigational affordances in the human brain",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.06. 27.546695, 2023",
            "author": "Kshitij Dwivedi and Sari Sadiya and Marta P Balode and Gemma Roig and Radoslaw Cichy",
            "journal": "bioRxiv",
            "pages": "2023.06. 27.546695",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "To navigate through their immediate environment humans process scene information rapidly. How does the cascade of neural processing elicited by scene viewing to facilitate navigational planning unfold over time? To investigate, we recorded human brain responses to visual scenes with electroencephalography (EEG) and related those to computational models that operationalize three aspects of scene processing (2D, 3D, and semantic information), as well as to a behavioral model capturing navigational affordances. We found a temporal processing hierarchy: navigational affordance is processed later than the other scene features (2D, 3D, and semantic) investigated. This reveals the temporal order with which the human brain computes complex scene information and suggests that the brain leverages these pieces of information to plan navigation."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:j8SEvjWlNXcC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.06.27.546695.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Individual differences in internal models explain idiosyncrasies in scene perception",
            "pub_year": 2023,
            "citation": "PsyArXiv, 2023",
            "author": "Matthew J Foxwell and Gongting Wang and Radoslaw M Cichy and David Pitcher and Daniel Kaiser",
            "publisher": "PsyArXiv",
            "abstract": "According to predictive processing theories, vision is facilitated by predictions derived from our internal models of what the world should look like. However, the contents of these models and how they vary across people remains unclear. Here, we use drawing to directly access the contents of the internal models of individual participants. Participants were first asked to draw typical versions of scene categories, as descriptors of their internal models. These drawings were converted into standardized 3d renders, which we used as stimuli in subsequent scene categorization experiments. Across two experiments, participants\u2019 scene categorization was more accurate for renders tailored to their own drawings compared to renders based on others\u2019 drawings or copies of scene photographs, suggesting that scene perception is determined by a match with idiosyncratic internal models. These results demonstrate that visual perception can only be fully understood through the lens of our personally unique models of the world."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:7T2F9Uy0os0C",
        "num_citations": 0,
        "pub_url": "https://psyarxiv.com/98wt7/download?format=pdf",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jCng5_Qk1GYJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Alpha-frequency feedback to early visual cortex orchestrates coherent natural vision",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.02. 10.527986, 2023",
            "author": "Lixiang Chen and Radoslaw Martin Cichy and Daniel Kaiser",
            "journal": "bioRxiv",
            "pages": "2023.02. 10.527986",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "For coherent visual experience to emerge, the brain needs to spatially integrate the complex and dynamic information it receives from the environment. To meet this challenge, the visual system uses contextual information from one part of the visual field to create feedback signals that guide analysis in other parts of the visual field. Here, we set out to characterize the nature of this feedback across brain rhythms and cortical regions. In EEG and fMRI experiments, we experimentally recreated the spatially distributed nature of visual inputs by presenting natural videos at different visual field locations. Critically, we manipulated the spatiotemporal congruency of the videos, so that they did or did not demand integration into a coherent percept. Decoding stimulus information from frequency-specific EEG patterns revealed a shift from representations in feedforward-related gamma activity for spatiotemporally inconsistent videos to representations in feedback-related alpha activity for spatiotemporally consistent videos. Our fMRI data suggest high-level scene-selective areas as the putative source of this feedback. Combining the EEG data with spatially resolved fMRI recordings, we demonstrate that alpha-frequency feedback is directly associated with representations in early visual cortex. Together this demonstrates how the human brain orchestrates coherent visual experience across space: it uses feedback to integrate information from high-level to early visual cortex through a dedicated rhythmic code in the alpha frequency range."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:kzcrU_BdoSEC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.02.10.527986.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:fb5VqTo388UJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The link between visual representations and behavior in human scene perception",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.08. 17.553708, 2023",
            "author": "Johannes JD Singer and Agnessa Karapetian and Martin N Hebart and Radoslaw Martin Cichy",
            "journal": "bioRxiv",
            "pages": "2023.08. 17.553708",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Scene recognition is a core sensory capacity that enables humans to adaptively interact with their environment. Despite substantial progress in the understanding of the neural representations underlying scene recognition, it remains unknown how these representations translate into behavior given different task demands. To address this, we aimed to identify behaviorally relevant scene representations, to characterize them in terms of their underlying visual features, and to reveal how they vary given different tasks. We recorded fMRI data while human participants viewed manmade and natural scenes and linked brain responses to behavior in one of two tasks acquired in a separate set of subjects: a manmade/natural categorization task or an orthogonal task on fixation. First, we found correlations between scene categorization response times (RTs) and scene-specific brain responses, quantified as the distance to a hyperplane derived from a multivariate classifier, in occipital and ventral-temporal, but not parahippocampal cortex. This suggests that representations in early visual and object-selective cortex are relevant for scene categorization. Next, we revealed that mid-level visual features, as quantified using deep convolutional neural networks, best explained the relationship between scene representations and behavior, indicating that these features are read out in scene categorization. Finally, we observed opposite patterns of correlations between brain responses and RTs in the categorization and orthogonal task, suggesting a critical influence of task on the behavioral relevance of scene representations. Together, these results reveal the \u2026"
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:z_wVstp3MssC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.08.17.553708.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Attention Modulates Human Visual Responses to Objects by Tuning Sharpening",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.06. 01.543205, 2023",
            "author": "Narges Doostani and Gholam-Ali Hossein-Zadeh and Radoslaw Martin Cichy and Maryam Vaziri-Pashkam",
            "journal": "bioRxiv",
            "pages": "2023.06. 01.543205",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Visual stimuli compete with each other for cortical processing and attention biases this competition in favor of the attended stimulus. How does the relationship between the stimuli affect the strength of this attentional bias? Here, we used functional MRI to explore the effect of target-distractor similarity in neural representation on attentional modulation in the human visual cortex using univariate and multivariate pattern analyses. Using stimuli from four object categories (human bodies, cats, cars and houses), we investigated attentional effects in the primary visual area V1, the object-selective regions LO and pFs, the body-selective region EBA, and the scene-selective region PPA. We demonstrated that the strength of the attentional bias towards the target is not fixed but decreases with increasing distractor-target similarity. Simulations provided evidence that this result pattern is explained by tuning sharpening rather than an increase in gain. Our findings provide a mechanistic explanation for behavioral effects of target-distractor similarity on attentional biases and suggest tuning sharpening as the underlying mechanism in object-based attention."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:evX43VCCuoAC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.06.01.543205.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:e1QGogg_aXwJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The influence of the bullseye versus standard fixation cross on eye movements and classifying natural images from EEG",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.03. 21.532944, 2023",
            "author": "Greta H\u00e4berle and Aynur Pelin \u00c7elikkol and Radoslaw M Cichy",
            "journal": "bioRxiv",
            "pages": "2023.03. 21.532944",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Eye movements are a ubiquitous and natural behavior, but in many tightly controlled experimental visual paradigms, eye movements are undesirable. Their occurrence can pose challenges to the interpretation of behavioral and neuroscientific data, in particular for magneto- and electroencephalography (M/EEG), which is sensitive to signals created by eye muscle movement. Here we compared the effect of two different fixation symbols - the standard fixation cross and the bullseye fixation cross - in the context of a visual paradigm with centrally presented naturalistic object images. We investigated eye movements and EEG data recorded simultaneously using behavioral and multivariate analysis techniques. Our findings comparing the bullseye to the standard fixation cross are threefold. First, the bullseye fixation cross reduces the number of saccades and amplitude size of microsaccades. Second, the bullseye subtly reduces classification accuracy in both eye tracking and EEG data for the classification of single object images, but not for the superlevel category animacy. Third, using representational similarity analysis, we found a systematic relationship between eye tracking and EEG data at the level of single images for the standard, but not for the bullseye fixation cross. In conclusion, we recommend the bullseye fixation cross in experimental paradigms with fixation when particularly tight control of fixation is beneficial."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:tKAzc9rXhukC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.03.21.532944.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:OF8_VmjXv10J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "BOLD Moments: modeling short visual events through a video fMRI dataset and metadata",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.03. 12.530887, 2023",
            "author": "Benjamin Lahner and Kshitij Dwivedi and Polina Iamshchinina and Monika Graumann and Alex Lascelles and Gemma Roig and Alessandro Thomas Gifford and Bowen Pan and SouYoung Jin and N Apurva Ratan Murty and Kendrick Kay and Aude Oliva and Radoslaw Cichy",
            "journal": "bioRxiv",
            "pages": "2023.03. 12.530887",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Grasping the meaning of everyday visual events is a fundamental feat of human intelligence that hinges on diverse neural processes ranging from vision to higher-level cognition. Deciphering the neural basis of visual event understanding requires rich, extensive, and appropriately designed experimental data. However, this type of data is hitherto missing. To fill this gap, we introduce the BOLD Moments Dataset (BMD), a large dataset of whole-brain fMRI responses to over 1,000 short (3s) naturalistic video clips and accompanying metadata. We show visual events interface with an array of processes, extending even to memory, and we reveal a match in hierarchical processing between brains and video-computable deep neural networks. Furthermore, we showcase that BMD successfully captures temporal dynamics of visual events at second resolution. BMD thus establishes a critical groundwork for investigations of the neural basis of visual event understanding."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:NJ774b8OgUMC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.03.12.530887.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:jESFCRR-e_wJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Empirically identifying and computationally modelling the brain-behaviour relationship for human scene categorization",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.01. 22.525084, 2023",
            "author": "Agnessa Karapetian and Antoniya Boyanova and Muthukumar Pandaram and Klaus Obermayer and Tim Christian Kietzmann and Radoslaw Martin Cichy",
            "journal": "bioRxiv",
            "pages": "2023.01. 22.525084",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Humans effortlessly make quick and accurate perceptual decisions about the nature of their immediate visual environment, such as the category of the scene they face. Previous research has revealed a rich set of cortical representations potentially underlying this feat. However, it remains unknown which of these representations are suitably formatted for decision-making. Here, we approached this question empirically and computationally, using neuroimaging and computational modelling. For the empirical part, we collected electroencephalography (EEG) data and reaction times from human participants during a scene categorization task (natural vs. man-made). We then related neural representations to behaviour using a multivariate extension of signal detection theory. We observed a correlation specifically between ~100 ms and ~200 ms after stimulus onset, suggesting that the neural scene representations in this time period are suitably formatted for decision-making. For the computational part, we evaluated a recurrent convolutional neural network (RCNN) as a model of brain and behaviour. Unifying our previous observations in an image-computable model, the RCNN predicted well the neural representations, the behavioural scene categorization data, as well as the relationship between them. Our results identify and computationally characterize the neural and behavioural correlates of scene categorization in humans."
        },
        "filled": true,
        "author_pub_id": "XZtcvyEAAAAJ:W5xh706n7nkC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.01.22.525084.abstract",
        "cites_per_year": {}
    }
]