[
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
<<<<<<< Updated upstream
=======
            "title": "Dimensions underlying the representational alignment of deep neural networks with humans",
            "pub_year": 2024,
            "citation": "arXiv preprint arXiv:2406.19087, 2024",
            "author": "Florian P Mahner and Lukas Muttenthaler and Umut G\u00fc\u00e7l\u00fc and Martin N Hebart",
            "journal": "arXiv preprint arXiv:2406.19087",
            "abstract": "Determining the similarities and differences between humans and artificial intelligence is an important goal both in machine learning and cognitive neuroscience. However, similarities in representations only inform us about the degree of alignment, not the factors that determine it. Drawing upon recent developments in cognitive science, we propose a generic framework for yielding comparable representations in humans and deep neural networks (DNN). Applying this framework to humans and a DNN model of natural images revealed a low-dimensional DNN embedding of both visual and semantic dimensions. In contrast to humans, DNNs exhibited a clear dominance of visual over semantic features, indicating divergent strategies for representing images. While in-silico experiments showed seemingly-consistent interpretability of DNN dimensions, a direct comparison between human and DNN representations revealed substantial differences in how they process images. By making representations directly comparable, our results reveal important challenges for representational alignment, offering a means for improving their comparability."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:bVQMTfhMCi4C",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2406.19087",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:mt3HgYqvmwAJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Unrealized promise of joint modeling of choice and reaction time in improving representation learning",
            "pub_year": 2024,
            "citation": "Proceedings of the Annual Meeting of the Cognitive Science Society 46, 2024",
            "author": "Russell Richie and Nehal Ajmal and Martin N Hebart",
            "journal": "Proceedings of the Annual Meeting of the Cognitive Science Society",
            "volume": "46",
            "abstract": "As mental representations are standardly thought to underlie all cognitive processes, a major goal of cognitive science has been to uncover representations. Methods for representation learning from behavioral data often model choice or reaction time data alone, but not jointly, leaving out potentially useful information. Here we develop two models of choice and RT in the odd-one-out task, including one based on the Linear Ballistic Accumulator. Parameter recovery simulations show joint modeling of choice and RT with LBA recovers representations more accurately than modeling choice alone with softmax. However, on two empirical datasets of images and words, joint models performed no better than choice-only models, despite a significant correlation of reaction time with two measures of similarity and choice difficulty in both datasets. We speculate on reasons for the unrealized promise of joint modeling of RT and choice in representation learning."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:RMgMIBzvq-4C",
        "num_citations": 0,
        "pub_url": "https://escholarship.org/uc/item/87n5h98z",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:HoDwNcl2g1MJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "What comparing deep neural networks can teach us about human vision",
            "pub_year": 2024,
            "citation": "OSF, 2024",
            "author": "Katja Seeliger and Martin N Hebart",
            "publisher": "OSF",
            "abstract": "Recent work has demonstrated impressive parallels between human visual representations and those found in deep neural networks. A new study by Wang et al.(2023) highlights what factors may determine this similarity."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:L24QuVWYgZ0C",
        "num_citations": 0,
        "pub_url": "https://osf.io/e8vha/download",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
>>>>>>> Stashed changes
            "title": "Distributed representations of behaviorally-relevant object dimensions in the human visual system",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023",
            "author": "Oliver Contier and Chris I Baker and Martin N Hebart",
            "journal": "bioRxiv",
            "publisher": "Cold Spring Harbor Laboratory Preprints",
            "abstract": "Object vision is commonly thought to involve a hierarchy of brain regions processing increasingly complex image features, with high-level visual cortex supporting object recognition and categorization. However, object vision supports diverse behavioral goals, suggesting basic limitations of this category-centric framework. To address these limitations, here we map a series of behaviorally-relevant dimensions derived from a large-scale analysis of human similarity judgments directly onto the brain. Our results reveal broadly distributed representations of behaviorally-relevant information, demonstrating selectivity to a wide variety of novel dimensions while capturing known selectivities for visual features and categories. Behaviorally-relevant dimensions were superior to categories at predicting brain responses, yielding mixed selectivity in much of visual cortex and sparse selectivity in category-selective clusters. This \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:xm0LlTxljI0C",
        "num_citations": 0,
        "pub_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10473665/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Parallel cognitive maps for multiple knowledge structures in the hippocampal formation",
            "pub_year": 2023,
            "citation": "Radboud Data Repository, 2023",
            "author": "X Zheng and Martin Hebart and R Cools and Mona Garvert and CFV Grill and C Doeller and R Dolan",
            "publisher": "Radboud Data Repository",
            "abstract": "The hippocampal-entorhinal system uses cognitive maps to represent spatial knowledge and other types of relational information. However, objects can often be characterized by different types of relations simultaneously. How does the hippocampal formation handle the embedding of stimuli in multiple relational structures that differ vastly in their mode and timescale of acquisition? Does the hippocampal formation integrate different stimulus dimensions into one conjunctive map or is each dimension represented in a parallel map? Here, we reanalyzed human functional magnetic resonance imaging (fMRI) data from Garvert et al. (2017) that had previously revealed a map in the hippocampal formation coding for a newly learnt transition structure. Using fMRI adaptation analysis, we found that the degree of representational similarity in the bilateral hippocampus also decreased as a function of the semantic distance between presented objects. Importantly, while both map-like structures localized to the hippocampal formation, the semantic map was located in more posterior regions of the hippocampal formation than the transition structure and thus anatomically distinct. This finding supports the idea that the hippocampal-entorhinal system forms parallel cognitive maps that reflect the embedding of objects in diverse relational structures."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:7H_jS4BsgvYC",
        "num_citations": 0,
        "pub_url": "https://repository.ubn.ru.nl/handle/2066/299547",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dimensions that matter: Interpretable object dimensions in humans and deep neural networks",
            "pub_year": 2023,
            "citation": "Sl: sn, 2023",
            "author": "Florian P Mahner and Lukas Muttenthaler and U G\u00fc\u00e7l\u00fc and MN Hebart",
            "publisher": "Sl: sn",
            "abstract": "How do mind and machines represent objects? This question has sparked continued interest in the connected fields of cognitive neuroscience and artificial intelligence. Here we address this question by introducing a novel approach that allows us to compare human and deep neural network (DNN) representations through an interpretable embedding. We achieve this by treating the DNN as an in-silico human observer and asking it to rate the similarities between objects in a triplet task. We find that (i) DNN representations capture meaningful object properties, (ii) demonstrate with multiple in-silico tests that the DNN contains conceptual and perceptual representations including shape, and (iii) identify similarities and differences in their representational content."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:W2uZP3ddy8sC",
        "num_citations": 0,
        "pub_url": "https://repository.ubn.ru.nl/bitstream/handle/2066/297725/297725.pdf?sequence=1",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The features underlying the memorability of objects",
            "pub_year": 2023,
            "citation": "Science Advances 9 (17), eadd2981, 2023",
            "author": "Max A Kramer and Martin N Hebart and Chris I Baker and Wilma A Bainbridge",
            "journal": "Science Advances",
            "volume": "9",
            "number": "17",
            "pages": "eadd2981",
            "abstract": "Despite decades of study of memory, it remains unclear what makes an image memorable. There is considerable debate surrounding the underlying determinants of memory, including the roles of semantic (e.g., animacy, utility) and visual features (e.g., brightness) as well as whether the most prototypical or most atypical items are best remembered. Prior studies have relied on constrained stimulus sets, limiting any generalized view of the features that may contribute to memory. Here, we collected over one million memory ratings (N=13,946) for THINGS , a naturalistic dataset of 26,107 object images designed to comprehensively sample concrete objects. First, we establish a model of object features that is predictive of image memorability, capturing over half of the explainable variance. For this model, we find that semantic features have a stronger influence than visual features on what people will remember. Second, we examined whether memorability could be accounted for by the typicality of the objects, by comparing human behavioral data, object feature dimensions, and deep neural network features. While prototypical objects tend to be the most memorable, the relationship between memorability and typicality is more complex than a simple positive or negative association and typicality alone cannot account for memorability.Why is it that we seem to remember and forget the same things? Our lived experiences differ, but there is remarkable consistency in what is remembered across people. Here, we collected memory performance scores for a comprehensive and diverse collection of natural object images to \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:oTdOBqtIf_kC",
        "num_citations": 26,
        "citedby_url": "/scholar?hl=en&cites=5945500258311618340,10652378970080750051,12041836059132593332",
        "cites_id": [
            "5945500258311618340",
            "10652378970080750051",
            "12041836059132593332"
        ],
        "pub_url": "https://www.biorxiv.org/content/10.1101/2022.04.29.490104.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:JD-o_PuoglIJ:scholar.google.com/",
        "cites_per_year": {
            "2021": 1,
            "2022": 5,
            "2023": 20
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "THINGS-data, a multimodal collection of large-scale datasets for investigating object representations in human brain and behavior",
            "pub_year": 2023,
            "citation": "Elife 12, e82580, 2023",
            "author": "Martin N Hebart and Oliver Contier and Lina Teichmann and Adam H Rockter and Charles Y Zheng and Alexis Kidder and Anna Corriveau and Maryam Vaziri-Pashkam and Chris I Baker",
            "journal": "Elife",
            "volume": "12",
            "pages": "e82580",
            "publisher": "eLife Sciences Publications Limited",
            "abstract": "Understanding object representations requires a broad, comprehensive sampling of the objects in our visual world with dense measurements of brain activity and behavior. Here, we present THINGS-data, a multimodal collection of large-scale neuroimaging and behavioral datasets in humans, comprising densely sampled functional MRI and magnetoencephalographic recordings, as well as 4.70 million similarity judgments in response to thousands of photographic images for up to 1,854 object concepts. THINGS-data is unique in its breadth of richly annotated objects, allowing for testing countless hypotheses at scale while assessing the reproducibility of previous findings. Beyond the unique insights promised by each individual dataset, the multimodality of THINGS-data allows combining datasets for a much broader view into object processing than previously possible. Our analyses demonstrate the high quality of the datasets and provide five examples of hypothesisdriven and data-driven applications. THINGS-data constitutes the core public release of the THINGS initiative (https://things-initiative. org) for bridging the gap between disciplines and the advancement of cognitive neuroscience."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:PQEM9vzQD9gC",
        "num_citations": 26,
        "citedby_url": "/scholar?hl=en&cites=11776019156333001843,12813796853124410588,11855572849486790520",
        "cites_id": [
            "11776019156333001843",
            "12813796853124410588",
            "11855572849486790520"
        ],
        "pub_url": "https://elifesciences.org/articles/82580",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:c3xs2QfTbKMJ:scholar.google.com/",
        "cites_per_year": {
            "2021": 1,
            "2022": 4,
            "2023": 21
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Dimensions underlying human understanding of the reachable world",
            "pub_year": 2023,
            "citation": "Cognition 234, 105368, 2023",
            "author": "Emilie L Josephs and Martin N Hebart and Talia Konkle",
            "journal": "Cognition",
            "volume": "234",
            "pages": "105368",
            "publisher": "Elsevier",
            "abstract": "Near-scale environments, like work desks, restaurant place settings or lab benches, are the interface of our hand-based interactions with the world. How are our conceptual representations of these environments organized? What properties distinguish among reachspaces, and why? We obtained 1.25 million similarity judgments on 990 reachspace images, and generated a 30-dimensional embedding which accurately predicts these judgments. Examination of the embedding dimensions revealed key properties underlying these judgments, such as reachspace layout, affordance, and visual appearance. Clustering performed over the embedding revealed four distinct interpretable classes of reachspaces, distinguishing among spaces related to food, electronics, analog activities, and storage or display. Finally, we found that reachspace similarity ratings were better predicted by the function of the spaces than their \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:hNSvKAmkeYkC",
        "num_citations": 5,
        "citedby_url": "/scholar?hl=en&cites=16572595389553118791,9522837584176446662",
        "cites_id": [
            "16572595389553118791",
            "9522837584176446662"
        ],
        "pub_url": "https://www.sciencedirect.com/science/article/pii/S0010027723000021",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:Rw6XkLOv_eUJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 5
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The spatiotemporal neural dynamics of object recognition for natural images and line drawings",
            "pub_year": 2023,
            "citation": "Journal of Neuroscience 43 (3), 484-500, 2023",
            "author": "Johannes JD Singer and Radoslaw M Cichy and Martin N Hebart",
            "journal": "Journal of Neuroscience",
            "volume": "43",
            "number": "3",
            "pages": "484-500",
            "publisher": "Society for Neuroscience",
            "abstract": "Drawings offer a simple and efficient way to communicate meaning. While line drawings capture only coarsely how objects look in reality, we still perceive them as resembling real-world objects. Previous work has shown that this perceived similarity is mirrored by shared neural representations for drawings and natural images, which suggests that similar mechanisms underlie the recognition of both. However, other work has proposed that representations of drawings and natural images become similar only after substantial processing has taken place, suggesting distinct mechanisms. To arbitrate between those alternatives, we measured brain responses resolved in space and time using fMRI and MEG, respectively, while human participants (female and male) viewed images of objects depicted as photographs, line drawings, or sketch-like drawings. Using multivariate decoding, we demonstrate that object category \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:Ic1VZgkJnDsC",
        "num_citations": 2,
        "citedby_url": "/scholar?hl=en&cites=14102090530843616330",
        "cites_id": [
            "14102090530843616330"
        ],
        "pub_url": "https://www.jneurosci.org/content/43/3/484.abstract",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:SmRVKZ-wtMMJ:scholar.google.com/",
        "cites_per_year": {
            "2023": 2
        }
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Getting aligned on representational alignment",
            "pub_year": 2023,
            "citation": "arXiv preprint arXiv:2310.13018, 2023",
            "author": "Ilia Sucholutsky and Lukas Muttenthaler and Adrian Weller and Andi Peng and Andreea Bobu and Been Kim and Bradley C Love and Erin Grant and Jascha Achterberg and Joshua B Tenenbaum and Katherine M Collins and Katherine L Hermann and Kerem Oktar and Klaus Greff and Martin N Hebart and Nori Jacoby and Raja Marjieh and Robert Geirhos and Sherol Chen and Simon Kornblith and Sunayana Rane and Talia Konkle and Thomas P O'Connell and Thomas Unterthiner and Andrew K Lampinen and Klaus-Robert M\u00fcller and Mariya Toneva and Thomas L Griffiths",
            "journal": "arXiv preprint arXiv:2310.13018",
            "abstract": "Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \\textbf{\\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from the fields of cognitive science, neuroscience, and machine learning, and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:FsLZdJ3BAzkC",
        "num_citations": 0,
        "pub_url": "https://arxiv.org/abs/2310.13018",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Multidimensional object properties are dynamically represented in the human brain",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.09. 08.556679, 2023",
            "author": "Lina Teichmann and Martin N Hebart and Chris I Baker",
            "journal": "bioRxiv",
            "pages": "2023.09. 08.556679",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Our visual world consists of an immense number of unique objects and yet, we are easily able to identify, distinguish, interact, and reason about the things we see within several hundred milliseconds. This requires that we flexibly integrate and focus on different object properties to support specific behavioral goals. In the current study, we examined how these rich object representations unfold in the human brain by modelling time-resolved MEG signals evoked by viewing thousands of objects. Using millions of behavioral judgments to guide our understanding of the neural representation of the object space, we find distinct temporal profiles across the object dimensions. These profiles fell into two broad types with either a distinct and early peak (~150 ms) or a slow rise to a late peak (~300 ms). Further, the early effects are stable across participants in contrast to later effects which show more variability across people. This highlights that early peaks may carry stimulus-specific and later peaks subject-specific information. Given that the dimensions with early peaks seem to be primarily visual dimensions and those with later peaks more conceptual, our results suggest that conceptual processing is more variable across people. Together, these data provide a comprehensive account of how a variety of object properties unfold in the human brain and contribute to the rich nature of object vision."
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:cBPnxVikjH8C",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.09.08.556679.abstract",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Distributed representations of behaviorally relevant object dimensions in the human visual system",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023",
            "author": "Oliver Contier and Chris I Baker and Martin N Hebart",
            "journal": "bioRxiv",
            "publisher": "Cold Spring Harbor Laboratory Preprints",
            "abstract": "Object vision is commonly thought to involve a hierarchy of brain regions processing increasingly complex image features, with high-level visual cortex supporting object recognition and categorization. However, object vision supports diverse behavioral goals, suggesting basic limitations of this category-centric framework. To address these limitations, here we map a series of behaviorally-relevant dimensions derived from a large-scale analysis of human similarity judgments directly onto the brain. Our results reveal broadly-distributed representations of behaviorally-relevant information, demonstrating selectivity to a wide variety of novel dimensions while capturing known selectivities for visual features and categories. Behaviorally-relevant dimensions were superior to categories at predicting brain responses, yielding mixed selectivity in much of visual cortex and sparse selectivity in category-selective clusters. This \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:xm0LlTxljI0C",
        "num_citations": 0,
        "pub_url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10473665/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Revisiting the animacy, size, and curvature organization of human visual cortex",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5072-5072, 2023",
            "author": "Laura M Stoinski and Oliver Contier and Talia Konkle and Martin N Hebart",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5072-5072",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Previous research has uncovered a large-scale organization of object categories in occipitotemporal cortex by the dimensions of animacy and real-world size (Konkle & Caramazza, 2013). The tripartite division of cortical zones with a preference for large objects, all animals, and small objects has been robustly replicated and appears to be driven by the mid-level visual feature curvature, ie large objects tend to be boxier, and small objects and animals curvier (Long et al., 2017). However, given the factorial design in the original studies, it has remained open to what degree these findings generalize to larger stimulus sets. To address this question, we used THINGS-fMRI, a large-scale dataset comprising fMRI responses to 8,740 naturalistic images of 720 animate and inanimate object categories (Contier et al., 2021). We then collected and applied a rich behavioral dataset of perceived animacy, real-world size, and \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:rCNdntzdTkkC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791451",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "THINGS-drawings: A large-scale dataset containing human sketches of 1,854 object concepts",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5975-5975, 2023",
            "author": "Judith E Fan and Kushin Mukherjee and Holly Huey and Martin N Hebart and Wilma A Bainbridge",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5975-5975",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "People\u2019s knowledge about objects has traditionally been probed using a combination of feature-listing and rating tasks. However, feature listing fails to capture nuances in what people know about how objects look\u2014their visual knowledge\u2014which cannot easily be described in words. Moreover, rating tasks are limited by the set of attributes that researchers even think to consider. By contrast, freehand sketching provides a way for people to externalize their visual knowledge about objects in an open-ended fashion. As such, sketch behavior provides a versatile substrate for asking a wide range of questions about visual object knowledge that go beyond the scope of a typical study. Here we introduce THINGS-drawings, a new crowdsourced dataset containing multiple freehand sketches of the 1,854 object concepts in the THINGS database (Hebart et al., 2019). THINGS-drawings contains fine-grained information \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:EaFouW7jFu4C",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792322",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Revealing interpretable object dimensions from a high-throughput model of the fusiform face area",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5356-5356, 2023",
            "author": "Oliver Contier and Shu Fujimori and Katja Seeliger and N Apurva Ratan Murty and Martin Hebart",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5356-5356",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "A central aim of visual neuroscience is to uncover the function of individual visually-responsive brain regions. A hallmark of occipitotemporal cortex is its functional organization into category-selective brain regions, and among these regions, it is well established that fusiform face area (FFA) responds highly selectively to the visual presentation of faces. At the same time, previous research has shown that FFA activity overlaps with several other feature maps that are not face specific, such as animacy, size, or curvature (Long et al., 2017), and FFA has been shown to carry above-chance information about non-face objects (Duchaine & Yovel, 2015). Thus, it remains an open question which other object dimensions may be represented in patterns of FFA responses. Here, we explored this question with a recent high-throughput neural-network model of FFA activity which has been shown to yield excellent predictive \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:x21FZCSn4ZoC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2792039",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Uncovering high-level visual cortex preferences by training convolutional neural networks on large neuroimaging data",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5493-5493, 2023",
            "author": "K Seeliger and R Leipe and J Roth and MN Hebart",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5493-5493",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Pretrained task-optimized convolutional neural networks are commonly used to predict brain responses to visual stimuli. Yet, they contain biases introduced by their training dataset and task objective (eg classification). Recent large-scale visual neuroimaging datasets have opened the avenue towards training modern convolutional neural networks with the objective of directly predicting brain responses measured with human neuroimaging data, which allows overcoming these biases. Here, we used the THINGS and the Natural Scenes Datasets\u2013both massive functional MRI datasets acquired during the presentation of object photographs\u2013to identify a suitable neural network architecture from the machine learning community from a set of candidate architectures (ResNet50, VGG-16, CORnet-S, and others) for predicting responses of individual regions in high-level visual cortex. Careful optimization of these \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:QoJ_w57xiyAC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791912",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "cneuromod-things: a large-scale fMRI dataset for task-and data-driven assessment of object representation and visual memory recognition in the human brain",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5424-5424, 2023",
            "author": "Marie St-Laurent and Basile Pinsard and Oliver Contier and Katja Seeliger and Valentina Borghesani and Julie Boyle and Pierre Bellec and Martin Hebart",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5424-5424",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Understanding how the brain represents objects is a transdisciplinary endeavor that benefits from large and comprehensive datasets. The THINGS initiative is a global effort that aims to collect large-scale datasets with diverse neuroimaging techniques and in multiple species to advance our understanding of object processing in the mind and brain. At its core lies the THINGS database, which includes a thoroughly annotated set of images that are unique for their broad and systematic sampling of natural and man-made objects. Contributing to this growing initiative, we present cneuromod-things, an fMRI dataset acquired while four participants each completed between 33 and 36 sessions of a continuous recognition paradigm on thousands of THINGS images. The same~ 4k unique images were shown three times to every participant over the course of the experiment (18 repetitions for each of 720 image categories \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:6VlyvFCUEfcC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791977",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Revealing the locus and content of behaviorally relevant information about real-world scenes in human visual cortex",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 4712-4712, 2023",
            "author": "Johannes Singer and Agnessa Karapetian and Martin Hebart and Radoslaw Cichy",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "4712-4712",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "Scene information can be rapidly categorized and translated into appropriate actions. While there has been substantial progress in understanding how scene information is represented in the brain, it remains unknown to what extent particular scene representations are relevant for decision behavior. To address this question, we recorded fMRI data while human participants (N= 29) viewed manmade and natural scenes and paired it with behavioral data recorded in a separate session from participants (N= 30) performing either a categorization task or an orthogonal task on the same stimuli. In order to identify behaviorally relevant information, we correlated the reaction times (RTs) of individual scenes with the distances of scene-specific fMRI responses to a hyperplane derived from a multivariate pattern classifier. Our findings are threefold. First, we found negative distance-RT correlations for the categorization task in \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:Azgs6IHzeyYC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791785",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Assessing the feasibility of high stimulus presentation rates for contrasting conditions in functional MRI studies",
            "pub_year": 2023,
            "citation": "Journal of Vision 23 (9), 5070-5070, 2023",
            "author": "Johannes Roth and Yoichi Miyawaki and Martin N Hebart",
            "journal": "Journal of Vision",
            "volume": "23",
            "number": "9",
            "pages": "5070-5070",
            "publisher": "The Association for Research in Vision and Ophthalmology",
            "abstract": "It is commonly assumed that event-related functional MRI studies of the visual system require slow stimulus presentation rates, with stimuli typically presented every 3-5s. At this rate, the BOLD signal is expected to be linear and can therefore be analyzed in a linear modeling framework. However, this assumption conflicts with recent findings that have successfully mapped the content of video stimuli to brain responses and with other work using more rapidly changing stimuli, suggesting that higher presentation rates may be possible. To address these seemingly conflicting views, we used simulations and measured brain responses with 7 Tesla fMRI (TR= 500ms) to determine the extent to which rapid stimulus presentation is achievable. For fMRI, we varied presentation rates between 0.5 s and 4s and presented observers with images of faces, places, objects, and scrambled objects. Our simulations showed that \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:7Frjd3zlGBUC",
        "num_citations": 0,
        "pub_url": "https://jov.arvojournals.org/article.aspx?articleid=2791453",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "A data-driven investigation of human action representations",
            "pub_year": 2023,
            "citation": "Scientific Reports 13 (1), 5171, 2023",
            "author": "Diana C Dima and Martin N Hebart and Leyla Isik",
            "journal": "Scientific Reports",
            "volume": "13",
            "number": "1",
            "pages": "5171",
            "publisher": "Nature Publishing Group UK",
            "abstract": "Understanding actions performed by others requires us to integrate different types of information about people, scenes, objects, and their interactions. What organizing dimensions does the mind use to make sense of this complex action space? To address this question, we collected intuitive similarity judgments across two large-scale sets of naturalistic videos depicting everyday actions. We used cross-validated sparse non-negative matrix factorization to identify the structure underlying action similarity judgments. A low-dimensional representation, consisting of nine to ten dimensions, was sufficient to accurately reconstruct human similarity judgments. The dimensions were robust to stimulus set perturbations and reproducible in a separate odd-one-out experiment. Human labels mapped these dimensions onto semantic axes relating to food, work, and home life; social axes relating to people and emotions; and one \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:DXE8ND7PrJAC",
        "num_citations": 0,
        "pub_url": "https://www.nature.com/articles/s41598-023-32192-5",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:ZiMKje3GGn8J:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The Three Terms Task-an open benchmark to compare human and artificial semantic representations",
            "pub_year": 2023,
            "citation": "Scientific Data 10 (1), 117, 2023",
            "author": "V Borghesani and J Armoza and Martin N Hebart and P Bellec and SM Brambati",
            "journal": "Scientific Data",
            "volume": "10",
            "number": "1",
            "pages": "117",
            "publisher": "Nature Publishing Group UK",
            "abstract": "Word processing entails retrieval of a unitary yet multidimensional semantic representation (e.g., a lemon\u2019s colour, flavour, possible use) and has been investigated in both cognitive neuroscience and artificial intelligence. To enable the direct comparison of human and artificial semantic representations, and to support the use of natural language processing (NLP) for computational modelling of human understanding, a critical challenge is the development of benchmarks of appropriate size and complexity. Here we present a dataset probing semantic knowledge with a three-terms semantic associative task: which of two target words is more closely associated with a given anchor (e.g., is lemon closer to squeezer or sour?). The dataset includes both abstract and concrete nouns for a total of 10,107 triplets. For the 2,255 triplets with varying levels of agreement among NLP word embeddings, we additionally collected \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:WwIwg2wKZ0QC",
        "num_citations": 0,
        "pub_url": "https://www.nature.com/articles/s41597-023-02015-3",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:rtU11RZxLSkJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "Revealing interpretable object representations from human visual cortex and artificial neural networks",
            "pub_year": 2023,
            "citation": "2023 11th International Winter Conference on Brain-Computer Interface (BCI), 1-3, 2023",
            "author": "Martin Hebart",
            "conference": "2023 11th International Winter Conference on Brain-Computer Interface (BCI)",
            "pages": "1-3",
            "publisher": "IEEE",
            "abstract": "Predictive models are often limited by their strong focus on prediction accuracy, leading to potential for shortcut learning and limited out-of-set generalization. Recent interpretability methods have focused primarily on understanding the contribution of individual features or image regions to classification performance, but have placed less emphasis on the larger set of representational motifs that are being learned by predictive models. In this talk, I will highlight recent work from our own group aimed at revealing interpretable object representations from human behavior, patterns of brain activity, and artificial neural networks. Our approach operates at the level of triplet similarities and yields low-dimensional human interpretable embeddings with excellent reconstruction accuracy, providing both perceptual as well as semantic representational dimensions. By providing a trade-off between complexity, interpretability and \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:Vr2j17o0sqMC",
        "num_citations": 0,
        "pub_url": "https://ieeexplore.ieee.org/abstract/document/10078606/",
        "url_related_articles": "/scholar?oi=bibs&hl=en&q=related:yyPcghleNEsJ:scholar.google.com/",
        "cites_per_year": {}
    },
    {
        "container_type": "Publication",
        "source": "AUTHOR_PUBLICATION_ENTRY",
        "bib": {
            "title": "The link between visual representations and behavior in human scene perception",
            "pub_year": 2023,
            "citation": "bioRxiv, 2023.08. 17.553708, 2023",
            "author": "Johannes JD Singer and Agnessa Karapetian and Martin N Hebart and Radoslaw Martin Cichy",
            "journal": "bioRxiv",
            "pages": "2023.08. 17.553708",
            "publisher": "Cold Spring Harbor Laboratory",
            "abstract": "Scene recognition is a core sensory capacity that enables humans to adaptively interact with their environment. Despite substantial progress in the understanding of the neural representations underlying scene recognition, it remains unknown how these representations translate into behavior given different task demands. To address this, we aimed to identify behaviorally relevant scene representations, to characterize them in terms of their underlying visual features, and to reveal how they vary given different tasks. We recorded fMRI data while human participants viewed manmade and natural scenes and linked brain responses to behavior in one of two tasks acquired in a separate set of subjects: a manmade/natural categorization task or an orthogonal task on fixation. First, we found correlations between scene categorization response times (RTs) and scene-specific brain responses, quantified as the distance to a hyperplane derived from a multivariate classifier, in occipital and ventral-temporal, but not parahippocampal cortex. This suggests that representations in early visual and object-selective cortex are relevant for scene categorization. Next, we revealed that mid-level visual features, as quantified using deep convolutional neural networks, best explained the relationship between scene representations and behavior, indicating that these features are read out in scene categorization. Finally, we observed opposite patterns of correlations between brain responses and RTs in the categorization and orthogonal task, suggesting a critical influence of task on the behavioral relevance of scene representations. Together, these results reveal the \u2026"
        },
        "filled": true,
        "author_pub_id": "Q-n9_FgAAAAJ:KS-xo-ZNxMsC",
        "num_citations": 0,
        "pub_url": "https://www.biorxiv.org/content/10.1101/2023.08.17.553708.abstract",
        "cites_per_year": {}
    }
]